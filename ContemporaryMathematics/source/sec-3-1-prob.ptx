<?xml version="1.0" encoding="UTF-8" ?>
<section xmlns:xi="http://www.w3.org/2001/XInclude" xml:id="sec-3-1-prob">
	<title>Probability</title>
	<subsection>
		<title>Introduction</title>
		<p>
			The probability of a specified event is the chance or likelihood that it will occur.
			There are several ways of viewing probability.
			One would be <term>experimental</term> in nature, where we repeatedly conduct an experiment.
			Suppose we flipped a coin over and over and over again and it came up heads about half of the time; we would expect that in the future whenever we flipped the coin it would turn up heads about half of the time.
			When a weather reporter says “there is a 10% chance of rain tomorrow,” she is basing that on prior evidence; that out of all days with similar weather patterns, it has rained on 10 out of 100 (or 1 out of 10) of those days.
		</p>

		<p>
			Another view would be <term>subjective</term> in nature, in other words an educated guess.
			If someone asked you the probability that the Seattle Mariners would win their next baseball game, it would be impossible to conduct an experiment where the same two teams played each other repeatedly, each time with the same starting lineup and starting pitchers, each starting at the same time of day on the same field under precisely the same conditions.
			Since there are so many variables to take into account, someone familiar with baseball and with the two teams involved might make an educated guess that there is a 75% chance they will win the game; that is, if the same two teams were to play each other repeatedly under identical conditions, the Mariners would win about three out of every four games.
			But this is just a guess, with no way to verify its accuracy, and depending upon how educated the educated guesser is, a subjective probability may not be worth very much.
		</p>

		<p>
			We will return to the experimental and subjective probabilities from time to time, but in this course we will mostly be concerned with <term>theoretical</term> probability, which is defined as follows: Suppose there is a situation with <m> n </m>
			<em>equally likely</em> possible outcomes and that <m> m </m> of those <m> n </m> outcomes correspond to a particular event; then the <term>probability</term> of that event is defined as <m> \frac{m}{n} </m>.
		</p>

		<p>
			If you have learned about probability in previous classes, it was probably theoretical probability.
			For example, if we think about the probability of tossing a head on a fair coin from a theoretical perspective, then tossing a fair coin has 2 equally likely outcomes (<m>n=2</m>) and heads is one of those outcomes (<m>m=1</m>), so the probability of tossing a head is <m>\frac{1}{2}</m>.
			More examples are below.
		</p>
	</subsection>

	<subsection>
		<title>Basic Concepts</title>
		<p>
			If you roll a die, pick a card from deck of playing cards, or randomly select a person and observe their hair color, we are executing an experiment or procedure.
			In probability, we look at the likelihood of different outcomes.
			We begin with some terminology.
		</p>

		<assemblage>
			<title>Events and Outcomes</title>
			<p>
				The result of an experiment is called an <term>outcome</term>.
				Outcomes must be <term>disjoint</term>, meaning that two different outcomes cannot happen at the same time, since every time you perform the experiment, there is exactly one outcome.
			</p>

			<p>
				The <term>sample space</term> is the set of all possible outcomes.
			</p>

			<p>
				An <term>event</term> is any particular outcome or group of outcomes.
				If an event includes multiple outcomes, it is known as a <term>compound event</term>; if it consists of exactly one outcome, i.e., it cannot be broken down further, it is known as a <term>simple event</term>.
			</p>
		</assemblage>

		<example>
			<p>
				If we roll a standard 6-sided die, describe the sample space and some events.
			</p>

			<p>
				The sample space is the set of all possible outcomes: {1,2,3,4,5,6}
			</p>

			<p>
				Some examples of simple events:
				<ul>
					<li>
						We roll a 1
					</li>

					<li>
						We roll a 5
					</li>
				</ul>
			</p>

			<p>
				Some compound events:
				<ul>
					<li>
						We roll a number bigger than 4
					</li>

					<li>
						We roll an even number
					</li>
				</ul>
			</p>
		</example>

		<assemblage>
			<title>Basic Probability</title>
			<p>
				The <term>probability</term> of an outcome or event is the proportion of times we should expect the outcome or event to occur if the experiment is run many times.
			</p>

			<p>
				Since outcomes are disjoint, the probability of an event is the sum of the probabilities of all the outcomes included in the event.
			</p>

			<p>
				If all outcomes of an experiment are equally likely, we can compute the probability of an event <m>E</m> using this formula:
			</p>

			<p>
				<me>
					P(E) = \frac{\text{Number of outcomes corresponding to the event } E }{\text{Total number of equally likely outcomes}}
				</me>
			</p>
		</assemblage>

		<example>
			<p>
				If we roll a 6-sided die, calculate:
				<ol>
					<li>
						P(rolling a 1)
					</li>

					<li>
						P(rolling a number greater than 4)
					</li>
				</ol>
			</p>

			<p>
				Recall that the possible outcomes are {1,2,3,4,5,6}
			</p>

			<p>
				<ol>
					<li>
						There is one outcome corresponding to “rolling a 1”, so the probability is <m>\frac{1}{6}</m>
					</li>

					<li>
						There are two outcomes bigger than a 4, so the probability is <m>\frac{2}{6}=\frac{1}{3}</m>
					</li>
				</ol>
			</p>
		</example>

		<p>
			Probabilities can be written as fractions and can be reduced to lower terms like fractions.
			They also can be written as decimals or as percentages.
		</p>

		<example>
			<p>
				Let's say you have a bag with 20 cherries, 14 sweet and 6 sour.
				If you pick a cherry at random, what is the probability that it will be sweet?
			</p>

			<p>
				There are 20 possible cherries that could be picked, so the number of possible outcomes is 20.
				Of these 20 possible outcomes, 14 are favorable (sweet), so the probability that the cherry will be sweet is <m> \frac{14}{20} = \frac{7}{10} </m>, which can also be written as 0.7 or 70%.
			</p>
		</example>

		<p>
			There is one potential complication to this example, however.
			It must be assumed that the probability of picking any of the cherries is the same as the probability of picking any other.
			This wouldn't be true if (let us imagine) the sweet cherries are smaller than the sour ones.
			(The larger, sour cherries would be selected more readily when you sampled from the bag.) Let us keep in mind, therefore, that when we assess probabilities in terms of the ratio of favorable to all potential cases, we rely heavily on the assumption of equal probability for all outcomes.
		</p>

		<exploration>
			<p>
				At some random moment, you look at your (digital) clock and note the minutes reading.
			</p>

			<ol>
				<li>
					What is the probability the minutes reading is 15?
				</li>

				<li>
					What is the probability the minutes reading is 15 or less?
				</li>
			</ol>
		</exploration>

		<assemblage>
			<title>Cards</title>
			<p>
				A standard deck of 52 playing cards consists of four <term>suits</term> (hearts, spades, diamonds and clubs).
				Spades and clubs are black while hearts and diamonds are red.
				Each suit contains 13 cards, each of a different <term>rank</term>: an Ace (which in many games functions as both a low card and a high card), cards numbered 2 through 10, a Jack, a Queen and a King.
			</p>
		</assemblage>

		<example>
			<p>
				Compute the probability of randomly drawing one card from a deck and getting an Ace.
			</p>

			<p>
				There are 52 cards in the deck and 4 Aces so P(Ace) = <m>\frac{4}{52} = \frac{1}{13} \approx 0.0769 </m>
			</p>

			<p>
				Thinking of probabilities as percents we would say there is a 7.69% chance that a randomly selected card will be an Ace.
			</p>
		</example>

		<p>
			Notice that the smallest possible probability of an event is 0, which means there are no outcomes that correspond with the event (i.e.
			the event is not possible).
			The largest possible probability is 1, which means all possible outcomes correspond with the event (i.e.
			one of the outcomes in the event is certain to occur).
		</p>

		<assemblage>
			<title>Certain and Impossible events</title>
			<p>
				An impossible event has a probability of 0.
			</p>

			<p>
				A certain event has a probability of 1.
			</p>

			<p>
				The probability of any event must be <m>0 \leq P(E) \leq 1</m>
			</p>
		</assemblage>

		<p>
			In the course of this chapter, if you compute a probability and get an answer that is negative or greater than 1, you have made a mistake and should check your work.
		</p>
	</subsection>

	<subsection>
		<title>Discrete Probability Distributions</title>
		<p>
			It is often useful to list the probabilities of all possible outcomes of an experiment.
			This can allow us to more easily compare events to each other and see other useful patterns in how probability is distributed between the outcomes.
		</p>

		<assemblage>
			<title>Discrete Probability Distributions</title>
			<p>
				If an experiment has finitely many possible outcomes, then the <term>probability distribution</term> of the experiment is the list of outcomes and their associated probabilities.
				It is typically given in a table or bar graph.
			</p>

			<p>
				The sum of the probabilities of all the outcomes is always 1 (or 100%), since there is a 100% chance that some outcome happens.
			</p>
		</assemblage>

		<exploration>
			<statement>
				<p>
					The table below suggests three possible probability distributions for the income of a randomly-chosen household in the United States, in thousands of dollars.
					Only one of (a), (b) or (c) is correct.
					Which one must it be? What is wrong with the other two?
				</p>

				<sidebyside>
					<tabular halign="center" right="minor" left="minor" top="minor" bottom="minor">
						<row>
						<cell>Income range ($1000s)</cell>
						<cell>0-25</cell>
						<cell>25-50</cell>
						<cell>50-100</cell>
						<cell>100+</cell>
						</row>
						<row>
						<cell>(a)</cell>
						<cell>0.18</cell>
						<cell>0.39</cell>
						<cell>0.33</cell>
						<cell>0.16</cell>
						</row>
						<row>
						<cell>(b)</cell>
						<cell>0.28</cell>
						<cell>0.27</cell>
						<cell>0.29</cell>
						<cell>0.16</cell>
						</row>
						<row>
						<cell>(c)</cell>
						<cell>0.18</cell>
						<cell>-0.47</cell>
						<cell>1.12</cell>
						<cell>0.17</cell>
						</row>
					</tabular>
				</sidebyside>
			</statement>

			<solution>
				<p>
					Only distribution (b) is a valid probability distribution.
					The probability of each outcome is between 0 and 1, and the probabilities together add to exactly 1.
					The probabilities of distribution (a) add to 1.06, and some probabilities in distribution (c) are below 0 or above 1.
				</p>
			</solution>
		</exploration>

		<example>
			<title>The sum of two cards</title>
			<p>
				Suppose we take the 2, 3, 4, 5, 6, and 7 of spades from a deck of cards, shuffle these six cards, then add together the values of the top two cards.
				The outcomes of this experiment are the possible sums: the integers 5 - 13.
				Below is the probability distribution of this experiment, in table and graph form.
				(It is possible to calculate the probability of each outcome by hand.
				Try it yourself for an extra challenge!)
			</p>

			<sidebyside>
				<tabular halign="center" right="minor" left="minor" top="minor" bottom="minor">
					<row>
					<cell>Sum of card values</cell>
					<cell>5</cell>
					<cell>6</cell>
					<cell>7</cell>
					<cell>8</cell>
					<cell>9</cell>
					<cell>10</cell>
					<cell>11</cell>
					<cell>12</cell>
					<cell>13</cell>
					</row>
					<row>
					<cell>Probability</cell>
					<cell><m>\frac{1}{15}</m></cell>
					<cell><m>\frac{1}{15}</m></cell>
					<cell><m>\frac{2}{15}</m></cell>
					<cell><m>\frac{2}{15}</m></cell>
					<cell><m>\frac{1}{5}</m></cell>
					<cell><m>\frac{2}{15}</m></cell>
					<cell><m>\frac{2}{15}</m></cell>
					<cell><m>\frac{1}{15}</m></cell>
					<cell><m>\frac{1}{15}</m></cell>
					</row>
				</tabular>
			</sidebyside>

			<image source="two-cards-prob-dist.png"/>
				<p>
					The table allows us to quickly calculate the probability of any event:
					<ul>
						<li>
							The probability that the sum of the cards is either 7 or 13 is
							<me>
								\frac{2}{15} + \frac{1}{15} = \frac{3}{15} = \frac{1}{5} = 0.2,
							</me>
							or 20%.
						</li>

						<li>
							The probability that the sum of the cards is even is
							<me>
								\frac{1}{15} + \frac{2}{15} + \frac{2}{15} + \frac{1}{15} = \frac{6}{15} = \frac{2}{5} = 0.4,
							</me>
							or 40%.
						</li>
					</ul>
				</p>

				<p>
					The bar graph allows us to more easily see the way the probability is distributed over the possible outcomes.
					For example, you likely noticed that this probability distribution is symmetric, centered at the outcome 9.
					Therefore, if we performed this experiment many times, we should expect the most common outcome to be 9 and the average of all the outcomes to be 9.
					We should also expect that an outcome greater than 11 is less likely than an outcome smaller than 8, since we can see that the area of the bars to the right of 11 is less than the area of the bars to the left of 8.
				</p>
			</example>

			<exploration xml:id="commute-example">
				<statement>
					<p>
						Consider the following probability distribution for the amount of time you spend commuting to work on a given day (rounded to the nearest number of minutes).
					</p>

					<image source="commute-length.png"/>
						<ol>
							<li>
								What is the probability that your commute takes 27 minutes or longer?
							</li>

							<li>
								Is it more likely that your commute takes more than 24 minutes or less than 23 minutes?
							</li>

							<li>
								If asked how long your commute takes, what number of minutes would you say?
							</li>

							<li>
								Do you think the average number of minutes you take to get to work is less than, greater than, or equal to the number you chose for the previous part?  If there is any difference, what do you think causes it?
							</li>
						</ol>
					</statement>

					<solution>
						<p>
							<ol>
								<li>
									<m>0.06+0.03+0.02+0.01 = 0.12</m> or 12%
								</li>

								<li>
									We could compute the probabilities, but looking at the graph it seems clear that there is more area in the bars to the right of 24 minutes than to the left of 23 minutes.
									So we can confidently say, without any calculations, that it is more likely for your commute to take more than 24 minutes.
								</li>

								<li>
									This is certainly subjective, but if you had to simplify this to a single number, as people often do, 23 minutes probably makes the most sense.
									Your commute takes between 22 and 24 minutes the majority (59%) of the time, and it most commonly takes 23 minutes.
								</li>

								<li>
									Unlike the previous example with the sum of two cards, this probability distribution is not symmetric; it trails off to the right.
									This means that there is more probability on the right side of the peak.
									Therefore, it's most likely that you will tend to take longer than 23 minutes on average.
									This may be caused by the fact that random occurrences that cause delays are likely to have a more significant impact on your commute than ones that save you time.
									For example, a bad traffic day would probably cause a longer delay than the amount of time that particularly clear roads would save you.
								</li>
							</ol>
						</p>
					</solution>
				</exploration>

				<exploration>
					<statement>
						<p>
							You may have seen a weather forecast giving the chance of rain over the course of a day in a bar graph that looks something like this:
						</p>

						<image source="weather-forecast.png"/>
							<p>
								Is this a probability distribution?  Explain why or why not.
							</p>
						</statement>

						<solution>
							<p>
								This graph is not a probability distribution.
								The horizontal axis lists times of day that it could rain, and it is possible that it could rain across multiple times.
								Since it could rain at 4:00pm and at 5:00pm, for example, these times cannot be considered the list of outcomes of an experiment.
								You might also have noticed that the probabilities all together will add up to much more than 100%, which also reflects the fact that it could rain at multiple times throughout the day.
							</p>
						</solution>
					</exploration>
				</subsection>
				<!--  THIS WAS MOVED TO SECTION 3.3
				<subsection>
					<title>Continuous Probability Distributions</title>
					<p>
						So far, we have only looked at experiments with finitely many possible outcomes.
						However, in some situations, it makes sense to consider the outcomes of an experiment to fall within a continous range of outcomes, in which case there are infinitely many possible outcomes.
					</p>

					<p>
						Consider again the probability distribution from <xref ref="commute-example"/>, regarding commute time.
						In that Exploration, we considered the outcomes to be integer numbers of minutes for simplicity's sake, but we could add more detail.
						A given commute might take closer to 23.25 minutes than 23 minutes, a level of precision that we ignored.
						In theory, your commute could take any number of minutes, including decimal numbers of minutes like 21.326 or even 26.202002000200002....
						Although we would never be able to measure your travel time with an infinite amount of precision, it makes sense to say that your actual travel time could be any of the infinitely-many real numbers within a reasonable range; all of these numbers are possible outcomes.
					</p>

					<p>
						There is an interesting consequence for probability when the outcomes of an experiment span a continous interval of numbers: the probability of any given outcome is typically 0.
						To get some intuition for why this should be true, consider what happens to the commute time probability distribution if we start splitting outcomes into more precise categories, from the nearest minute, to the nearest half-minute, to the nearest quarter-minute, and so on:
					</p>

					<figure xml:id="commute-discrete-distributions">
						<sidebyside>
							<figure>
								<caption>Measuring commute time to the nearest minute</caption>
								<image source="commute-hist-1.png"/>
								</figure>

								<figure>
									<caption>...to the nearest half-minute</caption>
									<image source="commute-hist-2.png"/>
									</figure>
								</sidebyside>

								<sidebyside>
									<figure>
										<caption>...to the nearest quarter-minute</caption>
										<image source="commute-hist-3.png"/>
										</figure>

										<figure>
											<caption>...to the nearest eighth-minute</caption>
											<image source="commute-hist-4.png"/>
											</figure>
										</sidebyside>
										<caption>Splitting the commute time distribution into more and more precise outcomes.</caption>
									</figure>

									<p>
										Notice that the probabilities of the outcomes keep getting closer and closer to 0 as we add more precision to our outcomes.
										This makes sense; it is much less likely that you will arrive within one second of your expected arrival time than that you will arrive within one minute of it.
										If we continue this process indefinitely, the probabilities approach 0.
										Since the probability of arriving exactly 23 minutes after you left, for example, must be even less than that of arriving within one millisecond of 23 minutes, or within any tiny amount of error, the probability of your commute taking exactly 23 minutes must be 0.
										Similarly, the probability of any number being the exact amount of time your commute takes is 0.
									</p>

									<p>
										Because each outcome of this experiment has a probability of 0, listing the probability of each outcome or drawing a graph of the probability of each outcome isn't going to be useful to us like it was with discrete probability distributions.
										However, given any interval of possible outcomes, there may be some non-zero probability that the outcome of the experiment falls within that interval; for example, there is some non-zero probability that your commute takes between 24 and 27 minutes, and a smaller probability that it takes between 24.5 and 26.15 minutes.
										Our goal will be to convey the probability corresponding to any <i>interval</i> of minutes, rather than a particular number of minutes.
									</p>

									<p>
										You may have noticed in <xref ref="commute-discrete-distributions"/> that, although the individual probabilities are all getting smaller, the general shapes of the distributions all look similar.
										To see this more clearly, here's what each of the distributions in <xref ref="commute-discrete-distributions"/> looks like if we scale their vertical axes so that they all appear to have the same height:
									</p>

									<figure xml:id="commute-discrete-distributions-scaled">
										<sidebyside>
											<figure>
												<caption>Measuring commute time to the nearest minute</caption>
												<image source="commute-hist-1.png"/>
												</figure>

												<figure>
													<caption>...to the nearest half-minute</caption>
													<image source="commute-hist-2-scaled.png"/>
													</figure>
												</sidebyside>

												<sidebyside>
													<figure>
														<caption>...to the nearest quarter-minute</caption>
														<image source="commute-hist-3-scaled.png"/>
														</figure>

														<figure>
															<caption>...to the nearest eighth-minute</caption>
															<image source="commute-hist-4-scaled.png"/>
															</figure>
														</sidebyside>
														<caption>Scaling each vertical axis so that each distribution appears to have the same height.</caption>
													</figure>

													<p>
														If we continue this process of splitting the distribution into more and more precise outcomes, and keep scaling the vertical axis to keep the height of the distributions constant, these graphs will approach a smooth curve, as in <xref ref="commute-density-function"/>.
														This curve is called the <term>probability density function</term>, or the <term>continuous probability distribution</term> for the experiment.
														The probability density function will tell us the probability corresponding to any interval.
													</p>

													<figure xml:id="commute-density-function">
														<image source="commute-continuous-distribution.png"/>
															<caption>The probability density function for your commute time.
															The area of the shaded region is the probability that your commute takes between 24.5 and 26.15 minutes.</caption>
														</figure>

														<p>
															Notice that, in each of the bar graphs representing a finite probability distribution, the proportion of area in the graph over a given interval roughly represents the probability that the outcome of the experiment will fall within that interval.
															For example, the proportion of area in the bars past the 27 minute mark represents the probability that your commute takes at least 27 minutes.
															In the same way, given any interval of minutes, the probability that your commute time falls within that interval is the proportion of area under the probability density function over that interval.
															To make things simpler, we scale the density function so that the total area under the curve is 1, making area equal to probability.
														</p>

														<assemblage>
															<title>Probability Density Functions</title>
															<p>
																A <term>probability density function</term> (also known as a <term>continuous probability distribution</term>) is a curve that fully describes the probabilities involved in an experiment where the collection of outcomes is a continuous interval.
															</p>

															<p>
																The probability of any given outcome of such an experiment is typically 0, so we only measure the probability that the result of the experiment falls within an interval of outcomes.
															</p>

															<p>
																Given an interval of outcomes, the probability associated to the interval is the area under the probability density function over that interval.
															</p>

															<p>
																The total area under a probability density function is 1, since this is the probability that any outcome in the interval of all possible outcomes occurs.
															</p>
														</assemblage>

														<exploration>
															%%%%% example where you can actually compute area under curve by hand, because it's piecewise linear (maybe just a triangle?)
															<statement>
																<p>
																	Suppose a computer program outputs a random number between 0 and 5 with the following probability density function:
																</p>

																<image source="piecewise-linear-distribution.png"/>
																	<ol>
																		<li>
																			What is the probability that the output is greater than 2?
																		</li>

																		<li>
																			What is the probability that the output is between 1 and 2?
																		</li>

																		<li>
																			What is the probability that the output is at least 1.5?
																		</li>
																	</ol>
																</statement>

																<solution>
																	<ol>
																		<li>
																			This probability is represented by the area under the curve from 2 to 5 on the horizontal axis, which is the area of the triangle with vertices <m>(2,0)</m>, <m>(2,0.4)</m>, and <m>(5,0)</m>.
																			It has a base of 3 and a height of 0.4, so its area is <m>\frac{1}{2} \cdot 3 \cdot 0.4 = 0.6</m>.
																			Therefore, the probability that the output is greater than 2 is 0.6, or 60%.
																		</li>

																		<li>
																			This probability is represented by the area of the trapezoid with vertices at <m>(1,0)</m>, <m>(1,0.2)</m>, <m>(2,0.4)</m>, and <m>(2,0)</m>.
																			One way to find its area is to split it into a triangle with base 1 and height 0.2, and a rectangle with base 1 and height 0.2, making the total area <m>\frac{1}{2} \cdot 1 \cdot 0.2 + 1 \cdot 0.2 = 0.3</m>.
																			Therefore, the probability that the output is between 1 and 2 is 0.3 or 30%.
																		</li>

																		<li>
																			This probability is represented by the area of the quadrilateral with vertices <m>(1.5,0)</m>, <m>(1.5,0.3)</m>, <m>(2,0.4)</m>, and <m>(5,0)</m>.
																			However, it is probably easiest to find this area by subtracting the area under the curve between 0 and 1.5 from the total area under the curve, which we know to be 1.
																			The area under the curve between 0 and 1.5 is the area of a triangle with vertices <m>(0,0)</m>, <m>(1.5,0.3)</m>, and <m>(1.5,0)</m>, which has a base of 1.5 and height of 0.3, making its area <m>\frac{1}{2} \cdot 1.5 \cdot 0.3 = 0.225</m>.
																			So the area under the curve from 1.5 to 5, and therefore the probability that the output is at least 1.5, is <m>1 - 0.225 = 0.775</m>, or 77.5%.
																		</li>
																	</ol>
																</solution>
															</exploration>

															<exploration>
																example where they just look at qualitative stuff, perhaps something bimodal would be interesting?
																<statement>
																	<p>
																		Riley sometimes works a morning shift and sometimes works an afternoon shift.
																		Below is a probability density function for the time they wake up in the morning.
																	</p>

																	<image source="wake-up.png"/>
																		<ol>
																			<li>
																				Is Riley more likely to wake up before or after 8am?
																			</li>

																			<li>
																				Distributions that have multiple peaks like this one are called <term>bimodal</term>.
																				Based on the context of the problem, why do you think this distribution is bimodal?
																			</li>

																			<li>
																				The first peak is taller and thinner, and the second peak is shorter and wider.
																				What do you think accounts for these differences in the width and height of the two peaks?
																			</li>
																		</ol>
																	</statement>

																	<solution>
																		<ol>
																			<li>
																				Although the peak in the graph before 8am is higher than the peak after, the area under the curve after 8am is larger, since the second peak is much wider.
																				Since area represents probability, this means that Riley is more likely to wake up after 8am.
																			</li>

																			<li>
																				It is probably bimodal because Riley sometimes needs to get up for work early in the morning (accounting for the first peak) and sometimes doesn't, and sleeps in more (accounting for the second peak).
																				Based on the answer to the previous question, Riley likely works afternoon shifts more often than morning shifts.
																			</li>

																			<li>
																				The fact that the first peak is tall and skinny means that a relatively large amount of probability is distributed over a short time period.
																				In other words, on days that Riley gets up early for their morning shift, they probably set an alarm around 7:15am and wake up to it fairly quickly and consistently in order to avoid being late for work.
																				The fact that the second peak is shorter and wider means that on days that Riley doesn't work a morning shift, their wake-up time is not quite as consistent; although they most often wake up around 9am, they also often sleep in.
																			</li>
																		</ol>
																	</solution>
																</exploration>

																<exploration>
																	<statement>
																		<p>
																			Can the height of a probability density function go above 1?  Explain why or why not.
																		</p>
																	</statement>

																	<solution>
																		<p>
																			The height can go above 1.
																			Since the height of the function does not represent probability, there is no limit on the height of a probability density function.
																			It is true that the area under a probability density function must equal 1, and therefore if there is a very high peak, it must be very thin in order to avoid accumulating more than 1 unit of area under the curve.
																		</p>
																	</solution>
																</exploration>
															</subsection>
															-->
															<!-- Material from before 2021
															<subsection>
																<title>Complementary Events</title>
																<p>
																	Now let us examine the probability that an event does <term>not</term> happen.
																	As in the previous section, consider the situation of rolling a six-sided die and first compute the probability of rolling a six: the answer is P(six) =1/6.
																	Now consider the probability that we do not roll a six: there are 5 outcomes that are not a six, so the answer is P(not a six) = <m> \frac{5}{6}</m> .
																	Notice that P(six)+P(not a six) = <m> \frac{1}{6} + \frac{5}{6} = \frac{6}{6} = 1</m>
																</p>

																<p>
																	This is not a coincidence.
																	Consider a generic situation with n possible outcomes and an event E that corresponds to m of these outcomes.
																	Then the remaining n - m outcomes correspond to E not happening, thus
																</p>

																<p>
																	P(not E) = <m> \frac{n-m}{n} = \frac{n}{n}-\frac{m}{n}=1</m> - P(E)
																</p>

																<assemblage>
																	<title>Complement of an Event</title>
																	<p>
																		The <term>complement</term> of an event is the event "E doesn't happen".
																	</p>

																	<p>
																		The notation <m>\overline{E}</m> is used for the complement of an event <m>E</m>.
																	</p>

																	<p>
																		We can compute the probability of the complement using <m>P(\overline{E}) = 1-P(E)</m>
																	</p>

																	<p>
																		Notice also that <m>P(E) = 1-P(\overline{E})</m>
																	</p>
																</assemblage>

																<example>
																	<p>
																		If you pull a random card from a deck of playing cards, what is the probability it is not a heart?
																	</p>

																	<p>
																		There are 13 hearts in the deck, so P(heart)=<m>\frac{13}{52}=\frac{1}{4}</m>
																	</p>

																	<p>
																		The probability of not drawing a heart is in the complement:
																	</p>

																	<p>
																		P(not heart) = 1-P(heart) = <m>1-\frac{1}{4}=\frac{3}{4}</m>
																	</p>
																</example>
															</subsection>

															<subsection>
																<title>Probability of two independent events</title>
																<example>
																	<p>
																		Suppose we flipped a coin and rolled a die, and wanted to know the probability of getting a head on the coin and a 6 on the die.
																	</p>

																	<p>
																		We could list all possible outcomes:   {H1,H2,H3,H4,H5,H6,T1,T2,T3,T4,T5,T6}.
																	</p>

																	<p>
																		Notice there are <m>2 \cdot 6 = 12</m> total outcomes.
																		Out of these, only 1 is the desired outcome, so the probability is  <m>\frac{1}{12}</m>.
																	</p>
																</example>

																<p>
																	The prior example was looking at two independent events.
																</p>

																<assemblage>
																	<title>Indepdentent Events</title>
																	<p>
																		Events A and B are <term>independent events</term> if the probability of Event B occurring is the same whether or not Event A occurs.
																	</p>
																</assemblage>

																<example>
																	<p>
																		Are these events independent?
																	</p>

																	<p>
																		<ol>
																			<li>
																				A fair coin is tossed two times.
																				The two events are (1) first toss is a head and (2) second toss is a head.
																			</li>

																			<li>
																				The two events (1) "It will rain tomorrow in Houston" and (2) "It will rain tomorrow in Galveston” (a city near Houston).
																			</li>

																			<li>
																				You draw a card from a deck, then draw a second card without replacing the first.
																			</li>
																		</ol><
																		/p>
																		<p>
																			Solutions:
																		</p>

																		<p>
																			<ol>
																				<li>
																					The probability that a head comes up on the second toss is 1/2 regardless of whether or not a head came up on the first toss, so these events are independent.
																				</li>

																				<li>
																					These events are not independent because it is more likely that it will rain in Galveston on days it rains in Houston than on days it does not.
																				</li>

																				<li>
																					The probability of the second card being red depends on whether the first card is red or not, so these events are not independent.
																				</li>
																			</ol><
																			/p>
																		</example>

																		<p>
																			When two events are independent, the probability of both occurring is the product of the probabilities of the individual events.
																		</p>

																		<assemblage>
																			<title>P(A and B) for indepdent events</title>
																			<p>
																				If events A and B are independent, then the probability of both A and B occurring is
																			</p>

																			<p>
																				<me>
																					P(A \text{ and } B) = P(A) \cdot P(B)
																				</me>
																			</p>

																			<p>
																				where P(A and B) is the probability of events A and B both occurring, P(A) is the probability of event A occurring, and P(B) is the probability of event B occurring.
																			</p>
																		</assemblage>

																		<p>
																			If you look back at the coin and die example from earlier, you can see how the number of outcomes of the first event multiplied by the number of outcomes in the second event multiplied to equal the total number of possible outcomes in the combined event.
																		</p>

																		<example>
																			<p>
																				In your drawer you have 10 pairs of socks, 6 of which are white, and 7 tee shirts, 3 of which are white.
																				If you randomly reach in and pull out a pair of socks and a tee shirt, what is the probability both are white?
																			</p>

																			<p>
																				The probability of choosing a white pair of socks is <m>\frac{6}{10}</m>.
																			</p>

																			<p>
																				The probability of choosing a white tee shirt is  <m>\frac{3}{7}</m>.
																			</p>

																			<p>
																				The probability of both being white is <m> \frac{6}{10} \cdot \frac{3}{7} = \frac{18}{70} = \frac{9}{35}</m>
																			</p>
																		</example>

																		<exploration>
																			<p>
																				A card is pulled a deck of cards and noted.
																				The card is then replaced, the deck is shuffled, and a second card is removed and noted.
																				What is the probability that both cards are Aces?
																			</p>
																		</exploration>

																		<p>
																			The previous examples looked at the probability of both events occurring.
																			Now we will look at the probability of either event occurring.
																		</p>

																		<example>
																			<p>
																				Suppose we flipped a coin and rolled a die, and wanted to know the probability of getting a head on the coin or a 6 on the die.
																			</p>

																			<p>
																				Here, there are still 12 possible outcomes: {H1,H2,H3,H4,H5,H6,T1,T2,T3,T4,T5,T6}
																			</p>

																			<p>
																				By simply counting, we can see that 7 of the outcomes have a head on the coin or a 6 on the die or both – we use or inclusively here (these 7 outcomes are H1, H2, H3, H4, H5, H6, T6), so the probability is <m> \frac{7}{12}</m>.
																				How could we have found this from the individual probabilities?
																			</p>

																			<p>
																				As we would expect, <m> \frac{1}{2} </m> of these outcomes have a head, and <m> \frac{1}{6} </m> of these outcomes have a 6 on the die.
																				If we add these, <m> \frac{1}{2} + \frac{1}{6} = \frac{6}{12}+ \frac{2}{12} = \frac{8}{12}</m>, which is not the correct probability.
																				Looking at the outcomes we can see why:  the outcome H6 would have been counted twice, since it contains both a head and a 6; the probability of both a head and rolling a 6 is <m> \frac{1}{12} </m>.
																			</p>

																			<p>
																				If we subtract out this double count, we have the correct probability: <m> \frac{8}{12}-\frac{1}{12} = \frac{7}{12} </m>.
																			</p>
																		</example>

																		<assemblage>
																			<title><m>P(A \text{ or } B)</m></title>
																			<p>
																				The The probability of either A or B occurring (or both) is
																				<me>
																					P(A \text{or} B) = P(A) + P(B) - P(A \text{ and } B)
																				</me>
																			</p>
																		</assemblage>

																		<example>
																			<p>
																				Suppose we draw one card from a standard deck.
																				What is the probability that we get a Queen or a King?
																			</p>

																			<p>
																				There are 4 Queens and 4 Kings in the deck, hence 8 outcomes corresponding to a Queen or King out of 52 possible outcomes.
																				Thus the probability of drawing a Queen or a King is: <m>P(\text{King and Queen}) = \frac{8}{52} </m>.
																			</p>

																			<p>
																				Note that in this case, there are no cards that are both a Queen and a King, so  <m>P(\text{King and Queen}) = 0</m>.
																				Using our probability rule, we could have said:
																				<me>
																					P(\text{King or Queen}) = P(\text{King}) + P(\text{Queen}) - P(\text{King and Queen}) = \frac{4}{52} + \frac{4}{52} - 0 = \frac{8}{52}
																				</me>
																			</p>
																		</example>

																		<p>
																			In the last example, the events were <term>mutually exclusive</term> so <m>P(A \text{ or } B) = P(A) + P(B)</m>.
																		</p>

																		<example>
																			<p>
																				Suppose we draw one card from a standard deck.
																				What is the probability that we get a red card or a King?
																			</p>

																			<p>
																				Half the cards are red, so <m>P(\text{red}) = \frac{26}{52}</m>
																			</p>

																			<p>
																				There are four kings, so <m>P(\text{King}) = \frac{4}{52}</m>
																			</p>

																			<p>
																				There are two red kings, so <m>P(\text{Red and King}) = \frac{2}{52}</m>
																			</p>

																			<p>
																				We can then calculate
																				<me>
																					P(\text{Red or King}) = P(\text{red}) + P(\text{King}) - P(\text{Red and King}) = \frac{26}{52} + \frac{4}{52} - \frac{2}{52} = \frac{28}{52}
																				</me>
																			</p>
																		</example>

																		<exploration>
																			<p>
																				In your drawer you have 10 pairs of socks, 6 of which are white, and 7 tee shirts, 3 of which are white.
																				If you reach in and randomly grab a pair of socks and a tee shirt, what the probability at least one is white?
																			</p>
																		</exploration>

																		<example>
																			<p>
																				The table below shows the number of survey subjects who have received and not received a speeding ticket in the last year, and the color of their car.
																				Find the probability that a randomly chosen person:
																				<ol>
																					<li>
																						Has a red car <emph>and</emph> got a speeding ticket
																					</li>

																					<li>
																						Has a red car <emph>or</emph> got a speeding ticket.
																					</li>
																				</ol>
																			</p>

																			<sidebyside>
																				<tabular top="minor" left="minor" right="minor" halign="center">
																					<row bottom="minor">
																					<cell>
																					</cell>
																					<cell>
																					Speeding ticket
																					</cell>
																					<cell>
																					No speeding ticket
																					</cell>
																					<cell>
																					Total
																					</cell>
																					</row>
																					<row bottom="minor">
																					<cell>
																					Red car
																					</cell>
																					<cell>
																					15
																					</cell>
																					<cell>
																					135
																					</cell>
																					<cell>
																					150
																					</cell>
																					</row>
																					<row bottom="minor">
																					<cell>
																					Not red car
																					</cell>
																					<cell>
																					45
																					</cell>
																					<cell>
																					470
																					</cell>
																					<cell>
																					515
																					</cell>
																					</row>
																					<row bottom="minor">
																					<cell>
																					Total
																					</cell>
																					<cell>
																					60
																					</cell>
																					<cell>
																					605
																					</cell>
																					<cell>
																					665
																					</cell>
																					</row>
																				</tabular>
																			</sidebyside>

																			<p>
																				We can see that 15 people of the 665 surveyed had both a red car and got a speeding ticket, so the probability is <m>\frac{15}{665} \approx 0.0226</m>.
																			</p>

																			<p>
																				Notice that having a red car and getting a speeding ticket are not independent events, so the probability of both of them occurring is not simply the product of probabilities of each one occurring.
																			</p>

																			<p>
																				We could answer this question by simply adding up the numbers:  15 people with red cars and speeding tickets + 135 with red cars but no ticket + 45 with a ticket but no red car = 195 people.
																				So the probability is  <m> \frac{195}{665} \approx 0.2932</m>.
																			</p>

																			<p>
																				We also could have found this probability by:
																				<me>
																					P(\text{has a red car}) + P(\text{got a speeding ticket}) - P(
																					\text{had a red car and got a speeding ticket}) = \frac{150}{665}+\frac{60}{665}-\frac{15}{665} = \frac{195}{665}
																				</me>.
																			</p>
																		</example>
																	</subsection>

																	<subsection>
																		<title>Conditional Probability</title>
																		<p>
																			Often it is required to compute the probability of an event given that another event has occurred.
																		</p>

																		<example>
																			<p>
																				What is the probability that two cards drawn at random from a deck of playing cards will both be aces?
																			</p>

																			<p>
																				It might seem that you could use the formula for the probability of two independent events and simply multiply <m> \frac{4}{52} \cdot \frac{4}{52} = \frac{1}{169}.</m> This would be incorrect, however, because the two events are not independent.
																				If the first card drawn is an ace, then the probability that the second card is also an ace would be lower because there would only be three aces left in the deck.
																			</p>

																			<p>
																				Once the first card chosen is an ace, the probability that the second card chosen is also an ace is called the <term>conditional probability</term> of drawing an ace.
																				In this case the "condition" is that the first card is an ace.
																				Symbolically, we write this as:
																				<m>P(\text{ace on second draw} \mid \text{an ace on the first draw}).
																				</m>
																			</p>

																			<p>
																				The vertical bar "|" is read as "given," so the above expression is short for "The probability that an ace is drawn on the second draw given that an ace was drawn on the first draw." What is this probability?  After an ace is drawn on the first draw, there are 3 aces out of 51 total cards left.
																				This means that the conditional probability of drawing an ace after one ace has already been drawn is  <m>\frac{3}{51} = \frac{1}{17} </m>.
																			</p>

																			<p>
																				Thus, the probability of both cards being aces is  <m> \frac{4}{52} \cdot \frac{3}{51} = \frac{12}{2652}= \frac{1}{221}</m>.
																			</p>
																		</example>

																		<assemblage>
																			<title>Conditional Probability</title>
																			<p>
																				If events <m>A</m> and <m>B</m> are not independent, then
																				<me>
																					P(A \text{ and } B) = P(A) \cdot P(B \mid A)
																				</me>
																			</p>
																		</assemblage>

																		<example>
																			<p>
																				If you pull 2 cards out of a deck, what is the probability that both are spades?
																			</p>

																			<p>
																				The probability that the first card is a spade is <m>\frac{13}{52}</m>.
																			</p>

																			<p>
																				The probability that the second card is a spade, given the first was a spade, is  <m>\frac{12}{51}</m>, since there is one less spade in the deck, and one less total cards.
																			</p>

																			<p>
																				The probability that both cards are spades is <m>\frac{13}{52}\cdot \frac{12}{51} = \frac{156}{2652} \approx 0.0588</m>.
																			</p>
																		</example>

																		<example>
																			<p>
																				If you draw two cards from a deck, what is the probability that you will get the Ace of Diamonds and a black card?
																			</p>

																			<p>
																				You can satisfy this condition by having Case A or Case B, as follows:
																				Case A) you can get the Ace of Diamonds first and then a black card or
																				Case B) you can get a black card first and then the Ace of Diamonds.
																			</p>

																			<p>
																				Let's calculate the probability of Case A.
																				The probability that the first card is the Ace of Diamonds is  <m> \frac{1}{52}</m>.
																				The probability that the second card is black given that the first card is the Ace of Diamonds is <m> \frac{26}{51} </m> because 26 of the remaining 51 cards are black.
																				The probability is therefore  <m> \frac{1}{52} \cdot \frac{26}{51} = \frac{1}{102}.</m>
																			</p>

																			<p>
																				Now for Case B: the probability that the first card is black is  <m>\frac{26}{52} = \frac{1}{2}</m>.
																				The probability that the second card is the Ace of Diamonds given that the first card is black is <m> \frac{1}{51}</m>.
																				The probability of Case B is therefore <m>\frac{1}{2} \cdot \frac{1}{51} = \frac{1}{102}</m>, the same as the probability of Case 1.
																			</p>

																			<p>
																				Recall that the probability of A or B is <m>P(A) + P(B) - P(A \text{ and} B)</m>.
																				In this problem, <m>P(A \text{ and } B) = 0</m> since the first card cannot be the Ace of Diamonds and be a black card.
																				Therefore, the probability of Case A or Case B is  <m> \frac{1}{102} + \frac{1}{102} = \frac{2}{102} = \frac{1}{51}</m>.
																				The probability that you will get the Ace of Diamonds and a black card when drawing two cards from a deck is <m>\frac{1}{51}</m>.
																			</p>
																		</example>

																		<exploration>
																			<p>
																				In your drawer you have 10 pairs of socks, 6 of which are white.
																				If you reach in and randomly grab two pairs of socks, what is the probability that both are white?
																			</p>
																		</exploration>

																		<example>
																			<p>
																				A home pregnancy test was given to women, then pregnancy was verified through blood tests.
																				The following table shows the home pregnancy test results.
																				Find
																				<ol>
																					<li>
																						<m>P(\text{not pregnant} \mid \text{positive test result})</m>
																					</li>

																					<li>
																						<m>P(\text{positive test result} \mid \text{not pregnant}) </m>
																					</li>
																				</ol>
																			</p>

																			<sidebyside>
																				<tabular top="minor" left="minor" right="minor" halign="center">
																					<row bottom="minor">
																					<cell>
																					</cell>
																					<cell>
																					Positive test
																					</cell>
																					<cell>
																					Negative test
																					</cell>
																					<cell>
																					Total
																					</cell>
																					</row>
																					<row bottom="minor">
																					<cell>
																					Pregnant
																					</cell>
																					<cell>
																					70
																					</cell>
																					<cell>
																					4
																					</cell>
																					<cell>
																					74
																					</cell>
																					</row>
																					<row bottom="minor">
																					<cell>
																					Not pregnant
																					</cell>
																					<cell>
																					5
																					</cell>
																					<cell>
																					14
																					</cell>
																					<cell>
																					19
																					</cell>
																					</row>
																					<row bottom="minor">
																					<cell>
																					Total
																					</cell>
																					<cell>
																					75
																					</cell>
																					<cell>
																					18
																					</cell>
																					<cell>
																					93
																					</cell>
																					</row>
																				</tabular>
																			</sidebyside>

																			<p>
																				<ol>
																					<li>
																						Since we know the test result was positive, we're limited to the 75 women in the first column, of which 5 were not pregnant.
																						<me>
																							P(\text{not pregnant}
																							\mid \text{positive test result}) = \frac{5}{75} \approx 0.067 .
																						</me>
																					</li>

																					<li>
																						Since we know the woman is not pregnant, we are limited to the 19 women in the second row, of which 5 had a positive test.
																						<me>
																							P(\text{positive test result} \mid \text{not pregnant}) = \frac{5}{19} \approx 0.263.
																						</me>
																					</li>
																				</ol>
																			</p>

																			<p>
																				The second result is what is usually called a false positive:  A positive result when the woman is not actually pregnant.
																			</p>
																		</example>
																	</subsection>
																	-->
																</section>
