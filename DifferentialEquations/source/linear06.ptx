<?xml version="1.0" encoding="UTF-8"?>
<!-- This file is part of the book                                 -->
<!--                                                               -->
<!--    Ordinary Differential Equations Project                    -->
<!--                                                               -->
<!-- Copyright (C) 2013-2022 Thomas W. Judson                      -->
<!-- See the file COPYING for copying conditions.                  -->
<section xml:id="linear06" xmlns:xi="http://www.w3.org/2001/XInclude">
	<title>Changing Coordinates</title>
	<objectives>
		<ul>
			<li>
				<p>
					To understand that a linear map <m>T</m> converts solutions of <m>{\mathbf y}' = (T^{-1} A T) {\mathbf y}</m> to solutions of <m>{\mathbf x}' = A {\mathbf x}</m>, and, conversely, the inverse of a linear map <m>T</m> takes solutions of <m>{\mathbf x}' = A {\mathbf x}</m> to solutions of <m>{\mathbf y}' = (T^{-1} A T) {\mathbf y}</m>.
				</p>
			</li>

			<li>
				<p>
					To understand that a change of coordinates converts the system <m>{\mathbf x}' = A {\mathbf x}</m> to one of the following special cases,
					<me>
						\begin{pmatrix}
						\lambda &amp; 0 \\
						0 &amp; \mu
						\end{pmatrix},
						\begin{pmatrix}
						\alpha &amp;  \beta \\
						-\beta &amp; \alpha
						\end{pmatrix},
						\begin{pmatrix}
						\lambda &amp; 0 \\
						0 &amp; \lambda
						\end{pmatrix},
						\begin{pmatrix}
						\lambda &amp; 1 \\
						0 &amp; \lambda
						\end{pmatrix}.
					</me>
				</p>
			</li>
		</ul>
	</objectives>

	<introduction>
		<p>
			In the previous sections of this chapter, we outlined procedures for solving systems of linear differential equations of the form
			<me>
				\begin{pmatrix}
				dx/dt \\ dy/dt
				\end{pmatrix}
				=
				\begin{pmatrix}
				a &amp; b \\
				c &amp; d
				\end{pmatrix}
				\begin{pmatrix}
				x \\ y
				\end{pmatrix}
				=
				A
				\begin{pmatrix}
				x \\ y
				\end{pmatrix}
			</me>
			by determining the eigenvalues of <m>A</m>.
			In this section we will consider the following special cases for <m>A</m>,
			<men xml:id="linear06-equation-special-systems">\begin{pmatrix}
				\lambda &amp; 0 \\
				0 &amp; \mu
				\end{pmatrix},
				\begin{pmatrix}
				\alpha &amp;  \beta \\
				-\beta &amp; \alpha
				\end{pmatrix},
				\begin{pmatrix}
				\lambda &amp; 0 \\
				0 &amp; \lambda
				\end{pmatrix},
				\begin{pmatrix}
				\lambda &amp; 1 \\
				0 &amp; \lambda
				\end{pmatrix}.
			</men>
			Although it may seem that we have limited ourselves by attacking only a very small part of the problem of finding solutions for <m>\mathbf x' = A \mathbf x</m>, we are actually very close to providing a complete classification of all solutions.
			We will now show that we can transform any <m>2 \times 2 </m> system of first-order linear differential equations with constant coefficients into one of these special systems by using a change of coordinates.
		</p>
	</introduction>

	<subsection xml:id="linear06-subsection-linear-maps">
		<title>Linear Maps</title>
		<p>
			First, we need to add a few things to our knowledge of matrices and linear algebra.
			A <term>linear map</term><idx>linear map</idx> or <term>linear transformation</term><idx>linear transformation</idx> on <m>{\mathbb R}^2</m> is a function <m>T: {\mathbb R}^2 \to {\mathbb R}^2</m> that is defined by a matrix.
			That is,
			<me>
				T
				\begin{pmatrix}
				x \\ y
				\end{pmatrix}
				=
				\begin{pmatrix}
				a &amp; b \\
				c &amp; d
				\end{pmatrix}
				\begin{pmatrix}
				x \\ y
				\end{pmatrix}.
			</me>
			When there is no confusion, we will think of the linear map <m>T: {\mathbb R}^2 \to {\mathbb R}^2</m> and the matrix
			<me>
				\begin{pmatrix} a &amp; b \\ c &amp; d \end{pmatrix}
			</me>
			as interchangeable.
		</p>

		<p>
			We will say that <m>T: {\mathbb R}^2 \to {\mathbb R}^2</m> is an <term>invertible linear map</term><idx><h>linear map</h><h>invertible</h></idx> if we can find a second linear map <m>S</m> such that <m>T \circ S = S \circ T = I</m>, where <m>I</m> is the identity transformation.
			In terms of matrices, this means that we can find a matrix <m>S</m> such that
			<me>
				TS = ST = I,
			</me>
			where
			<me>
				I = \begin{pmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{pmatrix}
			</me>
			is the <m>2 \times 2</m> identity matrix.
			We write <m>T^{-1}</m> for the inverse matrix of <m>T</m>.
			It is easy to check that the inverse of
			<me>
				T = \begin{pmatrix} a &amp; b \\ c &amp; d \end{pmatrix}
			</me>
			is
			<me>
				T^{-1} = \frac{1}{\det T} \begin{pmatrix} d &amp; - b \\ -c &amp; a \end{pmatrix}.
			</me>
		</p>

		<theorem xml:id="linear06-theorem-invertible-linear-maps">
			<statement>
				<p>
					A linear map <m>T</m> is invertible if and only if <m>\det T \neq 0</m>.
				</p>
			</statement>


			<proof>
				<p>
					If <m>\det T = 0</m>, then there are infinitely many nonzero vectors <m>{\mathbf x}</m> such that <m>T {\mathbf x} = {\mathbf 0}</m>.
					Suppose that <m>T^{-1}</m> exists and <m>{\mathbf x} \neq {\mathbf 0}</m> such that <m>T {\mathbf x} = {\mathbf 0}</m>.
					Then
					<me>
						{\mathbf x} = T^{-1} T {\mathbf x} = T^{-1} {\mathbf 0} = {\mathbf 0},
					</me>
					which is a contradiction.
					On the other hand, we can certainly compute <m>T^{-1}</m>, at least in the <m>2 \times 2</m> case, if the determinant is nonzero.
				</p>
			</proof>
		</theorem>
	</subsection>

	<subsection xml:id="linear06-subsection-changing-coordinates">
		<title>Changing Coordinates</title>
		<p>
			In <xref ref="linear01-subsection-linear-independence" />, we discussed what a basis was along with the coordinates with respect to a particular basis.
			The vectors <m>{\mathbf e}_1 = (1, 0)</m> and <m>{\mathbf e}_2 = (0, 1)</m> form a basis for <m>{\mathbb R}^2</m>.
			Indeed, if <m>{\mathbf z} = (-5, -4)</m>, then we can write
			<me>
				{\mathbf z} = -5 {\mathbf e}_1 - 4 {\mathbf e}_2.
			</me>
			We say that the coordinates of <m>\mathbf z</m> with respect to the basis <m>\{ {\mathbf e}_1, {\mathbf e}_2 \}</m> are <m>(-5,-4)</m>.
			Now consider the vectors <m>{\mathbf v}_1 = (2,1)</m> and <m>{\mathbf v}_2 = (3, 2)</m>.
			Since
			<me>
				\det
				\begin{pmatrix}
				2 &amp; 3 \\
				1 &amp; 2
				\end{pmatrix}
				\neq 0,
			</me>
			these vectors are linearly independent form a different basis for <m>{\mathbb R}^2</m>.
			If <m>{\mathbf z} = (-5, -4)</m>, then we can write
			<me>
				{\mathbf z} = 2 {\mathbf v}_1 - 3 {\mathbf v}_2.
			</me>
			The coordinates of <m>{\mathbf z}</m> with respect to the basis <m>\{ {\mathbf v}_1, {\mathbf v}_2 \}</m> are <m>(2, -3)</m>.
		</p>

		<p>
			Suppose we wish to convert the coordinates with repect to one basis to a new set of coordinates with respect to a different basis; that is, we wish to do a <term>change of coordinates</term><idx><h>coordiantes</h><h>change of</h></idx>.
			Observe that
			<md>
				<mrow>\mathbf v_1  &amp; = 2 \mathbf e_1 + \mathbf e_2</mrow>
				<mrow>\mathbf v_2 &amp; = 3 \mathbf e_1 + 2 \mathbf e_2.</mrow>
			</md>
			It follows that
			<md>
				<mrow>c_1 \mathbf v_1 + c_2 \mathbf v_2   &amp; = c_1 (2 \mathbf e_1 + \mathbf e_2) + c_2( 3\mathbf e_1 + 2 \mathbf e_2)</mrow>
				<mrow>&amp; = (2c_1 + 3c_2 )\mathbf e_1 + (c_2 + 2c_2) \mathbf e_2.</mrow>
			</md>
			Thus, the coordinates of <m>c_1 \mathbf v_1 + c_2 \mathbf v_2</m> with respect to the basis <m>\{ {\mathbf e}_1, {\mathbf e}_2 \}</m> can be determined by
			<me>
				\begin{pmatrix} 2c_1 + 3c_2 \\ c_2 + 2c_2 \end{pmatrix}
				=
				\begin{pmatrix} 2 &amp; 3 \\ 1 &amp; 2 \end{pmatrix}
				\begin{pmatrix} c_1 \\ c_2 \end{pmatrix}.
			</me>
			If we let
			<me>
				T = \begin{pmatrix} 2 &amp; 3 \\ 1 &amp; 2 \end{pmatrix} \quad \text{and} \quad \mathbf c = \begin{pmatrix} c_1 \\ c_2 \end{pmatrix},
			</me>
			then the coordinates with respect to the basis <m>\{ {\mathbf e}_1, {\mathbf e}_2 \}</m> are given by <m>\mathbf d = T \mathbf c</m>.
			If we are given the coordinates with respect to the basis <m>\{ {\mathbf v}_1, {\mathbf v}_2 \}</m> for a vector, we simply need to multiply by the matrix <m>T</m>.
		</p>

		<p>
			Now suppose that we wish to find the coordinates with respect to the basis <m>\{ \mathbf v_1, \mathbf v_2\}</m> if we know that a vector <m>\mathbf z = d_1 \mathbf e_1 + d_2 \mathbf e_2</m>.
			Since <m>\mathbf d = T \mathbf c</m>, we need only multiply both sides of the equation by <m>T^{-1}</m> to get <m>\mathbf c = T^{-1} \mathbf d</m>.
			In our example,
			<me>
				T^{-1} \mathbf d
				= \begin{pmatrix} 2 &amp; - 3 \\ -1 &amp; 2\end{pmatrix} \begin{pmatrix} d_1 \\ d_2 \end{pmatrix}.
			</me>
			In our particular example,
			<me>
				T^{-1} \mathbf d  = \begin{pmatrix} 2 &amp; - 3 \\ -1 &amp; 2\end{pmatrix} \begin{pmatrix} -5 \\ -4 \end{pmatrix} = \begin{pmatrix} 2 \\ -3 \end{pmatrix},
			</me>
			which are the coordinates of <m>\mathbf z</m> with respect to the basis <m>\{ {\mathbf v}_1, {\mathbf v}_2 \}</m>.
		</p>
	</subsection>

	<subsection xml:id="linear06-subsection-changing-coordinates-system">
		<title>Systems and Changing Coordinates</title>
		<p>
			The idea now is to use a change of coordinates to convert an arbitrary system <m>{\mathbf x}' = A {\mathbf x}</m> into one of the special systems mentioned at the beginning of the section <xref ref="linear06-equation-special-systems" />, solve the new system, and then convert our new solution back to a solution of the original system using another change of coordinates.
		</p>

		<p>
			Suppose that we consider a linear system
			<men xml:id="linear06-equation-change-of-coordinates">{\mathbf y}' = (T^{-1} A T) {\mathbf y}
			</men>
			where <m>T</m> is an invertible matrix.
			If <m>{\mathbf y}(t)</m> is a solution of <xref ref="linear06-equation-change-of-coordinates" />, we claim that <m>{\mathbf x}(t) = T {\mathbf y}(t)</m> solves the equation <m>{\mathbf x}' = A {\mathbf x}</m>.
			Indeed,
			<md>
				<mrow>{\mathbf x}'(t) &amp; = (T {\mathbf y})'(t)</mrow>
				<mrow>&amp; = T {\mathbf y}'(t)</mrow>
				<mrow>&amp; = T( (T^{-1} A T) {\mathbf y}(t))</mrow>
				<mrow>&amp; = A (T {\mathbf y}(t))</mrow>
				<mrow>&amp; =  A {\mathbf x}(t).</mrow>
			</md>
			We can think of this in two ways.
			<ol>
				<li>
					<p>
						A linear map <m>T</m> converts solutions of <m>{\mathbf y}' = (T^{-1} A T) {\mathbf y}</m> to solutions of <m>{\mathbf x}' = A {\mathbf x}</m>.
					</p>
				</li>

				<li>
					<p>
						The inverse of a linear map <m>T</m> takes solutions of <m>{\mathbf x}' = A {\mathbf x}</m> to solutions of <m>{\mathbf y}' = (T^{-1} A T) {\mathbf y}</m>.
					</p>
				</li>
			</ol>W
			e are now in a position to solve our problem of finding solutions of an arbitrary linear system
			<me>
				\begin{pmatrix}
				x' \\ y'
				\end{pmatrix}
				=
				\begin{pmatrix}
				a &amp; b \\
				c &amp; d
				\end{pmatrix}
				=
				\begin{pmatrix}
				x \\ y
				\end{pmatrix}.
			</me>
		</p>
	</subsection>

	<subsection xml:id="linear06-subsection-distinct-real-eigenvalues">
		<title>Distinct Real Eigenvalues</title>
		<p>
			Consider the system <m>{\mathbf x}' = A {\mathbf x}</m>, where <m>A</m> has two real, distinct eigenvalues <m>\lambda_1</m> and <m>\lambda_2</m> with eigenvectors <m>{\mathbf v}_1</m> and <m>{\mathbf v}_2</m>, respectively.
			Let <m>T</m> be the matrix with columns <m>{\mathbf v}_1</m> and <m>{\mathbf v}_2</m>.
			If <m>{\mathbf e}_1 = (1, 0)</m> and <m>{\mathbf e}_2 = (0, 1)</m>, then <m>T {\mathbf e}_i = {\mathbf v}_i</m> for <m>i = 1, 2</m>.
			Consequently, <m>T^{-1} {\mathbf v}_i = {\mathbf e}_i</m> for <m>i = 1, 2</m>.
			Thus, we have
			<md>
				<mrow>(T^{-1} A T) {\mathbf e}_i &amp; = T^{-1} A  {\mathbf v}_i</mrow>
				<mrow>&amp; = T^{-1} (\lambda_i  {\mathbf v}_i)</mrow>
				<mrow>&amp; = \lambda_i T^{-1} {\mathbf v}_i</mrow>
				<mrow>&amp; = \lambda_i {\mathbf e}_i</mrow>
			</md>
			for <m>i = 1, 2</m>.
			Therefore, the matrix <m>T^{-1} A T</m> is in canonical form,
			<me>
				T^{-1} A T = \begin{pmatrix} \lambda_1 &amp; 0 \\ 0 &amp; \lambda_2 \end{pmatrix}.
			</me>
			The eigenvalues of the matrix <m>T^{-1} A T</m> are <m>\lambda_1</m> and <m>\lambda_2</m> with eigenvectors <m>(1, 0)</m> and <m>(0, 1)</m>, respectively.
			Thus, the general solution of
			<me>
				{\mathbf y}' = (T^{-1}AT) {\mathbf y}
			</me>
			is
			<me>
				{\mathbf y}(t)
				=
				\alpha e^{\lambda_1 t}
				\begin{pmatrix} 1 \\ 0 \end{pmatrix}
				+
				\beta e^{\lambda_2 t}
				\begin{pmatrix} 0\\ 1 \end{pmatrix}.
			</me>
			Hence, the general solution of
			<me>
				{\mathbf x}' = A {\mathbf x}
			</me>
			is
			<md>
				<mrow>T {\mathbf y}(t)
				&amp; = T \left(
				\alpha e^{\lambda_1 t}
				\begin{pmatrix} 1 \\ 0 \end{pmatrix}
				+
				\beta e^{\lambda_2 t}
				\begin{pmatrix} 0 \\ 1 \end{pmatrix}
				\right)</mrow>
				<mrow>&amp; =
				\alpha e^{\lambda_1 t}
				T \begin{pmatrix} 1 \\ 0\end{pmatrix}
				+
				\beta e^{\lambda_2 t}
				T \begin{pmatrix} 0 \\ 1 \end{pmatrix}</mrow>
				<mrow>&amp; =
				\alpha e^{\lambda_2 t} \mathbf v_1 + \beta e^{\lambda_2 t} \mathbf v_2.</mrow>
			</md>
		</p>

		<example>
			<p>
				Suppose <m>d{\mathbf x}/dt = A {\mathbf x}</m>, where
				<me>
					A = \begin{pmatrix} 1 &amp; 2 \\ 4 &amp; 3 \end{pmatrix}.
				</me>
				The eigenvalues of <m>A</m> are <m>\lambda_1 = 5</m> and <m>\lambda_2 = -1</m> and the associated eigenvectors are <m>(1, 2)</m> and <m>(1, -1)</m>, respectively.
				In this case, our matrix <m>T</m> is
				<me>
					\begin{pmatrix} 1 &amp; 1 \\ 2 &amp; -1 \end{pmatrix}.
				</me>
				If <m>{\mathbf e}_1 = (1, 0)</m> and <m>{\mathbf e}_2 = (0, 1)</m>, then <m>T {\mathbf e}_i = {\mathbf v}_i</m> for <m>i = 1, 2</m>.
				Consequently, <m>T^{-1} {\mathbf v}_i = {\mathbf e}_i</m> for <m>i = 1, 2</m>, where
				<me>
					T^{-1} = \begin{pmatrix} 1/3 &amp; 1/3 \\ 2/3 &amp; -1/3 \end{pmatrix}.
				</me>
				Thus,
				<me>
					T^{-1} A T
					=
					\begin{pmatrix}
					1/3 &amp; 1/3 \\
					2/3 &amp; -1/3
					\end{pmatrix}
					\begin{pmatrix}
					1 &amp; 2 \\
					4 &amp; 3
					\end{pmatrix}
					\begin{pmatrix}
					1 &amp; 1 \\
					2 &amp; -1
					\end{pmatrix}
					=
					\begin{pmatrix}
					5 &amp; 0 \\
					0 &amp; -1
					\end{pmatrix}.
				</me>
				The eigenvalues of the matrix
				<me>
					\begin{pmatrix} 5 &amp; 0 \\ 0 &amp; -1 \end{pmatrix}
				</me>
				are <m>\lambda_1 = 5</m> and <m>\lambda_2 = -1</m> with eigenvectors <m>(1, 0)</m> and <m>(0, 1)</m>, respectively.
				Thus, the general solution of
				<me>
					{\mathbf y}' = (T^{-1}AT) {\mathbf y}
				</me>
				is
				<me>
					{\mathbf y}(t)
					=
					\alpha e^{5t}
					\begin{pmatrix} 1 \\ 0 \end{pmatrix}
					+
					\beta e^{-t}
					\begin{pmatrix} 0\\ 1 \end{pmatrix}.
				</me>
				Hence, the general solution of
				<me>
					{\mathbf x}' = A {\mathbf x}
				</me>
				is
				<md>
					<mrow>T {\mathbf y}(t)
					&amp; =
					\begin{pmatrix}
					1 &amp; 1 \\
					2 &amp; -1
					\end{pmatrix}
					\left(
					\alpha e^{5t}
					\begin{pmatrix} 1 \\ 0 \end{pmatrix}
					+
					\beta e^{-t}
					\begin{pmatrix} 0 \\ 1 \end{pmatrix}
					\right)</mrow>
					<mrow>&amp; =
					\alpha e^{5t}
					\begin{pmatrix} 1 \\ 2 \end{pmatrix}
					+
					\beta e^{-t}
					\begin{pmatrix} 1 \\ -1 \end{pmatrix}</mrow>
				</md>
				The linear map <m>T</m> converts the phase portrait of the system <m>{\mathbf y}' = (T^{-1}AT) {\mathbf y}</m> (<xref ref="linear06-figure-change-of-coordinates-1" />) to the phase portrait of the system <m>{\mathbf x}' = A {\mathbf x}</m> (<xref ref="linear06-figure-change-of-coordinates-2" />).
			</p>

			<figure xml:id="linear06-figure-change-of-coordinates-1">
				<caption>Phase portrait for <m>{\mathbf y}' = (T^{-1}AT) {\mathbf y}</m></caption>
				<image width="60%" xml:id="linear06-change-of-coordinates-1">
					<description>
					<p>
						a direction field of slope arrows and solution curves in each quadrant with the solution curves approaching the horizontal and vertical axes for large values
					</p>
					</description>
					<sageplot>
                        x, y ,t = var('x y t')
                        F = [5*x, -y]
                        n = sqrt(F[0]^2 + F[1]^2)
                        p = plot_vector_field((F[0]/n, F[1]/n), (x,-3,3), (y,-3,3), aspect_ratio=1)
                        P1=desolve_system_rk4(F,[x,y],ics=[0,0.01,3],ivar=t,end_points=1.15,step=0.1)
                        P2=desolve_system_rk4(F,[x,y],ics=[0,0.01,-3],ivar=t,end_points=1.15,step=0.1)
                        P3=desolve_system_rk4(F,[x,y],ics=[0,-0.01,3],ivar=t,end_points=1.15,step=0.1)
                        P4=desolve_system_rk4(F,[x,y],ics=[0,-0.01,-3],ivar=t,end_points=1.15,step=0.1)
                        S1=[ [j, k] for i,j,k in P1]
                        S2=[ [j, k] for i,j,k in P2]
                        S3=[ [j, k] for i,j,k in P3]
                        S4=[ [j, k] for i,j,k in P4]
                        p += line(S1, thickness=2, axes_labels=['$x(t)$','$y(t)$'], fontsize=12)
                        p += line(S2, thickness=2)
                        p += line(S3, thickness=2)
                        p += line(S4, thickness=2)
                        p
					</sageplot>
				</image>
			</figure>

			<figure xml:id="linear06-figure-change-of-coordinates-2">
				<caption>Phase portrait for <m>{\mathbf x}' = A {\mathbf x}</m></caption>
				<image width="60%" xml:id="linear06-change-of-coordinates-2">
					<description>
					<p>
						a direction field of slope arrows and solution curves that approach the straight-line solutions for large values
					</p>
					</description>
					<sageplot>
                        x, y ,t = var('x y t')
                        F = [x + 2*y, 4*x + 3*y]
                        n = sqrt(F[0]^2 + F[1]^2)
                        p = plot_vector_field((F[0]/n, F[1]/n), (x,-3,3), (y,-3,3), aspect_ratio=1)
                        P1=desolve_system_rk4(F,[x,y],ics=[0,-3,2.99],ivar=t,end_points=1.28,step=0.1)
                        P2=desolve_system_rk4(F,[x,y],ics=[0,-2.99,3],ivar=t,end_points=1.15,step=0.1)
                        P3=desolve_system_rk4(F,[x,y],ics=[0,3,-2.99],ivar=t,end_points=1.28,step=0.1)
                        P4=desolve_system_rk4(F,[x,y],ics=[0,2.99,-3],ivar=t,end_points=1.15,step=0.1)
                        S1=[ [j, k] for i,j,k in P1]
                        S2=[ [j, k] for i,j,k in P2]
                        S3=[ [j, k] for i,j,k in P3]
                        S4=[ [j, k] for i,j,k in P4]
                        p += line(S1, thickness=2, axes_labels=['$x(t)$','$y(t)$'], fontsize=12)
                        p += line(S2, thickness=2)
                        p += line(S3, thickness=2)
                        p += line(S4, thickness=2)
                        p
					</sageplot>
				</image>
			</figure>
		</example>

		<activity>
			<title>Distinct Real Eigenvalues and Transformation of Coordinates</title>
			<introduction>
				<p>
					Consider the system of linear differential equations <m>d\mathbf x/dt = A \mathbf x</m>, where
					<me>
						A  = \begin{pmatrix} 1 \amp 3 \\ 1 \amp -1 \end{pmatrix}.
					</me>
				</p>
			</introduction>


			<task>
				<statement>
					<p>
						Find the eigenvalues of <m>A</m>.
						You should find distinct real eigenvalues <m>\lambda</m> and <m>\mu</m>.
					</p>
				</statement>
			</task>


			<task>
				<statement>
					<p>
						Find the general solution for <m>d\mathbf x/dt = A \mathbf x</m>.
					</p>
				</statement>
			</task>


			<task>
				<statement>
					<p>
						Construct the <m>2 \times 2</m> matrix <m>T = (\mathbf v_1, \mathbf v_2)</m> and find <m>T^{-1}</m>.
					</p>
				</statement>
			</task>


			<task>
				<statement>
					<p>
						Calculate <m>T^{-1}AT</m>.
						You should obtain the diagonal matrix
						<me>
							\begin{pmatrix}
							\lambda &amp; 0 \\
							0 &amp; \mu
							\end{pmatrix}
						</me>
						with eigenvectors <m>\mathbf e_1 = (1,0)</m> and <m>\mathbf e_2 = (0,1)</m>.
					</p>
				</statement>
			</task>


			<task>
				<statement>
					<p>
						The general solution of
						<me>
							{\mathbf y}' = (T^{-1}AT) {\mathbf y}
						</me>
						is
						<me>
							{\mathbf y}(t)
							=
							\alpha e^{\lambda t}
							\begin{pmatrix} 1 \\ 0 \end{pmatrix}
							+
							\beta e^{\mu t}
							\begin{pmatrix} 0\\ 1 \end{pmatrix}.
						</me>
						Now calculate <m>T \mathbf y</m> and compare this solution with the one that you obtained in <xref ref="linear02-activity-distinct-real-eigenvalues" />.<fn>Of couurse, we have much quicker ways of solving a system <m>d\mathbf x/dt = A \mathbf x</m> with distinct real eigenvalues.
						The goal of this section is show that we have covered all possible cases for <m>2 \times 2</m> systems of linear differential equations and not to invent new methods of solution.</fn>
					</p>
				</statement>
			</task>
		</activity>
	</subsection>

	<subsection xml:id="linear06-subsection-complex-eigenvalues">
		<title>Complex Eigenvalues</title>
		<p>
			Suppose the matrix
			<me>
				A = \begin{pmatrix} a \amp b \\ c \amp d \end{pmatrix}
			</me>
			in system <m>{\mathbf x}' = A {\mathbf x}</m> has complex eigenvalues.
			In this case, the characteristic polynomial <m>p(\lambda) = \lambda^2 - (a + d)\lambda + (ad - bc)</m> will have roots <m>\lambda = \alpha + i \beta</m> and <m>\overline{\lambda} = \alpha - i \beta</m>, where
			<md>
				<mrow>\alpha \amp = \frac{a + d}{2}</mrow>
				<mrow>\beta \amp = \frac{\sqrt{4bc - (a - d)^2}}{2}.</mrow>
			</md>
			The eigenvalues <m>\lambda</m> and <m>\overline{\lambda}</m> are complex conjugates.
			Now, suppose that the eigenvalue <m>\lambda = \alpha + i \beta</m> has an eigenvector of the form
			<me>
				\mathbf v = {\mathbf v}_ 1 + i {\mathbf v}_2,
			</me>
			where <m>\mathbf v_1</m> and <m>\mathbf v_2</m> are real vectors.
			Then <m>\overline{\mathbf v} = {\mathbf v}_ 1 - i {\mathbf v}_2</m> is an eigenvector for <m>\overline{\lambda}</m>, since
			<me>
				A \overline{\mathbf v} = \overline{A \mathbf v} = \overline{\lambda \mathbf v} = \overline{\lambda} \overline{\mathbf v}.
			</me>
			Consequently, if <m>A</m> is a real matrix with complex eigenvalues, one of the eigenvalues determines the other.
		</p>


		<proposition>
			<statement>
				<p>
					If <m>\lambda = \alpha + i \beta</m> is an eigenvalue of a real matrix <m>A</m> with <m>\beta \neq 0</m> and eigenvector the form
					<me>
						{\mathbf v}_ 1 + i {\mathbf v}_2,
					</me>
					where <m>{\mathbf v}_1</m> and <m>{\mathbf v}_2</m> are real vectors, then the vectors <m>{\mathbf v}_1</m> and <m>{\mathbf v}_2</m> are linearly independent.
				</p>
			</statement>


			<proof>
				<p>
					If <m>{\mathbf v}_1</m> and <m>{\mathbf v}_2</m> are not linearly independent, then <m>{\mathbf v}_1 = c {\mathbf v}_2</m> for some <m>c \in \mathbb R</m>.
					On one hand, we have
					<me>
						A ({\mathbf v}_ 1 + i {\mathbf v}_2) = A (c {\mathbf v}_2 + i {\mathbf v}_2) = (c + i) A {\bf v}_2.
					</me>
					However,
					<md>
						<mrow>A ({\mathbf v}_ 1 + i {\mathbf v}_2)  &amp; = (\alpha + i \beta) ( {\mathbf v}_ 1 + i {\mathbf v}_2)</mrow>
						<mrow>&amp; = (\alpha + i \beta) ( c + i) {\mathbf v}_2</mrow>
						<mrow>&amp; = ( c + i) (\alpha + i \beta)  {\mathbf v}_2</mrow>
					</md>
					In other words, <m>A {\mathbf v}_2 = (\alpha + i \beta)  {\mathbf v}_2</m>.
					However, this is a contradiction since the left-side of the equation says that we have real eigenvector while the right-side of the equation is complex.
					Thus, <m>{\mathbf v}_1</m> and <m>{\mathbf v}_2</m> are linearly independent.
				</p>
			</proof>
		</proposition>


		<proposition>
			<statement>
				<p>
					Let <m>A</m> be a real matrix with eigenvalue <m>\lambda = \alpha + i \beta</m>, where <m>\beta \neq 0</m>.
					If
					<me>
						{\mathbf v}_ 1 + i {\mathbf v}_2,
					</me>
					is an eigenvector for <m>\lambda</m>, then there exists a matrix <m>T</m> such that
					<me>
						T^{-1} AT = \begin{pmatrix} \alpha &amp; \beta \\ - \beta &amp; \alpha \end{pmatrix}.
					</me>
				</p>
			</statement>


			<proof>
				<p>
					Since <m>{\mathbf v}_1 + i {\mathbf v}_2</m> is an eigenvector associated to the eigenvalue <m>\alpha + i \beta</m>, we have
					<me>
						A ( {\mathbf v}_1 + i {\mathbf v}_2) = (\alpha + i \beta) ({\mathbf v}_1 + i {\mathbf v}_2).
					</me>
					Equating the real and imaginary parts, we find that
					<md>
						<mrow>A {\mathbf v}_1 &amp; = \alpha {\mathbf v}_1 - \beta {\mathbf v}_2</mrow>
						<mrow>A {\mathbf v}_2 &amp; = \beta {\mathbf v}_1 + \alpha {\mathbf v}_2.</mrow>
					</md>
					If <m>T</m> is the matrix with columns <m>{\mathbf v}_1</m> and <m>{\mathbf v}_2</m>, then
					<md>
						<mrow>T {\mathbf e}_1 &amp; = {\mathbf v}_1</mrow>
						<mrow>T {\mathbf e}_2 &amp; = {\mathbf v}_2.</mrow>
					</md>
					Thus, we have
					<me>
						(T^{-1} A T) {\mathbf e}_1 = T^{-1} (\alpha {\mathbf v}_1 - \beta {\mathbf v}_2) = \alpha {\mathbf e}_1 - \beta {\mathbf e}_2.
					</me>
					Similarly,
					<me>
						(T^{-1} A T) {\mathbf e}_2 = \beta {\mathbf e}_1 + \alpha {\mathbf e}_2.
					</me>
					Therefore, we can write the matrix <m>T^{-1}A T</m> as
					<me>
						T^{-1} AT = \begin{pmatrix} \alpha &amp; \beta \\ - \beta &amp; \alpha \end{pmatrix}.
					</me>
				</p>
			</proof>
		</proposition>

		<p>
			The system <m>{\mathbf y}' = (T^{-1} AT ) {\mathbf y}</m> is in one of the canonical forms and has a phase portrait that is a spiral sink (<m>\alpha \lt 0</m>), a center (<m>\alpha = 0</m>), or a spiral source (<m>\alpha \gt 0</m>).
			After a change of coordinates, the phase portrait of <m>{\mathbf x}' = A {\mathbf x}</m> is equivalent to a sink, center, or source.
		</p>

		<example>
			<p>
				Suppose that we wish to find the solutions of the second order equation
				<me>
					2x'' + 2x' + x = 0.
				</me>
				This particular equation might model a damped harmonic oscillator.
				If we rewrite this second-order equation as a first-order system, we have
				<md>
					<mrow>x' &amp; = y</mrow>
					<mrow>y' &amp; = - \frac{1}{2} x - y,</mrow>
				</md>
				or equivalently <m>\mathbf x' = A \mathbf x</m>, where
				<me>
					A = \begin{pmatrix} 0 &amp; 1 \\ - 1/2 &amp; - 1 \end{pmatrix}.
				</me>
				The eigenvalues of <m>A</m> are
				<me>
					- \frac{1}{2} \pm i \frac{1}{2}.
				</me>
				The eigenvalue <m>\lambda = (1 + i)/2</m> has an eigenvector
				<me>
					\mathbf v =
					\begin{pmatrix}
					2 \\ -1 + i
					\end{pmatrix}
					=
					\begin{pmatrix}
					2 \\ -1
					\end{pmatrix}
					+
					i
					\begin{pmatrix}
					0 \\ 1
					\end{pmatrix},
				</me>
				respectively.
				Therefore, we can take <m>T</m> to be the matrix
				<me>
					T = \begin{pmatrix} 2 \amp 0 \\ -1 \amp 1 \end{pmatrix}.
				</me>
				Consequently,
				<me>
					T^{-1} A T
					=
					\begin{pmatrix}
					1/2 &amp; 0 \\
					1/2 &amp; 1
					\end{pmatrix}
					\begin{pmatrix}
					0 &amp; 1 \\
					-1/2 &amp; -1
					\end{pmatrix}
					\begin{pmatrix}
					2 &amp; 0 \\
					-1 &amp; 1
					\end{pmatrix}
					=
					\begin{pmatrix}
					-1/2 &amp; 1/2 \\
					-1/2 &amp; -1/2
					\end{pmatrix},
				</me>
				which is in the canonical form
				<me>
					\begin{pmatrix} \alpha &amp; \beta \\ - \beta &amp; \alpha \end{pmatrix}.
				</me>
				The general solution to <m>{\mathbf y}' = (T^{-1} A T) {\mathbf y}</m> is
				<me>
					{\mathbf y}(t)
					=
					c_1 e^{-t/2}
					\begin{pmatrix}
					\cos(t/2) \\ -\sin(t/2)
					\end{pmatrix}
					+
					c_2 e^{-t/2}
					\begin{pmatrix}
					\sin(t/2) \\ \cos(t/2)
					\end{pmatrix}.
				</me>
				The phase portrait of <m>{\mathbf y}' = (T^{-1} A T) {\mathbf y}</m> is given in <xref ref="linear06-figure-complex-canonical" />.
			</p>

			<figure xml:id="linear06-figure-complex-canonical">
				<caption>Phase portrait for <m>{\mathbf y}' = (T^{-1} A T) {\mathbf y}</m></caption>
				<image width="60%" xml:id="linear06-complex-canonical">
					<description>
					<p>
						a direction field of slope arrows and solution curves that spiral towards the origin
					</p>
					</description>
					<sageplot>
                        x, y ,t = var('x y t')
                        F = [-x/2 + y/2, -x/2 - y/2]
                        n = sqrt(F[0]^2 + F[1]^2)
                        p = plot_vector_field((F[0]/n, F[1]/n), (x,-3,3), (y,-3,3), aspect_ratio=1)
                        P1 = desolve_system_rk4(F,[x,y],ics=[0,-3,3],ivar=t,end_points=10,step=0.1)
                        P2 = desolve_system_rk4(F,[x,y],ics=[0,-3,-3],ivar=t,end_points=10,step=0.1)
                        P3 = desolve_system_rk4(F,[x,y],ics=[0,3,-3],ivar=t,end_points=10,step=0.1)
                        P4 = desolve_system_rk4(F,[x,y],ics=[0,3,3],ivar=t,end_points=10,step=0.1)
                        S1 = [ [j, k] for i,j,k in P1]
                        S2 = [ [j, k] for i,j,k in P2]
                        S3 = [ [j, k] for i,j,k in P3]
                        S4 = [ [j, k] for i,j,k in P4]
                        p += line(S1, thickness=2, axes_labels=['$x(t)$','$y(t)$'])
                        p += line(S2, thickness=2)
                        p += line(S3, thickness=2)
                        p += line(S4, thickness=2)
                        p
					</sageplot>
				</image>
			</figure>

			<p>
				The general solution of <m>{\mathbf x}' = A {\mathbf x}</m> is
				<md>
					<mrow>T {\mathbf y}(t)
					&amp; =
					\begin{pmatrix}
					2 &amp; 0 \\
					-1 &amp; 1
					\end{pmatrix}
					\left[
					c_1 e^{-t/2}
					\begin{pmatrix}
					\cos(t/2) \\ -\sin(t/2)
					\end{pmatrix}
					+
					c_2 e^{-t/2}
					\begin{pmatrix}
					\sin(t/2) \\ \cos(t/2)
					\end{pmatrix}
					\right]</mrow>
					<mrow>&amp; =
					c_1 e^{-t/2}
					\begin{pmatrix}
					2 &amp; 0 \\
					-1 &amp; 1
					\end{pmatrix}
					\begin{pmatrix}
					\cos(t/2) \\ -\sin(t/2)
					\end{pmatrix}
					+
					c_2 e^{-t/2}
					\begin{pmatrix}
					2 &amp; 0 \\
					-1 &amp; 1
					\end{pmatrix}
					\begin{pmatrix}
					\sin(t/2) \\ \cos(t/2)
					\end{pmatrix}</mrow>
					<mrow>&amp; =
					c_1 e^{-t/2}
					\begin{pmatrix}
					2 \cos(t/2)  \\ - \cos(t/2) - \sin(t/2)
					\end{pmatrix}
					+
					c_2 e^{-t/2}
					\begin{pmatrix}
					2 \sin(t/2) \\ - \sin(t/2) + \cos(t/2)
					\end{pmatrix}.</mrow>
				</md>
				The phase portrait for this system is given in <xref ref="linear06-figure-underdamped-harmonic-oscillator" />.
			</p>

			<figure xml:id="linear06-figure-underdamped-harmonic-oscillator">
				<caption>Phase portrait of <m>{\mathbf x}' = A {\mathbf x}</m></caption>
				<image width="60%" xml:id="linear06-underdamped-harmonic-oscillator">
					<description>
					<p>
						a direction field of slope arrows and solution curves that spiral towards the origin
					</p>
					</description>
					<sageplot>
                        x, y ,t = var('x y t')
                        F = [y, -x/2 - y]
                        n = sqrt(F[0]^2 + F[1]^2)
                        p = plot_vector_field((F[0]/n, F[1]/n), (x,-3,3), (y,-3,3), aspect_ratio=1)
                        P1 = desolve_system_rk4(F,[x,y],ics=[0,-3,3],ivar=t,end_points=10,step=0.1)
                        P2 = desolve_system_rk4(F,[x,y],ics=[0,-1,-3],ivar=t,end_points=10,step=0.1)
                        P3 = desolve_system_rk4(F,[x,y],ics=[0,3,-3],ivar=t,end_points=10,step=0.1)
                        P4 = desolve_system_rk4(F,[x,y],ics=[0,1.5,3],ivar=t,end_points=10,step=0.1)
                        S1 = [ [j, k] for i,j,k in P1]
                        S2 = [ [j, k] for i,j,k in P2]
                        S3 =[ [j, k] for i,j,k in P3]
                        S4 = [ [j, k] for i,j,k in P4]
                        p += line(S1, thickness=2, axes_labels=['$x(t)$','$y(t)$'])
                        p += line(S2, thickness=2)
                        p += line(S3, thickness=2)
                        p += line(S4, thickness=2)
                        p
					</sageplot>
				</image>
			</figure>
		</example>

		<remark>
			<p>
				Of course, we have a much more efficient way of solving the system <m>{\mathbf x}' = A {\mathbf x}</m>, where
				<me>
					A = \begin{pmatrix} 0 &amp; 1 \\ - 1/2 &amp; - 1 \end{pmatrix}.
				</me>
				Since <m>A</m> has eigenvalue <m>\lambda = (-1 + i)/2</m> with an eigenvector <m>\mathbf v = (2, -1 + i)</m>, we can apply Euler's formula and write the solution as
				<md>
					<mrow>\mathbf x(t) \amp = e^{(-1 + i)t/2} \mathbf v \amp</mrow>
					<mrow>\amp = e^{-t/2} e^{it/2} \begin{pmatrix} 2 \\ -1 + i \end{pmatrix}</mrow>
					<mrow>\amp = e^{-t/2} (\cos(t/2) + i \sin(t/2)) \begin{pmatrix} 2 \\ -1 + i \end{pmatrix}</mrow>
					<mrow>\amp =e^{-t/2} \begin{pmatrix} 2 \cos(t/2) \\ - \cos(t/2) - \sin(t/2) \end{pmatrix} + i e^{-t/2}  \begin{pmatrix} 2 \sin(t/2) \\ -\sin(t/2) + \cos(t/2) \end{pmatrix}.</mrow>
				</md>
				Taking the real and the imaginary parts of the last expression, the general solution of <m>{\mathbf x}' = A {\mathbf x}</m> is
				<me>
					\mathbf x(t) = c_1 e^{-t/2}
					\begin{pmatrix}
					2 \cos(t/2)  \\ - \cos(t/2) - \sin(t/2)
					\end{pmatrix}
					+
					c_2 e^{-t/2}
					\begin{pmatrix}
					2 \sin(t/2) \\ - \sin(t/2) + \cos(t/2)
					\end{pmatrix},
				</me>
				which agrees with the solution that we found by transforming coordinates.
			</p>
		</remark>
	</subsection>

	<subsection xml:id="linear06-subsection-repeated-eigenvalues">
		<title>Repeated Eigenvalues</title>
		<p>
			Now suppose that <m>A</m> has a single real eigenvalue <m>\lambda</m>.
			Then the characteristic polynomial of <m>A</m> is  <m>p(\lambda) = \lambda^2 - (a + d)\lambda + (ad - bc)</m>, then <m>A</m> has an eigenvalue <m>\lambda = (a + d)/2</m>.
		</p>


		<proposition>
			<statement>
				<p>
					If  <m>A</m> has a single eigenvalue and a pair of linearly independent eigenvectors, then <m>A</m> must be of the form
					<me>
						A = \begin{pmatrix} \lambda &amp; 0 \\ 0 &amp; \lambda \end{pmatrix}.
					</me>
				</p>
				<!-- Zach, please sanity check this.  I wrapped the statement in paragraph so it wouldn't throw an error.  Is that how we want the book to look? -->
			</statement>


			<proof>
				<p>
					Suppose that <m>{\mathbf u}</m> and <m>{\mathbf v}</m> are linearly indeendent eigenvectors for <m>A</m>, and let <m>T</m> be the matrix whose first column is <m>{\mathbf u}</m> and second column is <m>{\mathbf v}</m>.
					That is, <m>T {\mathbf e}_1 = {\mathbf u}</m> and <m>T{\mathbf e}_2 = {\mathbf v}</m>.
					Since <m>{\mathbf u}</m> and <m>{\mathbf v}</m> are linearly independent, <m>\det(T) \neq 0</m> and <m>T</m> is invertible.
					So, it must be the case that
					<me>
						AT = (A {\mathbf u}, A {\mathbf v}) = (\lambda {\mathbf u}, \lambda {\mathbf v}) = \lambda ({\mathbf u}, {\mathbf v}) = \lambda IT,
					</me>
					or
					<me>
						A = \begin{pmatrix} \lambda &amp; 0 \\ 0 &amp; \lambda \end{pmatrix}.
					</me>
				</p>
			</proof>
		</proposition>

		<p>
			In this case, the system is uncoupled and is easily solved.
			That is, we can solve each equation in the system
			<md>
				<mrow>x' \amp = \lambda x</mrow>
				<mrow>y' \amp = \lambda y</mrow>
			</md>
			separately to obtain the general solution
			<md>
				<mrow>x \amp = c_1 e^{\lambda t}</mrow>
				<mrow>y' \amp = c_2 e^{\lambda t}.</mrow>
			</md>
		</p>


		<proposition>
			<statement>
				<p>
					Suppose that <m>A</m> has a single eigenvalue <m>\lambda</m>.
					If <m>\mathbf v</m> is an eigenvector for <m>\lambda</m> and any other eigenvector for <m>\lambda</m> is a multiple of <m>\mathbf v</m>, then there exists a matrix <m>T</m> such that
					<me>
						T^{-1} A T
						=
						\begin{pmatrix}
						\lambda &amp; 1 \\
						0 &amp; \lambda
						\end{pmatrix}.
					</me>
				</p>
			</statement>


			<proof>
				<p>
					If <m>{\mathbf w}</m> is another vector in <m>{\mathbb R}^2</m> such that <m>{\mathbf v}</m> and <m>{\mathbf w}</m> are linearly independent, then <m>A \mathbf w</m> can be written as a linear combination of <m>\mathbf v</m> and <m>\mathbf w</m>,
					<me>
						A {\mathbf w} = \alpha {\mathbf v} + \beta {\mathbf w}.
					</me>
					We can assume that <m>\alpha \neq 0</m>; otherwise, we would have a second linearly independent eigenvector.
					We claim that <m>\beta = \lambda</m>.
					If this were not the case, then
					<md>
						<mrow>A
						\left(
						{\mathbf w} +
						\left(
						\frac{\alpha}{\beta - \lambda}
						\right)
						{\mathbf v}
						\right)
						\amp =
						A {\mathbf w} +
						\left(
						\frac{\alpha}{\beta - \lambda}
						\right)
						A {\mathbf v}
						</mrow>
						<mrow>\amp = \alpha {\mathbf v} + \beta {\mathbf w} +
						\lambda \left(
						\frac{\alpha}{\beta - \lambda}
						\right)
						{\mathbf v}</mrow>
						<mrow>\amp =  \beta {\mathbf w} +
						\alpha \left(1 +
						\frac{\lambda}{\beta - \lambda}
						\right)
						{\mathbf v}</mrow>
						<mrow>\amp =  \beta {\mathbf w} +
						\alpha \left(
						\frac{\beta - \lambda + \lambda}{\beta - \lambda}
						\right)
						{\mathbf v}</mrow>
						<mrow>\amp = \beta
						\left(
						{\mathbf w} +
						\left(
						\frac{\alpha}{\beta - \lambda}
						\right)
						{\mathbf v}
						\right)</mrow>
					</md>
					and <m>\beta</m> would be an eigenvalue distinct from <m>\lambda</m>.
					Thus, <m>A {\mathbf w}  = \alpha {\mathbf v} + \lambda {\mathbf w}</m>.
					If we will let <m>{\mathbf u} = (1/ \alpha) {\mathbf w}</m>, then
					<me>
						A {\mathbf u} = {\mathbf v} + \frac{\lambda}{\alpha} {\mathbf w} = {\mathbf v} + \lambda {\mathbf u}.
					</me>
					We now define <m>T {\mathbf e}_1 = {\mathbf v}</m> and <m>T{\mathbf e}_2 = {\mathbf u}</m>.
					Since
					<md>
						<mrow>AT \amp = A\mathbf u + A \mathbf v = \mathbf v + \lambda \mathbf u + \lambda \mathbf v</mrow>
						<mrow>T\begin{pmatrix}
						\lambda &amp; 1 \\
						0 &amp; \lambda
						\end{pmatrix} \amp = T (\lambda \mathbf e_1) + T \mathbf e_1 + T (\lambda \mathbf e_2) = \mathbf v + \lambda \mathbf u + \lambda \mathbf v,</mrow>
					</md>
					we have
					<me>
						T^{-1} A T
						=
						\begin{pmatrix}
						\lambda &amp; 1 \\
						0 &amp; \lambda
						\end{pmatrix}.
					</me>
					Therefore, <m>{\mathbf x}' = A {\mathbf x}</m> is in canonical form after a change of coordinates.
				</p>
			</proof>
		</proposition>

		<example>
			<p>
				Consider the system  <m>\mathbf x' = A \mathbf x</m>, where
				<me>
					A = \begin{pmatrix} 5 &amp; 1 \\ -4 &amp; 1 \end{pmatrix}.
				</me>
				The characteristic polynomial of <m>A</m> is <m>\lambda^2 - 6 \lambda + 9 = (\lambda - 3)^2</m>, we have only a single eigenvalue <m>\lambda = 3</m> with eigenvector <m>\mathbf v = (1, -2)</m>.
				Any other eigenvector for <m>\lambda</m> is a multiple of <m>\mathbf v</m>.
				If we choose <m>\mathbf w = (1, 0)</m>, then <m>\mathbf v</m> and <m>\mathbf w</m> are linearly independent.
				Furthermore,
				<me>
					A \mathbf w = \begin{pmatrix} 5 \\ - 4 \end{pmatrix} = 2 \begin{pmatrix} 1 \\ -2 \end{pmatrix} + \lambda \begin{pmatrix} 1 \\ 0 \end{pmatrix} = 2 \begin{pmatrix} 1 \\ -2 \end{pmatrix} + 3 \begin{pmatrix} 1 \\ 0 \end{pmatrix}.
				</me>
				So we can let <m>\mathbf u = (1/2) \mathbf w = (1/2, 0)</m>.
				Therefore, the matrix that we seek is
				<me>
					T = \begin{pmatrix}  1 \amp 1/2 \\ -2 \amp 0 \end{pmatrix},
				</me>
				and
				<me>
					T^{-1} A T
					=
					\begin{pmatrix} -1/2 &amp; 2 \\ 1 &amp; 1 \end{pmatrix}
					\begin{pmatrix} 5 &amp; 1 \\ -4 &amp; 1 \end{pmatrix}
					\begin{pmatrix}  1 \amp 1/2 \\ -2 \amp 0\end{pmatrix}
					=
					\begin{pmatrix} 3 &amp; 1 \\ 0 &amp; 3 \end{pmatrix}.
				</me>
				From <xref ref="linear03" />, we know that the general solution to the system
				<me>
					\begin{pmatrix} dx/dt \\ dy/dt \end{pmatrix} = \begin{pmatrix} 3 &amp; 1 \\ 0 &amp; 3 \end{pmatrix} \begin{pmatrix} x \\  y \end{pmatrix}
				</me>
				is
				<me>
					\mathbf y(t) = c_1 e^{3t} \begin{pmatrix} 1 \\ 0 \end{pmatrix} + c_2 e^{3t} \begin{pmatrix} t \\ 1 \end{pmatrix}.
				</me>
				Therefore, the general solution to
				<me>
					\begin{pmatrix} dx/dt \\ dy/dt \end{pmatrix} = \begin{pmatrix} 5 &amp; 1 \\ -4 &amp; 1 \end{pmatrix} \begin{pmatrix} x \\  y \end{pmatrix}
				</me>
				is
				<md>
					<mrow>\mathbf x(t) \amp = T \mathbf y(t)</mrow>
					<mrow>\amp = c_1 e^{3t} T \begin{pmatrix} 1 \\ 0 \end{pmatrix} + c_2 e^{3t} T \begin{pmatrix} t \\ 1 \end{pmatrix}</mrow>
					<mrow>\amp = c_1 e^{3t} \begin{pmatrix} 1 \\ -2 \end{pmatrix} + c_2 e^{3t}  \begin{pmatrix} 1/2 + t \\ -2t \end{pmatrix}.</mrow>
				</md>
				This solution agrees with the solution that we found in <xref ref="linear05-example-repeated-eigenvalues" />.
			</p>
		</example>

		<p>
			In practice, we find solutions to linear systems using the methods that we outlined in <xref first="linear02" last="linear04">Sections</xref>.
			What we have demonstrated in this section is that those solutions are exactly the ones that we want.
		</p>
	</subsection>

	<subsection xml:id="linear06-subsection-important-lessons">
		<title>Important Lessons</title>
		<p>
			<ul>
				<li>
					<p>
						A linear map <m>T</m> is invertible if and only if <m>\det T \neq 0</m>.
					</p>
				</li>

				<li>
					<p>
						A linear map <m>T</m> converts solutions of <m>{\mathbf y}' = (T^{-1} A T) {\mathbf y}</m> to solutions of <m>{\mathbf x}' = A {\mathbf x}</m>.
					</p>
				</li>

				<li>
					<p>
						The inverse of a linear map <m>T</m> takes solutions of <m>{\mathbf x}' = A {\mathbf x}</m> to solutions of <m>{\mathbf y}' = (T^{-1} A T) {\mathbf y}</m>.
					</p>
				</li>

				<li>
					<p>
						A change of coordinates converts the system <m>{\mathbf x}' = A {\mathbf x}</m> to one of the following special cases,
						<me>
							\begin{pmatrix}
							\lambda &amp; 0 \\
							0 &amp; \mu
							\end{pmatrix},
							\begin{pmatrix}
							\alpha &amp;  \beta \\
							-\beta &amp; \alpha
							\end{pmatrix},
							\begin{pmatrix}
							\lambda &amp; 0 \\
							0 &amp; \lambda
							\end{pmatrix},
							\begin{pmatrix}
							\lambda &amp; 1 \\
							0 &amp; \lambda
							\end{pmatrix}.
						</me>
					</p>
				</li>
			</ul>
		</p>
	</subsection>

	<reading-questions xml:id="reading-questions-linear06">
	<exercise xml:id="reading-questions-linear06-1">
		<statement>
			<p>
				Explain what a change of coordinates is.
			</p>
		</statement>
		<response/>
	</exercise>

	<exercise xml:id="reading-questions-linear06-2">
		<statement>
			<p>
				Given a <m>2 \times 2</m> linear system, what are the possible types of solutions?
			</p>
		</statement>
		<response/>
	</exercise>
	</reading-questions>

	<exercises xml:id="linear06-exercises"  xml:base="linear06">
	<title>Exercises</title>
	<exercisegroup cols="2" xml:id="linear06-exercises-diagonalization">
	<title>Diagonalizing Matrices with Distinct Real Eigenvalues</title>
	<introduction>
		<p>
			For each of the matrices <m>A</m> in <xref ref="linear06-exercises-diagonalization"/>, find (1) the eigenvalues, <m>\lambda_1</m> and <m>\lambda_2</m>; (2) for each eigenvalue <m>\lambda_1</m> and <m>\lambda_2</m>, find an eigenvector <m>\mathbf v_1</m> and <m>\mathbf v_2</m>, respectively; and (3) construct the matrix <m>T = (\mathbf v_1, \mathbf v_2)</m> and calculate <m>T^{-1}AT</m>.
		</p>
	</introduction>

	<exercise>
		<statement>
			<p>
				<m>\displaystyle A =
				\begin{pmatrix}
				7 &amp; 3 \\
				-4 &amp; 0
				\end{pmatrix}</m>
			</p>
		</statement>
	</exercise>

	<exercise>
		<statement>
			<p>
				<m>\displaystyle A =
				\begin{pmatrix}
				-3 &amp; -10 \\
				3 &amp; 8
				\end{pmatrix}</m>
			</p>
		</statement>
	</exercise>

	<exercise>
		<statement>
			<p>
				<m>\displaystyle A =
				\begin{pmatrix}
				18 &amp; 11 \\
				-22 &amp; -15
				\end{pmatrix}</m>
			</p>
		</statement>
	</exercise>

	<exercise>
		<statement>
			<p>
				<m>\displaystyle A =
				\begin{pmatrix}
				-14 &amp; -12 \\
				18 &amp; 16
				\end{pmatrix}</m>
			</p>
		</statement>
	</exercise>

	<exercise>
		<statement>
			<p>
				<m>\displaystyle A =
				\begin{pmatrix}
				35/3 &amp; 22 \\
				-22/3 &amp; -14
				\end{pmatrix}</m>
			</p>
		</statement>
	</exercise>

	<exercise>
		<statement>
			<p>
				<m>\displaystyle A =
				\begin{pmatrix}
				31/2 &amp; 85/6 \\
				-17 &amp; -47/3
				\end{pmatrix}</m>
			</p>
		</statement>
	</exercise>
	</exercisegroup>

	<exercisegroup cols="2" xml:id="linear06-exercises-complex">
	<title>Matrices with Complex Eigenvalues</title>
	<introduction>
		<p>
			For each of the matrices <m>A</m> in <xref ref="linear06-exercises-complex"/>, find (1) an eigenvalue, <m>\lambda</m>; (2) find an eigenvector <m>\mathbf v = \mathbf v_{\text{Re}} + i \mathbf v_{\text{Im}}</m> for <m>\lambda</m>; and (2) construct the matrix <m>T = (\mathbf v_{\text{Re}} , \mathbf v_{\text{Im}})</m> and calculate <m>T^{-1}AT</m>.
			Compare your result to <m>\lambda</m>.
		</p>
	</introduction>

	<exercise>
		<statement>
			<p>
				<m>\displaystyle A =
				\begin{pmatrix}
				5 &amp; 2 \\
				-5 &amp; -1
				\end{pmatrix}</m>
			</p>
		</statement>
	</exercise>

	<exercise>
		<statement>
			<p>
				<m>\displaystyle A =
				\begin{pmatrix}
				13 &amp; 4 \\
				-26 &amp; -7
				\end{pmatrix}</m>
			</p>
		</statement>
	</exercise>

	<exercise>
		<statement>
			<p>
				<m>\displaystyle A =
				\begin{pmatrix}
				-2 &amp; -2 \\
				25 &amp; 12
				\end{pmatrix}</m>
			</p>
		</statement>
	</exercise>

	<exercise>
		<statement>
			<p>
				<m>\displaystyle A =
				\begin{pmatrix}
				-23/3 &amp; -5 \\
				13 &amp; 25/3
				\end{pmatrix}</m>
			</p>
		</statement>
	</exercise>
	</exercisegroup>

	<exercisegroup cols="2" xml:id="linear06-exercises-repeated">
	<title>Matrices with Repeated Eigenvalues</title>
	<introduction>
		<p>
			For each of the matrices <m>A</m> in <xref ref="linear06-exercises-repeated"/>, find (1) the eigenvalue, <m>\lambda</m> and an eigenvector <m>\mathbf v</m> for <m>\lambda</m>; (2) choose a vector <m>\mathbf w</m> that is linearly independent of <m>\mathbf v</m> and compute <m>(A - \lambda I)\mathbf w</m>.
			You should find that
			<me>
				(A - \lambda I)\mathbf w = \alpha \mathbf v
			</me>
			for some real number <m>\alpha</m>.
			(3) Let <m>\mathbf u = (1/\alpha) \mathbf w</m> and form the matrix <m>T = (\mathbf v, \mathbf u)</m>.
			(4) Calculate  <m>T^{-1}AT</m>, which should be in the form
			<me>
				\begin{pmatrix} \lambda &amp; 1 \\ 0 &amp; \lambda \end{pmatrix}.
			</me>
		</p>
	</introduction>

	<exercise>
		<statement>
			<p>
				<m>\displaystyle A =
				\begin{pmatrix}
				7 &amp; 4 \\
				-9 &amp; -5
				\end{pmatrix}</m>
			</p>
		</statement>
	</exercise>

	<exercise>
		<statement>
			<p>
				<m>\displaystyle A =
				\begin{pmatrix}
				4 &amp; 4 \\
				-9 &amp; -8
				\end{pmatrix}</m>
			</p>
		</statement>
	</exercise>

	<exercise>
		<statement>
			<p>
				<m>\displaystyle A =
				\begin{pmatrix}
				4 &amp; -4 \\
				1 &amp; 8
				\end{pmatrix}</m>
			</p>
		</statement>
	</exercise>

	<exercise>
		<statement>
			<p>
				<m>\displaystyle A =
				\begin{pmatrix}
				6 &amp; 1 \\
				-4 &amp; 2
				\end{pmatrix}</m>
			</p>
		</statement>
	</exercise>

	<exercise>
		<statement>
			<p>
				<m>\displaystyle A =
				\begin{pmatrix}
				3 &amp; 1 \\
				-2 &amp; 0
				\end{pmatrix}</m>
			</p>
		</statement>
	</exercise>

	<exercise>
		<statement>
			<p>
				<m>\displaystyle A =
				\begin{pmatrix}
				1 &amp; -2 \\
				1 &amp; 3
				\end{pmatrix}</m>
			</p>
		</statement>
	</exercise>
	</exercisegroup>
	</exercises>
</section>
<!--
</section>
-->
