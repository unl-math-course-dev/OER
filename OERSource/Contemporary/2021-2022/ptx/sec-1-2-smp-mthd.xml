<section xmlns:xi="http://www.w3.org/2001/XInclude" xml:id="sec-1-2-smp-mthd">

<title>Random Sampling and Sampling Bias</title>

<subsection>
	<title>Sampling Methods</title>

<p>As we mentioned in <xref ref="sec-1-1-stat"/>, the first thing we should do before conducting a survey is to identify the population that we want to study.  Suppose we are hired by a politician to determine the amount of support he has among the electorate should he decide to run for another term.  What population should we study?  Every person in the district?  Not every person is eligible to vote, and regardless of how strongly someone likes or dislikes the candidate, they don't have much to do with him being re-elected if they are not able to vote.</p>

<p>What about eligible voters in the district?  That might be better, but if someone is eligible to vote but does not register by the deadline, they won't have any say in the election either.  What about registered voters?  Many people are registered but choose not to vote.  What about "likely voters?"</p>

<p>This is the criteria used in much political polling, but it is sometimes difficult to define a "likely voter."  Is it someone who voted in the last election?  In the last general election?  In the last presidential election?  Should we consider someone who just turned 18 a "likely voter?"  They weren't eligible to vote in the past, so how do we judge the likelihood that they will vote in the next election?</p>

<p>In November 1998, former professional wrestler Jesse "The Body" Ventura was elected governor of Minnesota.  Up until right before the election, most polls showed he had little chance of winning.  There were several contributing factors to the polls not reflecting the actual intent of the electorate:
	<ul>
		<li>Ventura was running on a third-party ticket and most polling methods are better suited to a two-candidate race.</li>
		<li>Many respondents to polls may have been embarrassed to tell pollsters that they were planning to vote for a professional wrestler.</li>
		<li>The mere fact that the polls showed Ventura had little chance of winning might have prompted some people to vote for him in protest to send a message to the major-party candidates.</li>
	</ul>
</p>

<p>But one of the major contributing factors was that Ventura recruited a substantial amount of support from young people, particularly college students, who had never voted before and who registered specifically to vote in the gubernatorial election.  The polls did not deem these young people likely voters (since in most cases young people have a lower rate of voter registration and a turnout rate for elections) and so the polling samples were subject to <term>sampling bias</term>: they omitted a portion of the electorate that was weighted in favor of the winning candidate.</p>

<assemblage>
	<title>Sampling bias</title>
	<p>A sampling method is biased if every member of the population doesn't have equal likelihood of being in the sample.</p>
</assemblage>

<p>So even identifying the population can be a difficult job, but once we have identified the population, how do we choose an appropriate sample?  Remember, although we would prefer to survey all members of the population, this is usually impractical unless the population is very small, so we choose a sample. There are many ways to sample a population, but there is one goal we need to keep in mind: we would like the sample to be <em>representative of the population</em>.</p>

<p>Returning to our hypothetical job as a political pollster, we would not anticipate very accurate results if we drew all of our samples from among the customers at a Starbucks, nor would we expect that a sample drawn entirely from the membership list of the local Elks club would provide a useful picture of district-wide support for our candidate.</p>

<p>One way to ensure that the sample has a reasonable chance of mirroring the population is to employ <em>randomness</em>.  The most basic random method is simple random sampling.</p>

<assemblage>
	<title>Simple random sample</title>
	<p>A <term>random sample</term> is one in which each member of the population has an equal probability of being chosen.  A <term>simple random sample</term> is one in which every member of the population and any group of members has an equal probability of being chosen.</p>
</assemblage>

<example>
	<p>If we could somehow identify all likely voters in the state, put each of their names on a piece of paper, toss the slips into a (very large) hat and draw 1000 slips out of the hat, we would have a simple random sample.  </p>
</example>

<p>In practice, computers are better suited for this sort of endeavor than millions of slips of paper and extremely large headgear.</p>

<p>
	It is always possible, however, that even a random sample might end up not being totally representative of the population.  If we repeatedly take samples of 1000 people from among the population of likely voters in the state of Washington, some of these samples might tend to have a slightly higher percentage of Democrats (or Republicans) than does the general population; some samples might include more older people and some samples might include more younger people; etc.  In most cases, this <term>sampling variability</term> is not significant.
</p>

<assemblage>
	<title>Sampling variability</title>
	<p>The natural variation of samples is called <term>sampling variability.</term></p>

	<p>This is unavoidable and expected in random sampling, and in most cases is not an issue. (In <xref ref="sec-4-2-norm2"/>, we will learn one way this effect can be quantified, and we will see why it is usually insignificant.)</p>
</assemblage>

<p>
	To help account for variability, pollsters might instead use a <term>stratified sample</term>.
</p>

<assemblage>
	<title>Stratified sampling</title>
	<p>In <term>stratified sampling</term>, a population is divided into a number of subgroups (or strata). Random samples are then taken from each subgroup with sample sizes proportional to the size of the subgroup in the population.</p>
</assemblage>

<example>
	<p>Suppose in a particular state that previous data indicated that the electorate was comprised of 39% Democrats, 37% Republicans and 24% independents.  In a sample of 1000 people, they would then expect to get about 390 Democrats, 370 Republicans and 240 independents.  To accomplish this, they could randomly select 390 people from among those voters known to be Democrats, 370 from those known to be Republicans, and 240 from those with no party affiliation.</p>
</example>

<p>Stratified sampling can also be used to select a sample with people in desired age groups, a specified mix ratio of males and females, etc.  A variation on this technique is called <term>quota sampling</term>.</p>

<assemblage>
	<title>Quota sampling</title>
		<p><term>Quota sampling</term> is a variation on stratified sampling, wherein samples are collected in each subgroup until the desired quota is met.</p>
</assemblage>

<example>
	<p>Suppose the pollsters call people at random, but once they have met their quota of 390 Democrats, they only gather people who do not identify themselves as a Democrat.</p>
</example>

<p>You may have had the experience of being called by a telephone pollster who started by asking you your age, income, etc. and then thanked you for your time and hung up before asking any "real" questions.  Most likely, they already had contacted enough people in your demographic group and were looking for people who were older or younger, richer or poorer, etc.  Quota sampling is usually a bit easier than stratified sampling, but also does not ensure the same level of randomness. </p>

<p>Another sampling method is <term>cluster sampling</term>, in which the population is divided into groups, and one or more groups are randomly selected to be in the sample. </p>

<assemblage>
	<title>Cluster sampling</title>
	<p>In <term>cluster sampling</term>, the population is divided into subgroups (clusters), and a set of subgroups are selected to be in the sample.</p>
</assemblage>

<example>
	<p>If the college wanted to survey students, since students are already divided into classes, they could randomly select 10 classes and give the survey to all the students in those classes.  This would be cluster sampling.</p>
</example>

<p>Other sampling methods include <term>systematic sampling.</term></p>

<assemblage>
	<title>Systematic sampling</title>
	<p>In <term>systematic sampling</term>, every <m>n^{th}</m> member of the population is selected to be in the sample.</p>
</assemblage>

<example>
	<p>To select a sample using systematic sampling, a pollster calls every 100th name in the phone book.</p>
</example>

<p>Systematic sampling is not as random as a simple random sample (if your name is Albert Aardvark and your sister Alexis Aardvark is right after you in the phone book, there is no way you could both end up in the sample) but it can yield acceptable samples.</p>

<p>Perhaps the worst types of sampling methods are <term>convenience samples</term> and <term>voluntary response samples</term>.</p>

<assemblage>
	<title>Convenience sampling and voluntary response sampling</title>
	<p><term>Convenience sampling</term> is samples chosen by selecting whoever is convenient.</p>
	<p><term>Voluntary response sampling</term> is allowing the sample to volunteer.</p>
</assemblage>

<example>
	<p>A pollster stands on a street corner and interviews the first 100 people who agree to speak to him.  This is a convenience sample.</p>
</example>

<example>
	<p>A website has a survey asking readers to give their opinion on a tax proposal.  This is a self-selected sample, or voluntary response sample, in which respondents volunteer to participate.</p>
</example>

<p>Usually voluntary response samples are skewed towards people who have a particularly strong opinion about the subject of the survey or who just have way too much time on their hands and enjoy taking surveys.</p>

<exploration>
	<statement><p>In each case, indicate what sampling method was used.
		<ol>
			<li>Every 4th person in the class was selected</li>
			<li> A sample was selected to contain 25 men and 35 women</li>
			<li>Viewers of a new show are asked to vote on the show's website</li>
			<li>A website randomly selects 50 of their customers to send a satisfaction survey to</li>
			<li>To survey voters in a town, a polling company randomly selects 10 city blocks, and interviews everyone who lives on those blocks.</li>
		</ol>
	</p>
</statement>
	<solution>
		<p> <ol>
				<li>Systematic</li>
    			<li>Stratified or Quota</li>
    			<li>Voluntary response</li>
    			<li>Simple random</li>
    			<li>Cluster</li>
    		</ol></p>
    </solution>
</exploration>

</subsection>

<subsection><title>How to Mess Things Up Before You Start (Sampling Bias)</title>

	<p>There are number of ways that a study can be ruined before you even start collecting data.  The first we have already explored – <term>sampling</term> or <term>selection bias</term>, which is when the sample is not representative of the population.  One example of this is <term>voluntary response bias</term>, which is bias introduced by only collecting data from those who volunteer to participate.  This is not the only potential source of bias.</p>

	<assemblage>
		<title>Sources of bias</title>
		<p><term>Sampling bias</term> – when the sample is not representative of the population</p>
		<p><term>Voluntary response bias</term> – the sampling bias that often occurs when the sample is volunteers</p>
		<p><term>Self-interest study</term> – bias that can occur when the researchers have an interest in the outcome</p>
		<p><term>Response bias</term>– when the responder gives inaccurate responses for any reason</p>
		<p><term>Perceived lack of anonymity</term> – when the responder fears giving an honest answer might negatively affect them</p>
		<p><term>Loaded questions</term> – when the question wording influences the responses</p>
		<p><term>Non-response bias</term> – when people refusing to participate in the study can influence the validity of the outcome</p>
	</assemblage>

	<example>
		<p>Consider a recent study which found that chewing gum may raise math grades in teenagers .  This study was conducted by the Wrigley Science Institute, a branch of the Wrigley chewing gum company.  This is an example of a <term>self-interest study</term>; one in which the researches have a vested interest in the outcome of the study.  While this does not necessarily ensure that the study was biased, it certainly suggests that we should subject the study to extra scrutiny.</p>
	</example>

	<example>
		<p>A survey asks people “when was the last time you visited your doctor?”  This might suffer from <term>response bias</term>, since many people might not remember exactly when they last saw a doctor and give inaccurate responses.</p>
	</example>

	<p>Sources of response bias may be innocent, such as bad memory, or as intentional as pressuring by the pollster.  Consider, for example, how many voting initiative petitions people sign without even reading them.</p>

	<example>
		<p>A survey asks participants a question about their interactions with members of other races.  Here, a <term>perceived lack of anonymity</term> could influence the outcome.  The respondent might not want to be perceived as racist even if they are, and give an untruthful answer.</p>
	</example>

	<example>
		<p>An employer puts out a survey asking their employees if they have a drug abuse problem and need treatment help.  Here, answering truthfully might have consequences; responses might not be accurate if the employees do not feel their responses are anonymous or fear retribution from their employer.</p>
	</example>

	<example>
		<p>A survey asks “do you support funding research of alternative energy sources to reduce our reliance on high-polluting fossil fuels?”  This is an example of a <term>loaded</term> or <term>leading question</term> – questions whose wording leads the respondent towards an answer.  </p>
	</example>

	<p>
		Loaded questions can occur intentionally by pollsters with an agenda, or accidentally through poor question wording.  Also a concern is question order, where the order of questions changes the results.  A psychology researcher provides an example:
		<fn> Swartz, Norbert. <c>http://www.umich.edu/~newsinfo/MT/01/Fal01/mt6f01.html</c>. Retrieved 2009-03-31.</fn>
	</p>

	<p>
		<q>My favorite finding is this: we did a study where we asked students, <q>How satisfied are you with your life? How often do you have a date?</q> The two answers were not statistically related - you would conclude that there is no relationship between dating frequency and life satisfaction. But when we reversed the order and asked, <q>How often do you have a date? How satisfied are you with your life?</q> the statistical relationship was a strong one. You would now conclude that there is nothing as important in a student's life as dating frequency.</q>
</p>

	<example>
		<p>A telephone poll asks the question <q>Do you often have time to relax and read a book?</q>, and 50% of the people called refused to answer the survey.  It is unlikely that the results will be representative of the entire population.  This is an example of <term>non-response bias</term>, introduced by people refusing to participate in a study or dropping out of an experiment.  When people refuse to participate, we can no longer be so certain that our sample is representative of the population.</p>
	</example>

	<exploration>
		<statement><p>In each situation, identify a potential source of bias.
			<ol>
				<li>A survey asks how many sexual partners a person has had in the last year.</li>
				<li>A radio station asks readers to phone in their choice in a daily poll.</li>
				<li>A substitute teacher wants to know how students in the class did on their last test. The teacher asks the 10 students sitting in the front row to state their latest test score.</li>
				<li>High school students are asked if they have consumed alcohol in the last two weeks.</li>
				<li>The Beef Council releases a study stating that consuming red meat poses little cardiovascular risk.</li>
				<li>A poll asks <q>Do you support a new transportation tax, or would you prefer to see our public transportation system fall apart?</q></li>
			</ol>
		</p></statement>
		<solution>
			<ol>
				<li>Response bias <ndash/> historically, men are likely to over-report, and women are likely to under-report to this question.</li>
    		<li>Voluntary response bias <ndash/> the sample is self-selected.</li>
   			<li>Sampling bias <ndash/> the sample may not be representative of the whole class.</li>
    		<li>Lack of anonymity</li>
    		<li>Self-interest study</li>
    		<li>Loaded question</li>
    	</ol>
    </solution>
	</exploration>

</subsection>

<subsection><title>Experiments</title>

<p>
	So far, we have primarily discussed <term>observational studies</term> <mdash/> studies in which conclusions would be drawn from observations of a sample or the population.  In some cases these observations might be unsolicited, such as studying the percentage of cars that turn right at a red light even when there is a <q>no turn on red</q> sign.  In other cases the observations are solicited, like in a survey or a poll.
</p>

<p>
	In contrast, it is common to use <term>experiments</term> when exploring how subjects react to an outside influence.  In an experiment, some kind of <term>treatment</term> is applied to the subjects and the results are measured and recorded. By applying some treatment to the subjects, the researchers are controlling one of the variables, which does not occur in an observational study. While the term <q>treatment</q> comes from the field of medicine, we are using it to refer to any effect controlled by the researchers.
</p>

<assemblage>
	<title>Observational studies and experiments</title>
	<p>An <term>observational study</term> is a study based on observations or measurements. The researchers do not control any variable being studied, but rather measure a population as it is.</p>
	<p>An <term>experiment</term> is a study in which the effects of a <term>treatment</term> are measured. The treatment is some effect that the researchers can control.</p>
</assemblage>

<p>Here are some examples of experiments:</p>

<example>
	<p><ol>
		<li>A pharmaceutical company tests a new medicine for treating Alzheimer's disease by administering the drug to 50 elderly patients with recent diagnoses.  The treatment here is the new drug.</li>

		<li>A gym tests out a new weight loss program by enlisting 30 volunteers to try out the program.  The treatment here is the new program.</li>

		<li>You test a new kitchen cleaner by buying a bottle and cleaning your kitchen.  The new cleaner is the treatment.</li>

		<li>A psychology researcher explores the effect of music on temperament by measuring people's temperament while listening to different types of music.  The music is the treatment.</li>


	</ol></p>
</example>

<exploration>
	<statement><p> Is each scenario describing an observational study or an experiment? </p>

	<p><ol>
		<li> The weights of 30 randomly selected people are measured.</li>

		<li>Subjects are asked to do 20 jumping jacks, and then their heart rates are measured.</li>

		<li>Twenty people are told to drink coffee and twenty are told to drink tea. They are then given a concentration test.</li>

		<li>Researchers survey 100 students, asking whether they drink coffee or tea. They then give these 100 people a concentration test.
		</li>
	</ol></p></statement>

	<solution>
		<p><ol>
			<li>Observational study</li>
			<li> Experiment; the treatment is the jumping jacks</li>
			<li> Experiment; the treatments are coffee and tea </li>
			<li>Observational study</li>
		</ol></p>
	</solution>
</exploration>

<p>
	Experiments can often yield more robust results than observational studies; however, observational studies are sometimes necessary for ethical or logistical reasons. For example, suppose researches are studying the effects of smoking. They could not ethically ask an experimental group to start smoking, so they would have to perform an observational study instead.
</p>

<p>
	The design of an experiment will influence its accuracy. Let's start to investigate this more.
</p>

<example>
	<p> Suppose a middle school (junior high) finds that their students are not scoring well on the state's standardized math test.  They decide to run an experiment to see if an alternate curriculum would improve scores.  To run the test, they hire a math specialist to come in and teach a class using the new curriculum.  To their delight, they see an improvement in test scores.</p>
</example>

<p>The difficulty with this scenario is that it is not clear whether the curriculum is responsible for the improvement, or whether the improvement is due to a math specialist teaching the class.  This is called <term>confounding </term> – when it is not clear which factor or factors caused the observed effect.  Confounding is the downfall of many experiments, though sometimes it is hidden.</p>

<assemblage>
	<title> Confounding </title>

	<p><term>Confounding</term> occurs when there are two potential variables that could have caused the outcome, and it is not possible to determine which actually caused the result.</p>
</assemblage>

<example>
	<p>A drug company study about a weight loss pill might report that people lost an average of 4 kg while using their new drug.  However, in the fine print you find a statement saying that participants were encouraged to also diet and exercise.  It is not clear in this case whether the weight loss is due to the pill, to diet and exercise, or a combination of both.  In this case confounding has occurred.</p>
</example>

<example>
	<p>Researchers conduct an experiment to determine whether students will perform better on an arithmetic test if they listen to music during the test.  They first give the student a test without music, then give a similar test while the student listens to music.  In this case, the student might perform better on the second test, regardless of the music, simply because it was the second test and they were warmed up.</p>
</example>

<p> There are a number of measures that can be introduced to help reduce the likelihood of confounding.  The primary measure is to use a <term>control group</term>.</p>

<assemblage>
	<title>Control Group</title>

	<p>When using a control group, the participants are divided into two or more groups, typically a <term>control group</term> and a treatment group.  The treatment group receives the treatment being tested; the control group does not receive the treatment.</p>
</assemblage>

<p>Ideally, the groups are otherwise as similar as possible, isolating the treatment as the only potential source of difference between the groups.  For this reason, the method of dividing groups is important.  Some researchers attempt to ensure that the groups have similar characteristics (same number of females, same number of people over 50, etc.), but it is nearly impossible to control for every characteristic.  Because of this, <em>random assignment</em> is very commonly used<mdash/>that is, the choice of which participants are in the treatment and control groups is random. For this reason, such experiments are often called <term>randomized controlled trials</term>.</p>

<p>
	Note that we have now introduced two kinds of randomness. First, the participants in the study must be <em>randomly selected:</em> that is, the choice of who participates in the study in the first place must be random. This reduces selection bias, ensuring that participants roughly represent the overall population. Second, the participants must be <em>randomly assigned</em> to either the treatment or control group, as described above. These two types of randomness are distinct, and are both important for good experimental design.
</p>

<example xml:id="eg-sat"><p>To determine if a two day prep course would help high school students improve their scores on the SAT test, a group of students was randomly divided into two subgroups.  The first group, the treatment group, was given a two day prep course.  The second group, the control group, was not given the prep course.  Afterwards, both groups were given the SAT.</p> </example>

<example><p>A company testing a new plant food grows two crops of plants in adjacent fields, the treatment group receiving the new plant food and the control group not.  The crop yield would then be compared.  By growing them at the same time in adjacent fields, they are controlling for weather and other confounding factors.</p></example>

<p>Sometimes not giving the control group anything does not completely control for confounding variables.  For example, suppose a medicine study is testing a new headache pill by giving the treatment group the pill and the control group nothing.  If the treatment group showed improvement, we would not know whether it was due to the medicine in the pill, or a response to have taken any pill.  This is called a <term>placebo effect</term>.</p>

<assemblage>
	<title>Placebo effect</title>
	<p>The <term>placebo effect</term> is when the effectiveness of a treatment is influenced by the patient's perception of how effective they think the treatment will be, so a result might be seen even if the treatment is ineffectual.</p>
</assemblage>

<example><p>A study found that when doing painful dental tooth extractions, patients told they were receiving a strong painkiller while actually receiving a saltwater injection found as much pain relief as patients receiving a dose of morphine. </p></example>

<p>To control for the placebo effect, a <term>placebo</term>, or dummy treatment, is often given to the control group.  This way, both groups are truly identical except for the specific treatment given.</p>

<assemblage>
	<title>Placebo and Placebo controlled experiments</title>
	<p>A <term>placebo</term> is a dummy treatment given to control for the placebo effect.</p>
	<p>An experiment that gives the control group a placebo is called a <term>placebo controlled experiment.</term></p>
</assemblage>

<example>
	<p><ol>
		<li>In a study for a new medicine that is dispensed in a pill form, a sugar pill could be used as a placebo.</li>
		<li>In a study on the effect of alcohol on memory, a non-alcoholic beer might be given to the control group as a placebo.</li>
		<li>In a study of a frozen meal diet plan, the treatment group would receive the diet food, and the control could be given standard frozen meals stripped of their original packaging.</li>
	</ol></p>
</example>

<p>In some cases, it is more appropriate to compare to a conventional treatment than a placebo.  For example, in a cancer research study, it would not be ethical to deny any treatment to the control group or to give a placebo treatment.  In this case, the currently acceptable medicine would be given to the control group, sometimes called a <term>comparison group</term> in this case.  In <xref ref="eg-sat"/>, the non-treatment group would most likely be encouraged to study on their own, rather than be asked to not study at all, to provide a meaningful comparison.</p>

<p>When using a placebo, it would defeat the purpose if the participant knew they were receiving the placebo.</p>

<assemblage>
	<title>Blind studies</title>

	<p>A <term>blind study</term> is one in which the participant does not know whether or not they are receiving the treatment or a placebo.</p>

	<p>A <term>double-blind study</term> is one in which those interacting with the participants don't know who is in the treatment group and who is in the control group.</p>
</assemblage>

<example><p>In a study about anti-depression medicine, you would not want the psychological evaluator to know whether the patient is in the treatment or control group either, as it might influence their evaluation, so the experiment should be conducted as a double-blind study.</p></example>

<p>It should be noted that not every experiment needs a control group.</p>

<example><p>If a researcher is testing whether a new fabric can withstand fire, she simply needs to torch multiple samples of the fabric – there is no need for a control group.</p></example>

<exploration>
	<statement><p>To test a new lie detector, two groups of subjects are given the new test.  One group is asked to answer all the questions truthfully, and the second group is asked to lie on one set of questions.  The person administering the lie detector test does not know what group each subject is in.</p>

	<p>Does this experiment have a control group?  Is it blind, double-blind, or neither?</p></statement>

	<solution><p>The truth-telling group could be considered the control group, but really both groups are treatment groups here, since it is important for the lie detector to be able to correctly identify lies, and also not identify truth telling as lying.  This study is blind, since the person running the test does not know what group each subject is in.</p></solution>

</exploration>

</subsection>

</section>
