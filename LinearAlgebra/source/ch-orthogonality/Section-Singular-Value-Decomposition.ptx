<?xml version="1.0" encoding="UTF-8"?>

<section xml:id="Section-Singular-Value-Decomposition" xmlns:xi="http://www.w3.org/2001/XInclude">
    <title>Extra Topic: Singular Value Decomposition</title>



 






<p>
We begin this section with an important definition.
</p>


<definition xml:id="singularvalues">

    <statement>
        <p>
            Let <m>A</m> be an <m>m\times n</m> matrix. The <term>singular values</term> of <m>A</m> are the square roots of the positive
eigenvalues of <m>A^TA.</m>
        </p>
    </statement>
</definition>

<p>
Singular Value Decomposition (SVD) can be thought of as
a generalization of orthogonal diagonalization of a symmetric matrix
to an arbitrary <m>m\times n</m> matrix. This decomposition is the focus of this section.

The following is a useful result that will help when computing the SVD of matrices.
</p>


<lemma xml:id="lem-samenonzeroeigenvalues">

    <statement>
        <p>
            Let <m>A</m> be an <m>m \times n</m> matrix. Then <m>A^TA</m> and <m>AA^T</m> have the same <m>\textbf{nonzero}</m> eigenvalues.
        </p>
    </statement>


<proof>
    <p>
Suppose <m>A</m> is an <m>m\times n</m> matrix, and suppose that  <m>\lambda</m> is a nonzero eigenvalue of <m>A^TA</m>.
Then there exists a nonzero vector <m>\mathbf{x} \in \R^n</m> such that
<men xml:id="nonzero">
(A^TA)\mathbf{x}=\lambda \mathbf{x}.
</men>
Multiplying both sides of this equation by <m>A</m> yields:
<md>
<mrow> A(A^TA)\mathbf{x} \amp  =   A\lambda \mathbf{x}, </mrow> 
<mrow> (AA^T)(A\mathbf{x}) \amp  =  \lambda (A\mathbf{x}). </mrow>
</md>
Since <m>\lambda\neq 0</m> and <m>\mathbf{x}\neq 0_n</m>, <m>\lambda \mathbf{x}\neq 0_n</m>,
and thus by <xref ref="nonzero"/>,
<m>(A^TA)\mathbf{x}\neq 0_m</m>; thus <m>A^T(A\mathbf{x})\neq 0_m</m>,
implying that <m>A\mathbf{x}\neq 0_m</m>.
</p> 

<p>
Therefore <m>A\mathbf{x}</m> is an eigenvector of <m>AA^T</m> corresponding to eigenvalue
<m>\lambda</m>.  An analogous argument can be used to show that every
nonzero eigenvalue of <m>AA^T</m> is an eigenvalue of <m>A^TA</m>, thus
completing the proof.
    </p>
</proof>
</lemma>


<p>
Given an <m>m\times n</m> matrix <m>A</m>, we will see how to express <m>A</m> as a product <m>A=U\Sigma V^T</m> where
<ul>
<li>
      <p> <m>U</m> is an <m>m\times m</m> orthogonal matrix whose columns are
eigenvectors of <m>AA^T</m>. </p>
</li>
<li>
      <p> <m>V</m> is an <m>n\times n</m> orthogonal matrix whose columns are
eigenvectors of <m>A^TA</m>. </p>
</li>
<li>
      <p> <m>\Sigma</m> is an <m>m\times n</m> matrix whose only nonzero values
lie on its main diagonal, and are the singular values of <m>A</m>. </p>
</li>
</ul>
How can we find such a decomposition? We are aiming to decompose <m>A</m> in the following form:
<me>
A=U\left[
\begin{array}{cc}
\sigma \amp  0 \\
0 \amp  0
\end{array}
\right] V^T
</me>
where <m>\sigma </m> is a block matrix of the form
<me>
\sigma =\left[
\begin{array}{ccc}
\sigma _{1} \amp   \amp  0 \\
\amp  \ddots \amp   \\
0 \amp   \amp  \sigma _{k}
\end{array}
\right]
</me>

Thus 
<m>A^T=V\left[
\begin{array}{cc}
\sigma \amp  0 \\
0 \amp  0
\end{array}
\right] U^T</m> and it follows that
<me>
A^TA=V\left[
\begin{array}{cc}
\sigma \amp  0 \\
0 \amp  0
\end{array}
\right] U^TU\left[
\begin{array}{cc}
\sigma \amp  0 \\
0 \amp  0
\end{array}
\right] V^T=V\left[
\begin{array}{cc}
\sigma ^{2} \amp  0 \\
0 \amp  0
\end{array}
\right] V^T
</me>
and so <m>A^TAV=V\left[
\begin{array}{cc}
\sigma ^{2} \amp  0 \\
0 \amp  0
\end{array}
\right] .</m> Similarly, <me>
    AA^TU=U\left[
\begin{array}{cc}
\sigma ^{2} \amp  0 \\
0 \amp  0
\end{array}
\right] .</me> 

Therefore, you would find an orthonormal basis of eigenvectors
for <m>AA^T</m> make them the columns of a matrix such that the
corresponding eigenvalues are decreasing. This gives <m>U.</m> You could then do
the same for <m>A^TA</m> to get <m>V</m>.
</p>


<p>
We formalize this discussion in the following theorem.
</p>


<theorem xml:id="th-singvaldecomp">
    <title>Singular Value Decomposition</title>
    <statement>
        <p>
            Let <m>A</m> be an <m>m\times n</m> matrix. Then there exist
orthogonal matrices <m>U</m> and <m>V</m> of the appropriate size such that <m>A= U \Sigma V^T</m> where <m>\Sigma</m> is of the form
<me>
\Sigma =
\left[
\begin{array}{cc}
\sigma \amp  0 \\
0 \amp  0
\end{array}
\right]
</me>
and <m>\sigma </m> is a block matrix of the form
<me>
\sigma =\left[
\begin{array}{ccc}
\sigma _{1} \amp   \amp  0 \\
\amp  \ddots \amp   \\
0 \amp   \amp  \sigma _{k}
\end{array}
\right]
</me>
for the <m>\sigma _{i}</m> the singular values of <m>A.</m>
        </p>
    </statement>


<proof>
    <p>
There exists an orthonormal basis, <m>\left\{ \mathbf{v}_{1}, \dots, \mathbf{v}_n\right\}</m> such that 
<me>
A^TA\mathbf{v}_{i}=\sigma _{i}^{2}\mathbf{v}_{i}
</me>

where <m>\sigma
_{i}^{2}\gt 0</m> for <m>i=1,\dots ,k,\left( \sigma _{i}\gt 0\right) </m> and equals zero
if <m>i\gt k.</m> Thus for <m>i\gt k,</m> <m>A\mathbf{v}_{i}=\mathbf{0}</m> because
<me>
 A\mathbf{v}_{i}\cdot A\mathbf{v}_{i} = (A\mathbf{v}_i)^T(A\mathbf{v}_i)=\mathbf{v}_i^T(A^TA\mathbf{v}_i)=\mathbf{0}.
</me>
For <m>i=1,\dots ,k,</m> define <m>\mathbf{u}_{i}\in \R^{m}</m> by
<me>
\mathbf{u}_{i}= \sigma _{i}^{-1}A\mathbf{v}_{i}.
</me>
Thus <m>A\mathbf{v}_{i}=\sigma _{i}\mathbf{u}_{i}.</m> Now,
<md>
<mrow> \mathbf{u}_{i} \cdot \mathbf{u}_{j} \amp =   \sigma _{i}^{-1}A
\mathbf{v}_{i} \cdot \sigma _{j}^{-1}A\mathbf{v}_{j}  = \sigma_{i}^{-1}\mathbf{v}_{i}^T \sigma _{j}^{-1}A^TA\mathbf{v}_{j} </mrow>
<mrow> \amp =  \sigma _{i}^{-1}\mathbf{v}_{i} \cdot \sigma _{j}^{-1}\sigma _{j}^{2} \mathbf{v}_{j} =
\frac{\sigma _{j}}{\sigma _{i}}\left( \mathbf{v}_{i} \cdot \mathbf{v}_{j}\right). </mrow>
</md>

This means that <m>\mathbf{u}_{i} \cdot \mathbf{u}_{j}=1</m> when <m>i=j</m> and <m>\mathbf{u}_{i} \cdot \mathbf{u}_{j}=0</m> when <m>i\neq j</m>.
Thus <m>\left\{ \mathbf{u}_{1}, \dots, \mathbf{u}_{k}\right\}</m> is an orthonormal set of
vectors in <m>\R^{m}.</m> Also,
<me>
AA^T\mathbf{u}_{i}=AA^T\sigma _{i}^{-1}A\mathbf{v}_{i}=\sigma
_{i}^{-1}AA^TA\mathbf{v}_{i}=\sigma _{i}^{-1}A\sigma _{i}^{2}\mathbf{v}
_{i}=\sigma _{i}^{2}\mathbf{u}_{i}.
</me>
Using Gram-Schmidt, extend <m>\left\{ \mathbf{u}_{1}, \dots, \mathbf{u}_{k}\right\}</m> to an orthonormal
basis for all of <m>\R^{m},\left\{ \mathbf{u}_{1},\dots,\mathbf{u}_{m}\right\}</m>
and let
<me>
U= \left[
\begin{array}{ccc}
\mathbf{u}_{1} \amp  \cdots \amp  \mathbf{u}_{m}
\end{array}
\right],
</me>
while <m>V= \left[ \begin{array}{ccc} \mathbf{v}_{1} \amp  \cdots \amp  \mathbf{v}_{n}\end{array}\right] .</m> Thus <m>U</m>
is the matrix which has the <m>\mathbf{u}_{i}</m> as columns and <m>V</m> is defined
as the matrix which has the <m>\mathbf{v}_{i}</m> as columns. Then
<me>
U^TAV=\left[
\begin{array}{c}
\mathbf{u}_{1}^T \\
\vdots \\
\mathbf{u}_{k}^T \\
\vdots \\
\mathbf{u}_{m}^T
\end{array}
\right] A  \left[ \begin{array}{ccc} \mathbf{v}_{1} \amp  \cdots \amp  \mathbf{v}_{n}\end{array}\right]
</me>
<me>
=\left[
\begin{array}{c}
\mathbf{u}_{1}^T \\
\vdots \\
\mathbf{u}_{k}^T \\
\vdots \\
\mathbf{u}_{m}^T
\end{array}
\right] \left[
\begin{array}{cccccc}
\sigma _{1}\mathbf{u}_{1} \amp  \cdots \amp  \sigma _{k}\mathbf{u}_{k} \amp  \mathbf{0}
\amp  \cdots \amp  \mathbf{0}
\end{array}
\right] =\left[
\begin{array}{cc}
\sigma \amp  0 \\
0 \amp  0
\end{array}
\right],
</me>
where <m>\sigma </m> is given in the statement of the theorem.
    </p>
</proof>
</theorem>

<p>
The SVD has as an immediate corollary which is given in the following interesting result.
</p>

<corollary xml:id="cor-ranksingularvalues">

    <statement>
        <p>
            Let <m>A</m> be an <m>m\times n</m> matrix. Then the rank of <m>A</m> (or of <m>A^T</m>) equals
the number of singular values.
        </p>
    </statement>
</corollary>

<p>
Let's compute the SVD of a simple matrix.
</p>

<example xml:id="ex-SVD2x3">
    <statement>
        <p>
            Let
<me>
A=\left[\begin{array}{rrr} 1 \amp  -1 \amp  3 \\ 3 \amp  1 \amp  1 \end{array}\right].
</me>


Find the SVD of <m>A</m>.
       </p>
    </statement>
    <answer>
        <p>
            To begin, we compute <m>AA^T</m> and <m>A^TA</m>.
<me>
AA^T = \left[\begin{array}{rrr} 1 \amp  -1 \amp  3 \\ 3 \amp  1 \amp  1 \end{array}\right]
\left[\begin{array}{rr} 1 \amp  3 \\ -1 \amp  1 \\ 3 \amp  1  \end{array}\right]
= \left[\begin{array}{rr} 11 \amp  5 \\ 5 \amp  11  \end{array}\right],
</me>

<me>
A^TA = \left[\begin{array}{rr} 1 \amp  3 \\ -1 \amp  1 \\ 3 \amp  1  \end{array}\right]
\left[\begin{array}{rrr} 1 \amp  -1 \amp  3 \\ 3 \amp  1 \amp  1 \end{array}\right]
= \left[\begin{array}{rrr} 10 \amp  2 \amp  6 \\ 2 \amp  2 \amp  -2\\
6 \amp  -2 \amp  10 \end{array}\right].
</me>


Since <m>AA^T</m> is <m>2\times 2</m> while <m>A^T A</m> is <m>3\times 3</m>, and <m>AA^T</m>
and <m>A^TA</m> have the same nonzero eigenvalues (by
<xref ref="lem-samenonzeroeigenvalues"/>), we compute the characteristic polynomial  <m>c_{AA^T}(x)</m> (because it is
easier to compute than <m>c_{A^TA}(x)</m>).
<md>
<mrow> c_{AA^T}(z)\amp  = \det(zI-AA^T)=  \det \left[\begin{array}{cc}
z-11 \amp  -5 \\ -5 \amp  z-11 \end{array}\right] </mrow> 
<mrow> \amp  =  (z-11)^2 - 25 </mrow> 
<mrow> \amp  =   z^2-22z+121-25 </mrow> 
<mrow> \amp  =   z^2-22z+96 </mrow>
<mrow> \amp  =   (z-16)(z-6). </mrow>
</md>

Therefore, the eigenvalues of <m>AA^T</m> are <m>\lambda_1=16</m> and <m>\lambda_2=6</m>.

The eigenvalues of <m>A^TA</m> are <m>\lambda_1=16</m>, <m>\lambda_2=6</m>, and
<m>\lambda_3=0</m>, and the singular values of <m>A</m> are <m>\sigma_1=\sqrt{16}=4</m> and
<m>\sigma_2=\sqrt{6}</m>.
By convention, we list the eigenvalues (and corresponding singular values)
in non increasing order (i.e., from largest to smallest).
</p>


<p> 
<m> 
\textbf{To find the matrix: <m>V</m>}
</m> 
</p> 

<p>
To construct the matrix <m>V</m> we need to find eigenvectors for <m>A^TA</m>.
Since the eigenvalues of <m>AA^T</m> are distinct, the corresponding
eigenvectors are orthogonal, and we need only normalize them.
</p> 

<p>
<m>\lambda_1=16</m>: solve <m>(16I-A^TA)\mathbf{x}_1= \mathbf{0}</m>.

<me> \left[\begin{array}{rrr|r}
6 \amp  -2 \amp  -6 \amp  0 \\ -2 \amp  14 \amp  2 \amp  0 \\ -6 \amp  2 \amp  6 \amp  0
\end{array}\right]
\rightarrow
\left[\begin{array}{rrr|r}
1 \amp  0 \amp  -1 \amp  0 \\ 0 \amp  1 \amp  0 \amp  0 \\ 0 \amp  0 \amp  0 \amp  0
\end{array}\right],
</me>

so
<me>
\mathbf{x}_1 =\left[\begin{array}{r} t \\ 0 \\ t \end{array}\right]
=t\left[\begin{array}{r} 1 \\ 0 \\ 1 \end{array}\right],
t\in \R.
</me>
</p> 


<p> 
<m>\lambda_2=6</m>: solve <m>(6I-A^TA)\mathbf{x}_2= \mathbf{0}</m>.

<me> \left[\begin{array}{rrr|r}
-4 \amp  -2 \amp  -6 \amp  0 \\ -2 \amp  4 \amp  2 \amp  0 \\ -6 \amp  2 \amp  -4 \amp  0
\end{array}\right]
\rightarrow
\left[\begin{array}{rrr|r}
1 \amp  0 \amp  1 \amp  0 \\ 0 \amp  1 \amp  1 \amp  0 \\ 0 \amp  0 \amp  0 \amp  0
\end{array}\right],
</me> 

so
<me>
\mathbf{x}_2=\left[\begin{array}{r} -s \\ -s \\ s \end{array}\right]
=s\left[\begin{array}{r} -1 \\ -1 \\ 1 \end{array}\right],
s\in \R. </me>
</p> 



<p> 
<m>\lambda_3=0</m>: solve <m>(-A^TA)\mathbf{x}_3= \mathbf{0}</m>.
<me> \left[\begin{array}{rrr|r}
-10 \amp  -2 \amp  -6 \amp  0 \\ -2 \amp  -2 \amp  2 \amp  0 \\ -6 \amp  2 \amp  -10 \amp  0
\end{array}\right]
\rightarrow
\left[\begin{array}{rrr|r}
1 \amp  0 \amp  1 \amp  0 \\ 0 \amp  1 \amp  -2 \amp  0 \\ 0 \amp  0 \amp  0 \amp  0
\end{array}\right],
</me> 

so
<me>
\mathbf{x}_3=\left[\begin{array}{r} -r \\ 2r \\ r \end{array}\right]
=r\left[\begin{array}{r} -1 \\ 2 \\ 1 \end{array}\right],
r\in \R. </me> 
</p> 

<p>
With the eigenvectors found, let
<me> 
\mathbf{v}_1=\frac{1}{\sqrt{2}}\left[\begin{array}{r} 1\\ 0\\ 1 \end{array}\right],
\mathbf{v}_2=\frac{1}{\sqrt{3}}\left[\begin{array}{r} -1\\ -1\\ 1 \end{array}\right],
\mathbf{v}_3=\frac{1}{\sqrt{6}}\left[\begin{array}{r} -1\\ 2\\ 1 \end{array}\right]
</me>
Then
<me> 
V=\frac{1}{\sqrt{6}}\left[\begin{array}{rrr}
\sqrt 3 \amp  -\sqrt 2 \amp  -1  \\
0 \amp  -\sqrt 2 \amp  2 \\
\sqrt 3 \amp  \sqrt 2 \amp  1 \end{array}\right].
</me> 


Also,
<me>
\Sigma = \left[\begin{array}{rrr} 4 \amp  0 \amp  0 \\
0 \amp  \sqrt 6 \amp  0 \end{array}\right]
</me>

and we use <m>A</m>, <m>V^T</m>, and <m>\Sigma</m> to find <m>U</m>. Since <m>V</m> is orthogonal and <m>A=U\Sigma V^T</m>, it follows that 
<me>
    AV=U\Sigma.
</me>

<!-- BLLLLLLLLAH --> 
Let <m>V=\left[\begin{array}{ccc} \mathbf{v}_1 \amp  \mathbf{v}_2 \amp  \mathbf{v}_3 \end{array}\right]</m>, and
let <m>U=\left[\begin{array}{cc} \mathbf{u}_1 \amp  \mathbf{u}_2 \end{array}\right]</m>, where
<m>\mathbf{u}_1</m> and <m>\mathbf{u}_2</m> are the two columns of <m>U</m>. Then we have
<md>
 <mrow> A\left[\begin{array}{ccc} \mathbf{v}_1 \amp  \mathbf{v}_2 \amp  \mathbf{v}_3 \end{array}\right]  
 \amp = \left[\begin{array}{cc} \mathbf{u}_1 \amp  \mathbf{u}_2 \end{array}\right]\Sigma 
 \left[\begin{array}{ccc} A\mathbf{v}_1 \amp  A\mathbf{v}_2 \amp  A\mathbf{v}_3 \end{array}\right] </mrow>
 <mrow> \amp =  \left[\begin{array}{ccc} \sigma_1\mathbf{u}_1 + 0\mathbf{u}_2 \amp 
0\mathbf{u}_1 + \sigma_2 \mathbf{u}_2 \amp  0 \mathbf{u}_1 + 0\mathbf{u}_2 \end{array}\right] </mrow>
 <mrow> = \left[\begin{array}{ccc} \sigma_1\mathbf{u}_1 \amp  \sigma_2 \mathbf{u}_2 \amp 
0 \end{array}\right] </mrow> 
</md> 

which implies that <m>A\mathbf{v}_1=\sigma_1\mathbf{u}_1 = 4\mathbf{u}_1</m> and
<m>A\mathbf{v}_2=\sigma_2\mathbf{u}_2 = \sqrt 6 \mathbf{u}_2</m>. Thus,
<me> \mathbf{u}_1 = \frac{1}{4}A\mathbf{v}_1
= \frac{1}{4}
\left[\begin{array}{rrr} 1 \amp  -1 \amp  3 \\ 3 \amp  1 \amp  1 \end{array}\right]
\frac{1}{\sqrt{2}}\left[\begin{array}{r} 1\\ 0\\ 1 \end{array}\right]
= \frac{1}{4\sqrt 2}\left[\begin{array}{r} 4\\ 4 \end{array}\right]
= \frac{1}{\sqrt 2}\left[\begin{array}{r} 1\\ 1 \end{array}\right]</me>
and
<md> 
<mrow> \mathbf{u}_2 \amp = \frac{1}{\sqrt 6}A\mathbf{v}_2
= \frac{1}{\sqrt 6}
\left[\begin{array}{rrr} 1 \amp  -1 \amp  3 \\ 3 \amp  1 \amp  1 \end{array}\right]
\frac{1}{\sqrt{3}}\left[\begin{array}{r} -1\\ -1\\ 1 \end{array}\right] </mrow>
<mrow> \amp =\frac{1}{3\sqrt 2}\left[\begin{array}{r} 3\\ -3 \end{array}\right]
=\frac{1}{\sqrt 2}\left[\begin{array}{r} 1\\ -1 \end{array}\right] </mrow>
</md>
Therefore,
<me> U=\frac{1}{\sqrt{2}}\left[\begin{array}{rr} 1 \amp  1 \\
1 \amp  -1 \end{array}\right]
</me>
and
<md>
<mrow> A \amp  =   \left[\begin{array}{rrr} 1 \amp  -1 \amp  3 \\ 3 \amp  1 \amp  1 \end{array}\right] </mrow>
<mrow> \amp  =  \left(\frac{1}{\sqrt{2}}\left[\begin{array}{rr} 1 \amp  1 \\
1 \amp  -1 \end{array}\right]\right)
\left[\begin{array}{rrr} 4 \amp  0 \amp  0 \\
0 \amp  \sqrt 6 \amp  0 \end{array}\right]
\left(\frac{1}{\sqrt{6}}\left[\begin{array}{rrr}
\sqrt 3 \amp  0 \amp  \sqrt 3  \\
-\sqrt 2 \amp  -\sqrt 2 \amp  \sqrt2 \\
-1 \amp  2 \amp  1 \end{array}\right]\right). </mrow>
</md>
       </p>
    </answer>
</example>


<example xml:id="ex-SVD3x1">
    <statement>
        <p>
            Find an SVD for
<me>
A=\left[\begin{array}{r} -1 \\ 2\\ 2 \end{array}\right]
</me>.
       </p>
    </statement>
    <answer>
        <p>
            Since <m>A</m> is <m>3\times 1</m>, <m>A^T A</m> is a <m>1\times 1</m> matrix
whose eigenvalues are easier to find than the eigenvalues of
the <m>3\times 3</m> matrix <m>AA^T</m>.

<me> 
A^TA=\left[\begin{array}{ccc} -1 \amp  2 \amp  2 \end{array}\right]
\left[\begin{array}{r} -1 \\ 2 \\ 2 \end{array}\right]
=\left[\begin{array}{r} 9 \end{array}\right].
</me>

Thus <m>A^TA</m> has eigenvalue <m>\lambda_1=9</m>, and
the eigenvalues of <m>AA^T</m> are <m>\lambda_1=9</m>, <m>\lambda_2=0</m>, and
<m>\lambda_3=0.</m>.
Furthermore, <m>A</m> has only one singular value, <m>\sigma_1=3</m>.
</p> 

<p> 
<m>
\textbf{To find the matrix <m>V</m>}:
</m> 
</p> 

<p> 
To do so we find an eigenvector for <m>A^TA</m> and
normalize it.
In this case, finding a unit eigenvector is trivial:
<m>\mathbf{v}_1=\left[\begin{array}{r} 1 \end{array}\right]</m>, and <m> V=\left[\begin{array}{r} 1 \end{array}\right] </m>.
Also,
<me>
\Sigma =\left[\begin{array}{r} 3 \\ 0\\ 0 \end{array}\right],
</me>

and we use <m>A</m>, <m>V^T</m>, and <m>\Sigma</m> to find <m>U</m>.
</p> 

<p> 
Now, <m>AV=U\Sigma</m> with
<m>V=\left[\begin{array}{r}\mathbf{v}_1 \end{array}\right]</m>,
and <m>U=\left[\begin{array}{rrr} \mathbf{u}_1 \amp  \mathbf{u}_2 \amp  \mathbf{u}_3 \end{array}\right]</m>,
where <m>\mathbf{u}_1</m>, <m>\mathbf{u}_2</m>, and <m>\mathbf{u}_3</m> are the columns of <m>U</m>.
Thus
<md>
<mrow> A\left[\begin{array}{r} \mathbf{v}_1 \end{array}\right]
\amp =  \left[\begin{array}{rrr} \mathbf{u}_1 \amp  \mathbf{u}_2 \amp  \mathbf{u}_3 \end{array}\right]\Sigma </mrow> 
<mrow> \left[\begin{array}{r} A\mathbf{v}_1 \end{array}\right]
\amp =  \left[\begin{array}{r} \sigma_1 \mathbf{u}_1+0\mathbf{u}_2+0\mathbf{u}_3 \end{array}\right] </mrow> 
<mrow> \amp =  \left[\begin{array}{r} \sigma_1 \mathbf{u}_1 \end{array}\right]. </mrow>
</md>


This gives us <m>A\mathbf{v}_1=\sigma_1 \mathbf{u}_1= 3\mathbf{u}_1</m>, so

<me> 
\mathbf{u}_1 = \frac{1}{3}A\mathbf{v}_1
= \frac{1}{3}
\left[\begin{array}{r} -1 \\ 2 \\ 2 \end{array}\right]
\left[\begin{array}{r} 1 \end{array}\right]
= \frac{1}{3}
\left[\begin{array}{r} -1 \\ 2 \\ 2 \end{array}\right].
</me>

The vectors <m>\mathbf{u}_2</m> and <m>\mathbf{u}_3</m> are eigenvectors of <m>AA^T</m> corresponding
to the eigenvalue <m>\lambda_2=\lambda_3=0</m>.
Instead of solving the system <m>(0I-AA^T)\mathbf{x}= 0</m> and then using the
Gram-Schmidt process on the resulting set of
two basic eigenvectors, the following approach may be used.
</p> 

<p> 
Find vectors <m>\mathbf{u}_2</m> and <m>\mathbf{u}_3</m> by first extending <m>\{ \mathbf{u}_1\}</m> to a basis of
<m>\R^3</m>, then using the Gram-Schmidt algorithm to orthogonalize the basis,
and finally normalizing the vectors.

Starting with <m>\{ 3\mathbf{u}_1 \}</m> instead of <m>\{ \mathbf{u}_1 \}</m> makes the
arithmetic a bit easier.
It is easy to verify that

<me> \left\{ \left[\begin{array}{r} -1 \\ 2 \\ 2 \end{array}\right],
\left[\begin{array}{r} 1 \\ 0 \\ 0 \end{array}\right],
\left[\begin{array}{r} 0 \\ 1 \\ 0 \end{array}\right]\right\}</me>
is a basis of <m>\R^3</m>.  Set

<me> \mathbf{x}_1 = \left[\begin{array}{r} -1 \\ 2 \\ 2 \end{array}\right],
\mathbf{x}_2 = \left[\begin{array}{r} 1 \\ 0 \\ 0 \end{array}\right],
\mathbf{x}_3 =\left[\begin{array}{r} 0 \\ 1 \\ 0 \end{array}\right]</me>
and apply the Gram-Schmidt algorithm to
<m>\{ \mathbf{x}_1, \mathbf{x}_2, \mathbf{x}_3\}</m>. This gives us

<me> \mathbf{f}_1 = \left[\begin{array}{r} -1 \\ 2 \\ 2 \end{array}\right], \mathbf{f}_2 = \left[\begin{array}{r} 4 \\ 1 \\ 1 \end{array}\right]
\mbox{ and }
\mathbf{f}_3 = \left[\begin{array}{r} 0 \\ 1 \\ -1 \end{array}\right]. </me>
Therefore,
<me> \mathbf{u}_2 = \frac{1}{\sqrt{18}}
 \left[\begin{array}{r} 4 \\ 1 \\ 1 \end{array}\right],
\mathbf{u}_3 = \frac{1}{\sqrt 2}
\left[\begin{array}{r} 0 \\ 1 \\ -1 \end{array}\right] </me>
and
<me> U = \left[  \begin{array}{rrr} -\frac{1}{3} \amp  \frac{4}{\sqrt{18}} \amp  0 \\
\frac{2}{3} \amp  \frac{1}{\sqrt{18}} \amp  \frac{1}{\sqrt 2} \\
\frac{2}{3} \amp  \frac{1}{\sqrt{18}} \amp  -\frac{1}{\sqrt 2} \end{array}\right] </me>
Finally,
<me> A =
\left[\begin{array}{r} -1 \\ 2 \\ 2 \end{array}\right]
=
\left[ \begin{array}{rrr} -\frac{1}{3} \amp  \frac{4}{\sqrt{18}} \amp  0 \\
\frac{2}{3} \amp  \frac{1}{\sqrt{18}} \amp  \frac{1}{\sqrt 2} \\
\frac{2}{3} \amp  \frac{1}{\sqrt{18}} \amp  -\frac{1}{\sqrt 2} \end{array}\right]
\left[\begin{array}{r} 3 \\ 0 \\ 0 \end{array}\right]
\left[\begin{array}{r} 1 \end{array}\right]. </me>
       </p>
    </answer>
</example>



<example xml:id="SVDanother2x3">
    <statement>
        <p>
            Find an SVD for the matrix
<me>
A= \left[
\begin{array}{ccc}
\frac{2}{5}\sqrt{2}\sqrt{5} \amp  \frac{4}{5}\sqrt{2}\sqrt{5} \amp  0 \\
\frac{2}{5}\sqrt{2}\sqrt{5} \amp  \frac{4}{5}\sqrt{2}\sqrt{5} \amp  0
\end{array}
\right].
</me>
       </p>
    </statement>
    <answer>
        <p>
            First consider <m>A^TA</m>
<me>
\left[
\begin{array}{ccc}
\frac{16}{5} \amp  \frac{32}{5} \amp  0 \\
\frac{32}{5} \amp  \frac{64}{5} \amp  0 \\
0 \amp  0 \amp  0
\end{array}
\right].
</me>

What are some eigenvalues and eigenvectors? Some computing shows that the eigenvalues are <m>0</m> and <m>16</m>.  Furthermore, we can find a basis for each eigenspace.

<me>
\mathcal{S}_0=\mbox{span}\left( \left[
\begin{array}{c}
0 \\
0 \\
1
\end{array}
\right] ,\left[  
\begin{array}{c}
-\frac{2}{5}\sqrt{5} \\
\frac{1}{5}\sqrt{5} \\
0
\end{array}
\right] \right),
\quad\mathcal{S}_{16}=\mbox{span}\left( \left[  
\begin{array}{c}
\frac{1}{5}\sqrt{5} \\
\frac{2}{5}\sqrt{5} \\
0
\end{array}
\right] \right).
</me>
Thus the matrix <m>V</m> is given by
<me>
V=\left[
\begin{array}{ccc}
\frac{1}{5}\sqrt{5} \amp  -\frac{2}{5}\sqrt{5} \amp  0 \\
\frac{2}{5}\sqrt{5} \amp  \frac{1}{5}\sqrt{5} \amp  0 \\
0 \amp  0 \amp  1
\end{array}
\right].
</me>
Next consider <m>AA^T</m>
<me>
\left[
\begin{array}{cc}
8 \amp  8 \\
8 \amp  8
\end{array}
\right].
</me>
Eigenvalues are <m>0</m> and <m>16</m>, and eigenspaces are

<me>
\mathcal{S}_0=\mbox{span}\left(\left[  
\begin{array}{c}
-\frac{1}{2}\sqrt{2} \\
\frac{1}{2}\sqrt{2}
\end{array}
\right] \right),\quad\mathcal{S}_{16}=\mbox{span}\left( \left[  
\begin{array}{c}
\frac{1}{2}\sqrt{2} \\
\frac{1}{2}\sqrt{2}
\end{array}
\right] \right) .
</me>
Thus you can let <m>U</m> be given by
<me>
U=\left[  
\begin{array}{cc}
\frac{1}{2}\sqrt{2} \amp  -\frac{1}{2}\sqrt{2} \\
\frac{1}{2}\sqrt{2} \amp  \frac{1}{2}\sqrt{2}
\end{array}
\right].
</me>
Let's check this. <m>U^TAV=</m>
<me>
\left[  
\begin{array}{cc}
\frac{1}{2}\sqrt{2} \amp  \frac{1}{2}\sqrt{2} \\
-\frac{1}{2}\sqrt{2} \amp  \frac{1}{2}\sqrt{2}
\end{array}
\right] \left[   
\begin{array}{ccc}
\frac{2}{5}\sqrt{2}\sqrt{5} \amp  \frac{4}{5}\sqrt{2}\sqrt{5} \amp  0 \\
\frac{2}{5}\sqrt{2}\sqrt{5} \amp  \frac{4}{5}\sqrt{2}\sqrt{5} \amp  0
\end{array}
\right] \left[ 
\begin{array}{ccc}
\frac{1}{5}\sqrt{5} \amp  -\frac{2}{5}\sqrt{5} \amp  0 \\
\frac{2}{5}\sqrt{5} \amp  \frac{1}{5}\sqrt{5} \amp  0 \\
0 \amp  0 \amp  1
\end{array}
\right]
</me>
<me>
=\left[
\begin{array}{ccc}
4 \amp  0 \amp  0 \\
0 \amp  0 \amp  0
\end{array}
\right].
</me>
       </p>
    </answer>
</example>

<p>
This illustrates that if you have a good way to find the eigenvectors and
eigenvalues for a Hermitian matrix which has nonnegative eigenvalues, then
you also have a good way to find the SVD of an
arbitrary matrix.
</p>



</section>