<?xml version="1.0" encoding="UTF-8"?>

<section xml:id="Section-Isomorphic-Vector-Spaces" xmlns:xi="http://www.w3.org/2001/XInclude">
    <title>Extra Topic: Isomorphic Vector Spaces</title>

<introduction>
<p>
A vector space is defined as a collection of objects together with operations of addition and scalar multiplication that follow certain rules <xref ref="def-vectorspacegeneral"/>.  In our study of abstract vector spaces, we have encountered spaces that appeared very different from each other.  Just how different are they?  Does <m>\mathbb{L}</m>, a vector space whose elements have the form <m>mx+b</m>, have anything in common with <m>\R^2</m>?  Is <m>\mathbb{P}^3</m> fundamentally different from <m>\mathbb{M}_{2,2}</m>?
</p> 

<p>
To answer these questions, we will have to look beyond the superficial appearance of the elements of a vector space and delve into its structure.  The ``structure" of a vector space is determined by how the elements of the vector space interact with each other through the operations of addition and scalar multiplication.  
The first step is to look at when we can ``undo" a linear transformation. 
</p>
</introduction>
    
<subsection xml:id="Subsection-Inverses-of-Linear-Transformations">
    <title>Inverses of Linear Transformations</title>

<exploration xml:id="ep-inverse">
    <p>
        Define a linear transformation <m>T:\R^2\rightarrow \R^2</m> by <m>T(\mathbf{v})=2\mathbf{v}</m>. 
         In other words, <m>T</m> doubles every vector in <m>\R^2</m>.  Now define <m>S:\R^2\rightarrow \R^2</m> by <m>S(\mathbf{v})=\frac{1}{2}\mathbf{v}</m>.  
         What happens when we compose these two transformations?

<me>
    (S\circ T)(\mathbf{v})=S(T(\mathbf{v}))=S(2\mathbf{v})=\left(\frac{1}{2}\right)(2)\mathbf{v}=\mathbf{v},
</me>


<me>
    (T\circ S)(\mathbf{v})=T(S(\mathbf{v}))=T \left (\frac{1}{2}\mathbf{v} \right )=(2)\left(\frac{1}{2}\right)\mathbf{v}=\mathbf{v}.
</me>

Both composite transformations return the original vector <m>\mathbf{v}</m>. 
In other words, <m>S\circ T=\id_{\R^2}</m> and <m>T\circ S=\id_{\R^2}</m>.  
We say that <m>S</m> is an <term>inverse</term> of <m>T</m>, and <m>T</m> is an inverse of <m>S</m>.
    </p>
</exploration>





<definition xml:id="def-inverseoflintrans">

    <statement>
        <p>
            Let <m>V</m> and <m>W</m> be vector spaces, and let <m>T:V\rightarrow W</m> be a linear transformation.  
            A transformation <m>S:W\rightarrow V</m> satisfying <m>S\circ T=\id_V</m> and <m>T\circ S=\id_W</m> is called an <term>inverse</term> of <m>T</m>. 
            If <m>T</m> has an inverse, <m>T</m> is called <term>invertible</term>.
        </p>
    </statement>
</definition>

<example xml:id="ex-inverseverify">
    <statement>
        <p>
            Let <m>T:\R^2\rightarrow \R^2</m> be a transformation defined by 
            <me>
                T\left(\begin{bmatrix}x\\y\end{bmatrix}\right)=\begin{bmatrix}x+y\\x-y\end{bmatrix}
            </me>. 
            
            (ow would you verify that <m>T</m> is linear?).  Show that <m>S:\R^2\rightarrow \R^2</m> given by 
            <me>
            S\left(\begin{bmatrix}x\\y\end{bmatrix}\right)=\begin{bmatrix}0.5x+0.5y\\0.5x-0.5y\end{bmatrix}
            </me> 
            
            is an inverse of <m>T</m>.
       </p>
    </statement>
    <answer>
        <p>
            We will show that <m>S\circ T=\id_{\R^2}</m>.  
<md>
<mrow> (S\circ T)\left(\begin{bmatrix}x\\y\end{bmatrix}\right)\amp =S\left(T\left(\begin{bmatrix}x\\y\end{bmatrix}\right)\right)=S\left(\begin{bmatrix}x+y\\x-y\end{bmatrix}\right) </mrow>
<mrow> \amp =\begin{bmatrix}0.5(x+y)+0.5(x-y)\\0.5(x+y)-0.5(x-y)\end{bmatrix} </mrow>
<mrow> \amp =\begin{bmatrix}x\\y\end{bmatrix}. </mrow>
</md>
We leave it to the reader to verify that <m>T\circ S=\id_{\R^2}</m>.
       </p>
    </answer>
</example>






<p>
<xref ref="def-inverseoflintrans"/> does not specifically require an inverse <m>S</m> of a linear transformation <m>T</m> to be linear, but it turns out that the requirement that <m>S\circ T=\id_V</m> and <m>T\circ S=\id_W</m> is sufficient to guarantee that <m>S</m> is linear.  
</p> 

<theorem xml:id="th-inverseislinear">

    <statement>
        <p>
            Suppose <m>T:V\rightarrow W</m> is an invertible linear transformation.  Let <m>S</m> be an inverse of <m>T</m>.  Then <m>S</m> is  linear.
        </p>
    </statement>


<proof>
    <p> 
    The proof of this result is left to the reader. (See  <xref ref="prob-inverseislinear"/>)
    </p>
</proof>
</theorem>

<p>
It is natural to ask when a linear transformation has an inverse.  Can we recognize them?  And we have the answer already,
in the concepts of injective (<xref ref="def-onetoone"/>) and surjective (<xref ref="def-onto"/>) from <xref ref="Section-Linear-Transformations"/>.
</p>

<theorem xml:id="th-isomeansinvert">

    <statement>
        <p>
            Let <m>V</m> and <m>W</m> be vector spaces, and let <m>T:V\rightarrow W</m> be a linear transformation.  Then <m>T</m> has an inverse if and only if <m>T</m> is injective and surjective.
        </p>
    </statement>

<proof>
    <p>
We will first assume that <m>T</m> is injective and surjective, and show that there exists a transformation <m>S:W\rightarrow V</m> such that <m>S\circ T=\id_V</m> and <m>T\circ S=\id_W</m>. 
    </p>
    
<p>
Because <m>T</m> is surjective, for every <m>\mathbf{w}</m> in <m>W</m>, there exists <m>\mathbf{v}</m> in <m>V</m> such that <m>T(\mathbf{v})=\mathbf{w}</m>.  Moreover, because <m>T</m> is injective, vector <m>\mathbf{v}</m> is the only vector that maps to <m>\mathbf{w}</m>.  To stress this, we will say that for every <m>\mathbf{w}</m>, there exists <m>\mathbf{v}_{\mathbf{w}}</m> such that <m>T(\mathbf{v}_{\mathbf{w}})=\mathbf{w}</m>. 
(Since every <m>\mathbf{v}</m> maps to exactly one <m>\mathbf{w}</m>, this notation makes sense for elements of <m>V</m> as well.)  We can now define <m>S:W\rightarrow V</m> by <m>S(\mathbf{w})=\mathbf{v}_{\mathbf{w}}</m>.
Then

<md>
<mrow>    (S\circ T)(\mathbf{v}_{\mathbf{w}})  \amp=  S(T(\mathbf{v}_{\mathbf{w}})) \amp =S (\mathbf{w}) = \mathbf{v}_{\mathbf{w}}, </mrow>
<mrow>    (T\circ S)(\mathbf{w})               \amp=  T(S(\mathbf{w}))              \amp =T (\mathbf{v}_{\mathbf{w}}) = \mathbf{w}. </mrow>
</md>

We conclude that <m>S\circ T=\id_V</m> and <m>T\circ S=\id_W</m>.  Therefore <m>S</m> is an inverse of <m>T</m>.

We will now assume that <m>T</m> has an inverse <m>S</m> and show that <m>T</m> must be injective and surjective.  
Suppose 
<me>
    T(\mathbf{v}_1)=T(\mathbf{v}_2).
</me>
 then 
<me>
    S(T(\mathbf{v}_1))=S(T(\mathbf{v}_2)),
</me>

but then

<me>
    \mathbf{v}_1=\mathbf{v}_2.
</me>

We conclude that <m>T</m> is injective.

Now suppose that <m>\mathbf{w}</m> is in <m>W</m>.  We need to show that some element of <m>V</m> maps to <m>\mathbf{w}</m>.  Let <m>\mathbf{v}=S(\mathbf{w})</m>.  Then

<me>
    T(\mathbf{v})=T(S(\mathbf{w}))=(T\circ S)(\mathbf{w})=\id_W(\mathbf{w})=\mathbf{w}.
</me>

We conclude that <m>T</m> is surjective.
    </p>
</proof>
</theorem>

</subsection>

<subsection xml:id="Subsection-SameVectorSpace">
    <title>When are two vector spaces the same?</title>

<p>
Let us return to the question of what <m>\mathbb{L}</m> has in common with <m>\R^2</m>.  Consider two typical elements of <m>\mathbb{L}</m>:
<men xml:id="eq-iso1">
    mx+b\quad\text{and}\quad qx+c.
</men>

We can add these elements together
<men xml:id="eq-iso2">
(mx+b)+(qx+c)=(m+q)x+(b+c)
</men>

or multiply each one by a scalar
<men xml:id="eq-iso3">
k(mx+b)=kmx+kb\quad\text{and}\quad t(qx+c)=tqx+tc.
</men>

But suppose we get tired of having to write  <m>x</m> down every time.  Could we leave off the <m>x</m> and represent <m>mx+b</m> by  <m>[m,b]</m>?  If we do this, expressions <xref ref="eq-iso1"/>, <xref ref="eq-iso2"/> and <xref ref="eq-iso3"/> would be mimicked by the following expressions involving vectors of <m>\R^2</m>:

<me>
    \begin{bmatrix}m\\b\end{bmatrix}\quad\text{and}\quad\begin{bmatrix}q\\c\end{bmatrix},
</me>


<me>
    \begin{bmatrix}m\\b\end{bmatrix}+\begin{bmatrix}q\\c\end{bmatrix}=\begin{bmatrix}m+q\\b+c\end{bmatrix},
</me>


<me>
    k\begin{bmatrix}m\\b\end{bmatrix}=\begin{bmatrix}km\\kb\end{bmatrix}\quad\text{and}\quad t\begin{bmatrix}q\\c\end{bmatrix}=\begin{bmatrix}tq\\tc\end{bmatrix}.
</me>

It appears that we should be able to switch back and forth between <m>\mathbb{L}</m> and <m>\R^2</m>, translating questions and answers from one space to the other and back again.  
</p>

 <image width="65%">
   <shortdescription>Picture of isomorphism</shortdescription>
    <latex-image>
      \begin{tikzpicture} 
   \fill[blue, opacity=0.3] (0,0) rectangle (5,5);
   \fill[pink, opacity=0.5] (6,0) rectangle (11,5);
   
   \node[] at (0.5, 4.5)  (p2)    {\(\mathbb{L}\)};
   \node[] at (10.5, 4.5)  (r3)    {\(\R^2\)};
   
   \node[] at (2, 3)  (p_1)    {\((mx+b)+(qx+c)\)};
   \node[] at (2, 1)  (p_4)    {\((m+q)x+(b+c)\)};
   \node[] at (2, 2)  (eq1)    {{\rotatebox{90}{\(=\)}}};
   \node[] at (8.5, 2)  (eq2)    {{\rotatebox{90}{\(=\)}}};
     
     \node[] at (8.5, 3)  (v_1)    {\(\begin{bmatrix}m\\b\end{bmatrix}+\begin{bmatrix}q\\c\end{bmatrix}\)};
  \node[] at (8.5, 1.2)  (v_4)    {\(\begin{bmatrix}m+q\\b+c\end{bmatrix}\)};
     
     \draw [-\gt ,line width=0.5pt,-stealth]  (3.6, 3)to[out=10, in=170](7.4,3);
     \draw [-\gt ,line width=0.5pt,-stealth]  (p_4.east)to[out=10, in=170](v_4.west);
     
     \draw [-\gt ,line width=0.5pt,-stealth]  (7.4,2.9)to[out=190, in=350](3.6, 2.9);
     \draw [-\gt ,line width=0.5pt,-stealth]  (7.7,1.1)to[out=190, in=350](3.6,0.9 );
     
      \end{tikzpicture}
    </latex-image>
</image>


<p>
We begin to suspect that <m>\mathbb{L}</m> and <m>\R^2</m> have the same ``structure".  Spaces such as <m>\mathbb{L}</m> and <m>\R^2</m> are said to be <term>isomorphic</term>.  This term is derived from the Greek ``iso," meaning ``same," and ``morphe," meaning ``form."  The term captures the idea that isomorphic vector spaces have the same structure.    Before we present a precise definition of the term, we need to better understand what we mean by  ``switching back and forth" between spaces.  The following exploration will help us formulate this vague notion in terms of transformations.
</p>


<exploration xml:id="init-isomorph">
    <p>
        Recall that the set of all polynomials of degree <m>2</m> or less, together with polynomial addition and scalar multiplication, is a vector space, denoted by <m>\mathbb{P}^2</m>.  Let <m>\mathcal{B}=\{1, x, x^2\}</m>. You should do a quick mental check that <m>\mathcal{B}</m> is a basis of <m>\mathbb{P}^2</m>.

Define a transformation <m>T:\mathbb{P}^2\rightarrow \R^3</m> by
<me>
T(a+bx+cx^2)=\begin{bmatrix}a\\b\\c\end{bmatrix}.
</me>

You may have recognized <m>T</m> as the transformation that maps each element of <m>\mathbb{P}^2</m> to its coordinate vector with respect to the ordered basis <m>\mathcal{B}</m>.  

Transformation <m>T</m> has several nice properties:
  <ol>
      <li>
      <p> By <xref ref="th-coordvectmappinglinear"/>, <m>T</m> is linear. 
    </p>
</li>
      <li>
      <p> It is easy to verify that <m>T</m> is injective and surjective (see <xref ref="prob-Tonetooneonto"/>.)
    </p>
</li>
      <li>
      <p> By <xref ref="th-isomeansinvert"/>, <m>T</m> has an inverse. 
      </p>
</li>
  </ol>
  
  Our goal is to investigate and illustrate what these properties mean for transformation <m>T</m>, and for the relationship between <m>\mathbb{P}^2</m> and <m>\R^3</m>.  
  
  First, observe that <m>T</m> being injective and surjective establishes ``pairings" between elements of <m>\mathbb{P}^2</m> and <m>\R^3</m> in such a way that every element of one vector space is uniquely matched with exactly one element of the other vector space, as shown in the diagram below.
</p> 



 <image width="65%">
   <shortdescription>The above isomorphism elaborated</shortdescription>
    <latex-image>
      \begin{tikzpicture} 
   \fill[blue, opacity=0.3] (0,0) rectangle (5,5);
   \fill[pink, opacity=0.5] (6,0) rectangle (11,5);
   
   \node[] at (0.5, 4.5)  (p2)    {\(\mathbb{P}^2\)};
   \node[] at (10.5, 4.5)  (r3)    {\(\R^3\)};
   
   \node[] at (2.5, 4.2)  (p_1)    {\(1+2x-3x^2\)};
   \node[] at (1.2, 2.8)  (p_2)    {\(-4\)};
    \node[] at (3, 1.8)  (p_3)    {\(x+5\)};
     \node[] at (2, 0.6)  (p_4)    {\(a+bx+cx^2\)};
     
     \node[] at (8.5, 4.2)  (v_1)    {\(\begin{bmatrix}1\\2\\-3\end{bmatrix}\)};
   \node[] at (7.2, 3.2)  (v_2)    {\(\begin{bmatrix}-4\\0\\0\end{bmatrix}\)};
    \node[] at (9, 1.9)  (v_3)    {\(\begin{bmatrix}5\\1\\0\end{bmatrix}\)};
     \node[] at (7.5, 0.8)  (v_4)    {\(\begin{bmatrix}a\\b\\c\end{bmatrix}\)};
     
     \draw [-\gt ,line width=0.5pt,-stealth]  (3.6, 4.2)to[out=10, in=170](7.9,4.2);
     \draw [-\gt ,line width=0.5pt,-stealth]  (p_2.east)to[out=10, in=170](v_2.west);
     \draw [-\gt ,line width=0.5pt,-stealth]  (p_3.east)to[out=10, in=170](v_3.west);
     \draw [-\gt ,line width=0.5pt,-stealth]  (p_4.east)to[out=10, in=170](v_4.west);
     
     \draw [-\gt ,line width=0.5pt,-stealth]  (7.9,4.1)to[out=190, in=350](3.6, 4.1);
     \draw [-\gt ,line width=0.5pt,-stealth]  (6.5,3.1)to[out=190, in=350](1.6,2.7 );
     \draw [-\gt ,line width=0.5pt,-stealth]  (8.5,1.8)to[out=190, in=350](3.6,1.7 );
     \draw [-\gt ,line width=0.5pt,-stealth]  (7,0.7)to[out=190, in=350](3,0.5 );
     
     \draw [-\gt ,line width=0.5pt,-stealth]  (8.5,-0.1)to[out=200, in=340](2.5,-0.1 );
     \draw [-\gt ,line width=0.5pt,-stealth]  (2.5,5.1)to[out=20, in=160](8.5, 5.1);
     
     \node[] at (5.5, 6)    {\(T\)};
      \node[] at (5.5, -1)    {\(T^{-1}\)};
      \end{tikzpicture}
    </latex-image>
</image>

<p>
  Second, the fact that <m>T</m> (and <m>T^{-1}</m>) are linear will allow us to translate questions related to linear combinations in one of the vector spaces to equivalent questions in the other vector space, then translate answers back to the original vector space.  To make this statement concrete, consider the following problem:
</p> 

<p>
  Let 
<me>
    p_1(x)=3-x+2x^2\quad\text{and}\quad p_2(x)=-1+3x+x^2.
</me>
 
Find <m>p_1(x)+p_2(x)</m>.  
  
  The answer is, of course
  
<me>
    p_1(x)+p_2(x)=2+2x+3x^2.
</me>

  Easy.  But suppose for a moment that we did not know how to add polynomials, or that we found the process extremely difficult, or maybe instead of <m>\mathbb{P}^2</m> we had another vector space that we did not want to deal with. 
  
  It turns out that we can use <m>T</m> and <m>T^{-1}</m> to answer the addition question.  We will start by applying <m>T</m> to <m>p_1(x)</m> and <m>p_2(x)</m> separately:
  
<me>
    T(p_1(x))=\begin{bmatrix}3\\-1\\2\end{bmatrix},\quad T(p_2(x))=\begin{bmatrix}-1\\3\\1\end{bmatrix}.
</me>

  Next, we add the images of <m>p_1(x)</m> and <m>p_2(x)</m> in <m>\R^3</m>:
  
<me>
    \begin{bmatrix}3\\-1\\2\end{bmatrix}+\begin{bmatrix}-1\\3\\1\end{bmatrix}=\begin{bmatrix}2\\2\\3\end{bmatrix}.
</me>

  This maneuver allows us to avoid the addition question in <m>\mathbb{P}^2</m> and answer the question in <m>\R^3</m> instead.  We use <m>T^{-1}</m> to translate the answer back to <m>\mathbb{P}^2</m>:
  
<me>
    T^{-1}\left(\begin{bmatrix}2\\2\\3\end{bmatrix}\right)=2+2x+3x^2.
</me>

  All of this relies on linearity.  Here is a formal justification for the process.  Try to spot where linearity is used.
  <mdn>
  <mrow number="yes" xml:id="steplin1">    p_1(x)+p_2(x)\amp =(3-x+2x^2)+(-1+3x+x^2) </mrow>
    <mrow number="yes" xml:id="steplin2">     \amp =T^{-1}\left(\begin{bmatrix}3\\-1\\2\end{bmatrix}\right)+T^{-1}\left(\begin{bmatrix}-1\\3\\1\end{bmatrix}\right) </mrow>
        <mrow number="yes" xml:id="steplin3">   \amp =T^{-1}\left(\begin{bmatrix}3\\-1\\2\end{bmatrix}+\begin{bmatrix}-1\\3\\1\end{bmatrix}\right) </mrow>
            <mrow number="yes"  xml:id="steplin4"> \amp =T^{-1}\left(\begin{bmatrix}2\\2\\3\end{bmatrix}\right) </mrow>
                <mrow number="yes" xml:id="steplin5">  \amp =2+2x+3x^2 </mrow>
  </mdn>
</p>

<problem>
<statement>
<p>
    Which transition in the above calculation requires linearity?
</p>
</statement>

  <choices>
  <choice>
      <statement>
          <p> From <xref ref="steplin1"/> to <xref ref="steplin2"/>. </p>
      </statement>
  </choice>
    <choice correct="yes">
      <statement>
          <p> From <xref ref="steplin2"/> to <xref ref="steplin3"/>. </p>
      </statement>
  </choice>
    <choice>
      <statement>
          <p> From <xref ref="steplin3"/> to <xref ref="steplin4"/>. </p>
      </statement>
  </choice>
    <choice>
      <statement>
          <p> From <xref ref="steplin4"/> to <xref ref="steplin5"/>. </p>
      </statement>
  </choice>
</choices>
</problem> 
</exploration>



<p>
Invertible linear transformations, such as transformation <m>T</m> of <xref ref="init-isomorph"/>, are useful because they preserve the structure of interactions between elements as we move back and forth between two vector spaces, allowing us to answer questions about one vector space in a different vector space.  In particular, any question related to linear combinations can be addressed in this fashion. This includes questions concerning linear independence, span, basis and dimension. 
</p>

<definition xml:id="def-isomorphism">

    <statement>
        <p>
            Let <m>V</m> and <m>W</m> be vector spaces.  If there exists an invertible linear transformation <m>T:V\rightarrow W</m> we say that <m>V</m> and <m>W</m> are <term>isomorphic</term> and write <m>V\cong W</m>. 
        </p>
        
        <p>
        The invertible linear transformation <m>T</m> is called an <term>isomorphism</term>.
        </p>
    </statement>
</definition>

<p>
It is worth pointing out that if <m>T:V\rightarrow W</m> is an isomorphism, then <m>T^{-1}:W\rightarrow V</m>, being linear and invertible, is also an isomorphism.
</p>

<p>
While all this has been fairly abstract, previous examples do have this content attached to them.
</p>

<example xml:id="ex-lisor2">
    <p>
        Our earlier discussion suggests that <m>\mathbb{L}\cong\R^2</m>.  We postpone the proof until <xref ref="ex-coordmapiso"/>.
    </p>
</example>

<example xml:id="ex-p2isor3">
    <p>
     <xref ref="init-isomorph"/> shows that <m>\mathbb{P}^2\cong\R^3</m>.
    </p>
</example>



<example xml:id="ex-isomorphexample1">
    <statement>
        <p>
            Show that <m>\mathbb{M}_{2,2}</m> and <m>\mathbb{P}^3</m> are isomorphic.
       </p>
    </statement>
    <answer>
        <p>
            We will start by finding a plausible candidate for an isomorphism.  Define <m>T:\mathbb{M}_{2,2}\rightarrow \mathbb{P}^3</m> by

<me>
    T\left(\begin{bmatrix}a\amp b\\c\amp d\end{bmatrix}\right)=a+bx+cx^2+dx^3.
</me>


We will first show that <m>T</m> is a linear transformation,  then verify that <m>T</m> is invertible.
<md>
<mrow>    T\left(k\begin{bmatrix}a\amp b\\c\amp d\end{bmatrix}\right)\amp =T\left(\begin{bmatrix}ka\amp kb\\kc\amp kd\end{bmatrix}\right) </mrow> 
<mrow>    \amp =ka+kbx+kcx^2+kdx^3=k(a+bx+cx^2+dx^3) </mrow>
<mrow>    \amp =kT\left(\begin{bmatrix}a\amp b\\c\amp d\end{bmatrix}\right) </mrow>
</md>

and
<md>
<mrow>    T\left(\begin{bmatrix}a\amp b\\c\amp d\end{bmatrix}+\begin{bmatrix}m\amp n\\p\amp q\end{bmatrix}\right)\amp =T\left(\begin{bmatrix}a+m\amp b+n\\c+p\amp d+q\end{bmatrix}\right) </mrow>
<mrow>    \amp =(a+m)+(b+n)x+(c+p)x^2+(d+q)x^3 </mrow>
<mrow>   \amp =(a+bx+cx^2+dx^3)+(m+nx+px^2+qx^3) </mrow>
<mrow>   \amp =T\left(\begin{bmatrix}a\amp b\\c\amp d\end{bmatrix}\right)+T\left(\begin{bmatrix}m\amp n\\p\amp q\end{bmatrix}\right) </mrow>
</md>
We can show that <m>T</m> is injective and surjective, and therefore has an inverse.  We can also observe directly that <m>T^{-1}</m> is given by 

<me>
    T^{-1}(a+bx+cx^2+dx^3)=\begin{bmatrix}a\amp b\\c\amp d\end{bmatrix}.
</me>

We conclude that <m>T</m> is an isomorphism, and <m>\mathbb{M}_{2,2}\cong\mathbb{P}^3</m>.
       </p>
    </answer>
</example>



<p>
The isomorphism <m>T</m> in <xref ref="ex-isomorphexample1"/> establishes the fact that <m>\mathbb{M}_{2,2}\cong\mathbb{P}^3</m>. However, there is nothing special about <m>T</m>, as there are many other isomorphisms from <m>\mathbb{M}_{2,2}</m> to <m>\mathbb{P}^3</m>.  Just for fun, try to verify that each of the following is an isomorphism.

<men xml:id="eq-justforfuniso1">
    \tau_1:\mathbb{M}_{2,2}\rightarrow\mathbb{P}^3; \quad \  \tau_1\left(\begin{bmatrix}a\amp b\\c\amp d\end{bmatrix}\right)=a-bx-cx^2+dx^3,,
</men>


<men xml:id="eq-justforfuniso2">
\tau_2:\mathbb{M}_{2,2}\rightarrow\mathbb{P}^3; \quad \  \tau_2\left(\begin{bmatrix}a\amp b\\c\amp d\end{bmatrix}\right)=a+(a+c)x+(b-c)x^2+dx^3.
</men>
</p>

</subsection>





<subsection xml:id="The-Coordinate-Vector-Isomorphism">
<title>The Coordinate Vector Isomorphism</title>


<p>
In <xref ref="init-isomorph"/> we made good use of a transformation that maps every element of <m>\mathbb{P}^2</m> to its coordinate vector in <m>\R^3</m>.  We observed that this transformation is linear and invertible, therefore it is an isomorphism.  The following example generalizes this result.
</p> 


<theorem xml:id="ex-coordmapiso">

    <statement>
        <p>
            Let <m>V</m> be an <m>n</m>-dimensional vector space, and let <m>\mathcal{B}</m> be an ordered basis for <m>V</m>.  Then  <m>T:V\rightarrow \R^n</m> given by <m>T(\mathbf{v})=[\mathbf{v}]_{\mathcal{B}}</m> is an isomorphism.
        </p>
    </statement>

<proof>
    <p>
We leave the proof of this result to the reader (see <xref ref="prob-verifyisomorphism"/>).
    </p>
</proof>
</theorem>
</subsection>










<subsection xml:id="Subsection-Properties-of-Isomorphic-Vector-Spaces-and-Isomorphisms">
    <title>Properties of Isomorphic Vector Spaces and Isomorphisms</title>

    <p>
In this section we will illustrate properties of isomorphisms with specific examples.  Formal proofs of properties will be presented in the next section.
    </p> 

<exploration xml:id="init-basestobasesiso">
    <p>
        In <xref ref="init-isomorph"/> we defined a transformation <m>T:\mathbb{P}^2\rightarrow \R^3</m> by
        <me>
        T(a+bx+cx^2)=\begin{bmatrix}a\\b\\c\end{bmatrix}.
        </me>
        
        We later observed that  <m>T</m> is an isomorphism.  We will now examine the effect of <m>T</m> on two different bases of <m>\mathbb{P}^2</m>.

Let 
<me>
\mathcal{B}_1=\{1, x, x^2\}\quad \text{and} \quad \mathcal{B}_2=\{x, 1+x, x+x^2\}.
</me> 

(Recall that <m>\mathcal{B}_2</m> is a basis of <m>\mathbb{P}^2</m> by <xref ref="ex-coordvectorinpolyvectspace2"/>). First,

<me>
    T(\mathcal{B}_1)=\{T(1), T(x), T(x^2)\}=\left\{\begin{bmatrix}1\\0\\0\end{bmatrix}, \begin{bmatrix}0\\1\\0\end{bmatrix}, \begin{bmatrix}0\\0\\1\end{bmatrix}\right\}.
</me>

Clearly, the images of the elements of <m>\mathcal{B}_1</m> form a basis of <m>\R^3</m>.

Now we consider <m>\mathcal{B}_2</m>.

<me>
    T(\mathcal{B}_2)=\{T(x), T(1+x), T(x+x^2)\}=\left\{\begin{bmatrix}0\\1\\0\end{bmatrix}, \begin{bmatrix}1\\1\\0\end{bmatrix}, \begin{bmatrix}0\\1\\1\end{bmatrix}\right\}.
</me>

It is easy to verify that 
<me>
\begin{bmatrix}0\\1\\0\end{bmatrix}, \quad \begin{bmatrix}1\\1\\0\end{bmatrix}, \quad \begin{bmatrix}0\\1\\1\end{bmatrix}
</me>

are linearly independent and span <m>\R^3</m>. Therefore the images of the elements of <m>\mathcal{B}_2</m> from a basis of <m>\R^3</m>.
    </p>
</exploration>

<p>
We can try any number of bases of <m>\mathbb{P}^2</m> and we will find that the image of each basis of <m>\mathbb{P}^2</m> is a basis of <m>\R^3</m>.  In general, we have the following result:
</p>

<fact>
    <statement>
        <p>
            An isomorphism maps a basis of the domain to a basis of the codomain. (We will state this result more formally as <xref ref="th-bijectionsbasis"/> in the next subsection.)
        </p>
    </statement>
</fact>

<p>
Isomorphisms preserve bases, but more generally, they preserve linear independence.  
</p>

<fact>
    <statement>
        <p>
            If <m>T:V\rightarrow W</m> is an isomorphism, then the subset <m>\{\mathbf{v_1}, \mathbf{v}_2,\ldots ,\mathbf{v}_n\}</m> of <m>V</m> is linearly independent if and only if 
            <me>
            \{T(\mathbf{v_1}), T(\mathbf{v}_2),\ldots ,T(\mathbf{v}_n)\}
            </me>
            
            is linearly independent in <m>W</m>. We will state and prove this result as <xref ref="th-linindtolinindiso"/>.
        </p>
    </statement>
</fact>


<p> 
The example below flags up the idea that there is little differences between the abstract case and the concrete one in <m> \R^n </m>.
</p> 

<example xml:id="ex-inverseimageoflinind">
    <statement>
        <p>
            Let <m>V</m> be a vector space, and let <m>\mathcal{B}=\{\mathbf{v}_1, \mathbf{v}_2, \mathbf{v}_3, \mathbf{v}_4\}</m> be an ordered basis of <m>V</m>.  Let 

<md>
<mrow>    \mathbf{w}_1 \amp = 2\mathbf{v}_1+3\mathbf{v}_2-5\mathbf{v}_3-2\mathbf{v}_4, </mrow>
<mrow>    \mathbf{w}_2 \amp = -\mathbf{v}_1+4\mathbf{v}_2-3\mathbf{v}_3+\mathbf{v}_4, </mrow>
<mrow>    \mathbf{w}_3 \amp = -3\mathbf{v}_1-\mathbf{v}_2+4\mathbf{v}_3+3\mathbf{v}_4. </mrow>
</md>


Are <m>\mathbf{w}_1, \mathbf{w}_2, \mathbf{w}_3</m> linearly independent?
       </p>
    </statement>

    <answer>
        <p>
            We could approach this question head-on by considering the vector equation

<me>
    a\mathbf{w}_1+b\mathbf{w}_2+c\mathbf{w}_3=\mathbf{0}
</me>

to see if the only solution is the trivial one (see <xref ref="prob-noiso"/>).

Instead, we will use isomorphisms.
</p>

<p> 
Observe that we do not know anything about <m>V</m> aside from the fact that it has four basis vectors.  Vectors <m>\mathbf{w}_1</m>, <m>\mathbf{w}_2</m>, <m>\mathbf{w}_3</m> are given in terms of these basis vectors.  This should give us an idea for constructing an isomorphism between <m>V</m> and <m>\R^4</m>.  Consider <m>T:V\rightarrow\R^4</m> such that <m>T(\mathbf{w})=[\mathbf{w}]_{\mathcal{B}}</m>.
Then 
<me>
    T(\mathbf{w}_1)=[\mathbf{w}_1]_{\mathcal{B}}=\begin{bmatrix}2\\3\\-5\\-2\end{bmatrix},\quad T(\mathbf{w}_2)=[\mathbf{w}_2]_{\mathcal{B}}=\begin{bmatrix}-1\\4\\-3\\1\end{bmatrix} ,
</me>
<me>
T(\mathbf{w}_3)=[\mathbf{w}_3]_{\mathcal{B}}=\begin{bmatrix}-3\\-1\\4\\3\end{bmatrix}.    
</me>

By <xref ref="ex-coordmapiso"/>, <m>T</m>   is an isomorphism.  This means that <m>\mathbf{w}_1</m>, <m>\mathbf{w}_2</m>, <m>\mathbf{w}_3</m> are linearly independent if and only if their coordinate vectors are linearly independent.  There are multiple ways of determining whether 

<me>
    \begin{bmatrix}2\\3\\-5\\-2\end{bmatrix}, \begin{bmatrix}-1\\4\\-3\\1\end{bmatrix}, \begin{bmatrix}-3\\-1\\4\\3\end{bmatrix}
</me>

are linearly independent.  One way is to find the reduced row echelon form of 

<me>
    \begin{bmatrix}2\amp -1\amp -3\\3\amp 4\amp -1\\-5\amp -3\amp 4\\-2\amp 1\amp 3\end{bmatrix}
</me>

The matrix reduces as follows:

<me>
    \begin{bmatrix}2\amp -1\amp -3\\3\amp 4\amp -1\\-5\amp -3\amp 4\\-2\amp 1\amp 3\end{bmatrix}\rightsquigarrow\begin{bmatrix}1\amp 0\amp -13/11\\0\amp 1\amp 7/11\\0\amp 0\amp 0\\0\amp 0\amp 0\end{bmatrix}
</me>

We see that the rank of the matrix is <m>2</m>.  We conclude that the column vectors are not linearly independent.   Thus, the vectors <m>\mathbf{w}_1</m>, <m>\mathbf{w}_2</m> and <m>\mathbf{w}_3</m> are not linearly independent.
       </p>
    </answer>
</example>
</subsection>








<subsection xml:id="Subsection-Proofs-of-Isomorphism-Properties">
    <title>Proofs of Isomorphism Properties</title>

<p>
Recall that a transformation <m>T</m> is <term>injective</term> provided that 
<me>
    T(\mathbf{v}_1)=T(\mathbf{v}_2)
</me>
 implies that 
<me>
    \mathbf{v}_1=\mathbf{v}_2.
</me>


 We will show that images of linearly independent vectors under an injective linear transformations are linearly independent.  
</p>


<theorem xml:id="th-onetoonelinind">

    <statement>
        <p>
            Let <m>T:V\rightarrow W</m> be an injectivelinear transformation.  Suppose <m>\{\mathbf{v}_1,\ldots,\mathbf{v}_n\}</m> is linearly independent in <m>V</m>.  Then <m>\{T(\mathbf{v}_1),\ldots,T(\mathbf{v}_n)\}</m> is linearly independent in <m>W</m>.
        </p>
    </statement>


<proof>
    <p>
Suppose <m>a_1, \ldots, a_n</m> satisfy
<men xml:id="onlysolution">
a_1T(\mathbf{v}_1)+\ldots +a_nT(\mathbf{v}_n)=\mathbf{0}.
</men>


We will show that for each <m>i</m>, we must have <m>a_i=0</m>. By linearity, we have:
<md>
<mrow> \mathbf{0}\amp =a_1T(\mathbf{v}_1)+\ldots +a_nT(\mathbf{v}_n) </mrow>
<mrow> \amp =T(a_1\mathbf{v}_1)+\ldots +T(a_n\mathbf{v}_n) </mrow>
<mrow> \amp =T(a_1\mathbf{v}_1+\ldots +a_n\mathbf{v}_n).  </mrow>
</md>


By <xref ref="th-zerotozero"/>, <m>T(\mathbf{0})=\mathbf{0}</m>.  Therefore,

<me>
    T(a_1\mathbf{v}_1+\ldots +a_n\mathbf{v}_n)=T(\mathbf{0}).
</me>


Because <m>T</m> is injective, we conclude that 
<men xml:id="onlytrivial">
a_1\mathbf{v}_1+\ldots +a_n\mathbf{v}_n=\mathbf{0}.
</men>


By assumption, <m>\{\mathbf{v}_1,\ldots,\mathbf{v}_n\}</m> is linearly independent.  Therefore <m>a_i=0</m> for <m>1\leq i\leq n</m>.  
    </p>
</proof>
</theorem>


<p>
Recall that a transformation <m>T</m> is <term>surjective</term> provided that every vector of the codomain of <m>T</m> is the image of some vector in the domain of <m>T</m>.  

We will show that a surjectivelinear transformation maps sets that span the domain to sets that span the codomain. 
</p>





<theorem xml:id="th-ontospan">

    <statement>
        <p>
            Let <m>T:V\rightarrow W</m> be a surjective linear transformation.  Suppose <m>V=\mbox{span}(\mathbf{v}_1,\ldots ,\mathbf{v}_n)</m>.  Then <m>W=\mbox{span}(T(\mathbf{v}_1),\ldots ,T(\mathbf{v}_n))</m>.
        </p>
    </statement>

<proof>
    <p>
Suppose <m>\mathbf{w}</m> is an element of <m>W</m>. To show that <m>\{T(\mathbf{v}_1),\ldots ,T(\mathbf{v}_n)\}</m> spans <m>W</m>, we will express <m>\mathbf{w}</m> as a linear combination of <m>T(\mathbf{v}_1),\ldots ,T(\mathbf{v}_n)</m>.
    </p>

    <p> 
Because <m>T</m> is onto, <m>\mathbf{w}=T(\mathbf{v})</m> for some <m>\mathbf{v}</m> in <m>V</m>.  But <m>V=\mbox{span}(\mathbf{v}_1,\ldots ,\mathbf{v}_n)</m>.  Therefore, 
<me>
\mathbf{v}=a_1\mathbf{v}_1+\ldots +a_n\mathbf{v}_n
</me>

for some scalar coefficients <m>a_1,\ldots ,a_n</m>.
By linearity, we have:
<md>
<mrow> \mathbf{w} \amp =T(\mathbf{v}) </mrow> 
<mrow> \amp =T(a_1\mathbf{v}_1+\ldots +a_n\mathbf{v}_n) </mrow> 
<mrow> \amp =a_1T(\mathbf{v}_1)+\ldots +a_nT(\mathbf{v}_n). </mrow>
</md>

Thus, <m>\mathbf{w}</m> is in the span of <m>T(\mathbf{v}_1),\ldots ,T(\mathbf{v}_n)</m>.
    </p>
</proof>
</theorem>

<p> 
We will now combine the results of <xref ref="th-onetoonelinind"/> and <xref ref="th-ontospan"/> to obtain a result about the effect of isomorphisms on a basis.
</p> 


<theorem xml:id="th-bijectionsbasis">

    <statement>
        <p>
            Let <m>T:V\rightarrow W</m> be an isomorphism.  Suppose <m>\mathcal{B}_V=\{\mathbf{v}_1,\ldots ,\mathbf{v}_n\}</m> is a basis for <m>V</m>.  Then 
            <me>
            \{T(\mathbf{v}_1),\ldots ,T(\mathbf{v}_n)\}
            </me>
            
            is a basis for <m>W</m>.
        </p>
    </statement>
</theorem>
<proof>
    <p>
Left to the reader. See <xref ref="prob-bijectionsbasisproof"/>.
    </p>
</proof>

<theorem xml:id="th-linindtolinindiso">

    <statement>
        <p>
            Suppose <m>T:V\rightarrow W</m> is an isomorphism, then the subset <m>\{\mathbf{v_1}, \mathbf{v}_2,\ldots ,\mathbf{v}_n\}</m> '
            of <m>V</m> is linearly independent if and only if 
            <me>\{T(\mathbf{v_1}), T(\mathbf{v}_2),\ldots ,T(\mathbf{v}_n)\}</me> 
        is linearly independent in <m>W</m>.
        </p>
    </statement>

<proof>
    <p>
We have already proved one direction of this this ``if and only if" statement as <xref ref="th-onetoonelinind"/>. 
    </p>
    
    <p>
    To prove the other direction, suppose that <m>T(\mathbf{v_1}), T(\mathbf{v}_2),\ldots ,T(\mathbf{v}_n)</m> are linearly independent vectors in <m>W</m>.  
    We need to show that this implies that <m>\mathbf{v_1}, \mathbf{v}_2,\ldots ,\mathbf{v}_n</m> are linearly independent in <m>V</m>.  
    Observe that if <m>T</m> is an isomorphism, then <m>T^{-1}:W\rightarrow V</m> is also an isomorphism.  Thus, by <xref ref="th-onetoonelinind"/>,
    <me>
    T^{-1}(T(\mathbf{v_1})), T^{-1}(T(\mathbf{v}_2)),\ldots ,T^{-1}(T(\mathbf{v}_n))
    </me>
    
    are linearly independent.  But this means that <m>\mathbf{v_1}, \mathbf{v}_2,\ldots ,\mathbf{v}_n</m> are linearly independent.
    </p>
</proof>
</theorem> 


<theorem xml:id="th-isocompisiso">

    <statement>
        <p>
            Let <m>U</m>, <m>V</m> and <m>W</m> be vector spaces.  Suppose <m>T_1:U\rightarrow V</m> and <m>T_2:V\rightarrow W</m> are isomorphisms.  
            Then <m>T_2\circ T_1:U\rightarrow W</m> is an isomorphism.
        </p>
    </statement>
</theorem>
<proof>
    <p>
The proof is left to the reader (see <xref ref="prob-isocompisisoproof"/>).
    </p>
</proof>
</subsection>










<subsection xml:id="Subsection-Finite-dimensional-Vector-Spaces">
    <title>Finite-dimensional Vector Spaces</title>


<theorem xml:id="th-ndimspacesisorn">

    <statement>
        <p>
            Let <m>V</m> and <m>W</m> be finite-dimensional vector spaces. Then

<me>
    V\cong W\quad\text{if and only if}\quad \mbox{dim}(V)=\mbox{dim}(W).
</me>
        </p>
    </statement>

<proof>
    <p>
First, assume that <m>V\cong W</m>.  Then there exists an isomorphism <m>T:V\rightarrow W</m>.  Suppose <m>\mbox{dim}(V)=n</m> and let 
<me>
\{\mathbf{v}_1,\mathbf{v}_2,\ldots ,\mathbf{v}_n\}
</me>

be a basis for <m>V</m>. By <xref ref="th-bijectionsbasis"/>, 
<me>
\{T(\mathbf{v}_1),\ldots ,T(\mathbf{v}_n)\}
</me>

is a basis for <m>W</m>. Therefore <m>\mbox{dim}(W)=n</m>.
    </p>

    <p>
Conversely, suppose <m>\mbox{dim}(V)=\mbox{dim}(W)=n</m>, and let 
<me>
\mathcal{B}=\{\mathbf{v}_1,\mathbf{v}_2,\ldots ,\mathbf{v}_n\},\quad
 \mathcal{C}=\{\mathbf{w}_1,\mathbf{w}_2,\ldots ,\mathbf{w}_n\}
</me> 

be bases for <m>V</m> and <m>W</m>, respectively.

Define a linear transformation <m>T:V\rightarrow W</m> by <m>T(\mathbf{v}_i)=\mathbf{w}_i</m> for <m>1\leq i\leq n</m>.  To show that <m>T</m> is an isomorphism, we need to prove that <m>T</m> is injective and surjective.

Suppose <m>T(\mathbf{u}_1)=T(\mathbf{u}_2)</m> for some vectors <m>\mathbf{u}_1</m>, <m>\mathbf{u}_2</m> in <m>V</m>.  We know that

<me>
    \mathbf{u}_1=a_1\mathbf{v}_1+\ldots +a_n\mathbf{v}_n,
</me>


<me>
    \mathbf{u}_2=b_1\mathbf{v}_1+\ldots +b_n\mathbf{v}_n,
</me>

for some scalars <m>a_i</m>'s and <m>b_i</m>'s.  Thus,

<me>
    T(a_1\mathbf{v}_1+\ldots +a_n\mathbf{v}_n)=T(b_1\mathbf{v}_1+\ldots +b_n\mathbf{v}_n)
</me>

By linearity of <m>T</m>,

<me>
    a_1\mathbf{w}_1+\ldots +a_n\mathbf{w}_n=b_1\mathbf{w}_1+\ldots +b_n\mathbf{w}_n
</me>


<me>
    (a_1-b_1)\mathbf{w}_1+\ldots +(a_n-b_n)\mathbf{w}_n=\mathbf{0}
</me>

But <m>\mathbf{w}_1,\mathbf{w}_2,\ldots ,\mathbf{w}_n</m> are linearly independent, so <m>a_i-b_i=0</m> for all <m>1\leq i\leq n</m>.  Therefore <m>a_i=b_i</m> for all <m>1\leq i\leq n</m>.  We conclude that <m>\mathbf{u}_1=\mathbf{u}_2</m>.

We now show that <m>T</m> is onto. Suppose that <m>\mathbf{w}</m> is an element of <m>W</m>.  Then <m>\mathbf{w}=c_1\mathbf{w}_1+\ldots +c_n\mathbf{w}_n</m> for some scalars <m>c_i</m>'s.  But then

<me>
    \mathbf{w}=c_1T(\mathbf{v}_1)+\ldots +c_nT(\mathbf{v}_n)=T(c_1\mathbf{v}_1+\ldots +c_n\mathbf{v}_n)
</me>

We conclude that <m>\mathbf{w}</m> is an image of an element of <m>V</m>, so <m>T</m> is onto.
    </p>
</proof>
</theorem>

<p>
From this theorem follows an important corollary that shows why we spent so much time trying to understand <m>\R^n</m> in this course.
</p>


<corollary xml:id="cor-ndimisotorn">

    <statement>
        <p>
            Every <m>n</m>-dimensional vector space is isomorphic to <m>\R^n</m>.
        </p>
    </statement>
</corollary>

<p>
    In applying the corollary to examples, the result looks less surprising:
</p> 

<example xml:id="ex-planeisoplane">
    <p>
        The span of any two linearly independent vectors in <m>\R^3</m> is isomorphic to <m>\R^2</m>.
    </p>

<image width="65%">
   <shortdescription>Plane isomorphic to a subspace of R3 drawn</shortdescription>
    <latex-image>
        \tdplotsetmaincoords{70}{130}
      \begin{tikzpicture}[scale=0.8]
	\draw[-\gt ](-2,0,0)--(3,0,0) ;
    \draw[-\gt ](0,-2,0)--(0,3,0) ;
    \draw[-\gt ](0,0,-2)--(0,0,5) ;
     \filldraw[blue, opacity=0.3] (2,3,0)--(0,2,2.5)--(-2,-3,0)--(0,-2,-2.5)--cycle;
     
    
    \draw[-\gt , line width=2pt,red, -stealth](0,0,0)--(2,3,0);
     \draw[-\gt , line width=2pt,blue, -stealth](0,0,0)--(0,2,2.5);
    
    \end{tikzpicture}
    </latex-image>
</image>

<image width="65%">
   <shortdescription>Drawing above continued</shortdescription>
    <latex-image>
      \begin{tikzpicture}[scale=0.5]

  \draw[\lt -\gt ] (-4,0)--(4,0);
  \draw[\lt -\gt ] (0,-4)--(0,4);
   \filldraw[blue, opacity=0.3] (-3,-3)--(-3,3)--(3,3)--(3,-3)--cycle;
     \end{tikzpicture}
    </latex-image>
</image>
</example>

<example xml:id="ex-p2isor3b">
    <statement>
        <p>
            <me>\mathbb{P}^2\ncong \R^2.</me>
       </p>
    </statement>
    <answer>
        <p>
            Recall that <m>\mbox{dim}(\mathbb{P}^2)=3</m>.  Since <m>\mbox{dim}(\mathbb{P}^2)=3\neq 2=\mbox{dim}(\R^2)</m>, we conclude that <m>\mathbb{P}^2</m> is not isomorphic to <m>\R^2</m>.
       </p>
    </answer>
</example>
</subsection>










<exercises>


<exercise xml:id="prob-Tonetooneonto">
    <statement>
        <p>
            Prove that transformation <m>T</m> of <xref ref="init-isomorph"/> is injective and surjective.
        </p>
    </statement>
</exercise>

<exercise xml:id="prob-tauone">
    <statement>
        <p>
            Verify that 
<me>
    \tau_1:\mathbb{M}_{2,2}\rightarrow\mathbb{P}^3
</me>

given by

<me>
    \tau_1\left(\begin{bmatrix}a\amp b\\c\amp d\end{bmatrix}\right)=a-bx-cx^2+dx^3
</me>

of <xref ref="eq-justforfuniso1"/> is an isomorphism.
        </p>
    </statement>
</exercise>

<exercise xml:id="prob-inverseislinear">
    <statement>
        <p>
            Prove <xref ref="th-inverseislinear"/>.
        </p>
    </statement>
</exercise>

<exercise xml:id="prob-noiso">
    <statement>
        <p>
            Do <xref ref="ex-inverseimageoflinind"/> without using isomorphisms.
        </p>
    </statement>
</exercise>

<exercise xml:id="prob-useisoshowlinind">
    <statement>
        <p>
            Let 

<me>
    \mathcal{S}=\left\{\begin{bmatrix}1\amp -3\\-2\amp 2\end{bmatrix}, \begin{bmatrix}4\amp -2\\1\amp 5\end{bmatrix}, \begin{bmatrix}5\amp 5\\8\amp 4\end{bmatrix}\right\}.
</me>

Is <m>\mathcal{S}</m> linearly independent in <m>\mathbb{M}_{2,2}</m>? 
        </p>
    </statement>

<answer>
    <p>
Yes!
    </p>
</answer>
</exercise>

<exercise xml:id="prob-basism22iso">
    <statement>
        <p>
            Let 

<me>
    \mathcal{S}=\left\{\begin{bmatrix}1\amp 2\\3\amp 4\end{bmatrix}, \begin{bmatrix}5\amp 6\\7\amp 8\end{bmatrix}, \begin{bmatrix}9\amp 10\\11\amp 12\end{bmatrix}, \begin{bmatrix}13\amp 14\\15\amp 16\end{bmatrix}\right\}
</me>

Is <m>\mathcal{S}</m> a basis for <m>\mathbb{M}_{2,2}</m>?
        </p>
    </statement>

<answer>
    <p>
No!
    </p>
</answer>
</exercise>

<exercise xml:id="prob-bijectionsbasisproof">
    <statement>
        <p>
            Prove <xref ref="th-bijectionsbasis"/>.
        </p>
    </statement>
</exercise>

<exercise xml:id="prob-chooseisospace">
    <statement>
        <p>
            Let <m>V</m> be a vector space, and suppose <m>\mathcal{B}=\{\mathbf{v}_1, \mathbf{v}_2, \mathbf{v}_3, \mathbf{v}_4, \mathbf{v}_5\}</m> is a basis for <m>V</m>.  What can we conclude about <m>V</m>?  Check ALL that apply.
        </p>
    </statement> 

 <choices>
  <choice>
      <statement>
          <p> We cannot conclude anything about <m>V</m> because we do not know what <m>V</m> is. </p>
      </statement>
  </choice>
      <choice correct="yes">
      <statement>
          <p> <m>V\cong \R^5</m>. </p>
      </statement>
  </choice>
      <choice>
      <statement>
          <p> <m>V\cong \mathbb{P}^5</m>. </p>
      </statement>
  </choice>
      <choice>
      <statement>
          <p> <m>V\cong \R^4</m>. </p>
      </statement>
  </choice>
      <choice correct="yes">
      <statement>
          <p> <m>V\cong \mathbb{P}^4 </m>. </p>
      </statement>
  </choice>
      <choice>
      <statement>
          <p> <m>V\cong \mathbb{M}_{5,5} </m>- </p>
      </statement>
  </choice>
</choices>
     
</exercise>

<exercise xml:id="prob-pickisospaces">
    <statement>
        <p>
            Which of the followng statements are true? Check ALL that apply.
        </p>
    </statement>

<choices>
  <choice>
      <statement>
          <p> <m>\mathbb{M}_{3,3} \cong \R^6 </m>. </p>
      </statement>
  </choice>
      <choice correct="yes">
      <statement>
          <p> <m>\mathbb{M}_{3,3}\cong \R^9</m>. </p>
      </statement>
  </choice>
      <choice>
      <statement>
          <p> <m>\mathbb{M}_{4,4} \cong \mathbb{P}^{16} </m>. </p>
      </statement>
  </choice>
      <choice correct="yes">
      <statement>
          <p> <m>\mathbb{M}_{4,4} \cong \mathbb{P}^{15}</m>. </p>
      </statement>
  </choice>
      <choice correct="yes">
      <statement>
          <p> <m>\mathbb{L} \cong \R^2 </m>. </p>
      </statement>
  </choice>
</choices>
</exercise>

<exercise xml:id="prob-verifyisomorphism">
    <statement>
        <p>
            Verify that <m>T:V\rightarrow \R^n</m> of <xref ref="ex-coordmapiso"/> is an isomorphism.
        </p>
    </statement>

<hint>
<p>
You may find the proof of T<xref ref="th-ndimspacesisorn"/> helpful.
</p> 
</hint>
</exercise>

<exercise xml:id="prob-isocompisisoproof">
    <statement>
        <p>
            Prove that the composition of two isomorphisms is an isomorphism  (see also <xref ref="th-isocompisiso"/>).
        </p>
    </statement>
</exercise>

<exercise xml:id="kerneliszero">
    <statement>
        <p>
Prove that a linear transformation <m>T:V\rightarrow W</m> is injective if and only if <m>\text{ker}(T)=\{\mathbf{0}\}</m>.
        </p>
    </statement>
</exercise>



</exercises>
</section>